<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Optimization on Nemo&#39;s Blog</title>
    <link>http://localhost:1313/blog/tags/optimization/</link>
    <description>Recent content in Optimization on Nemo&#39;s Blog</description>
    <generator>Hugo -- 0.140.2</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/tags/optimization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning Series 1: Optimization, Generalization and Supervised Learning</title>
      <link>http://localhost:1313/blog/posts/ml1/</link>
      <pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/posts/ml1/</guid>
      <description>&lt;h1 id=&#34;0饭后甜品你不能指望跟正餐一起&#34;&gt;0.饭后甜品，你不能指望跟正餐一起&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Everything should be made as simple as possible, but not simpler.&lt;/em&gt;
&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; Albert Einstein.&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;记得高三的时候写过一篇作文，文章的立意大概是 &lt;strong&gt;“整顿旗鼓再出发”&lt;/strong&gt; 。是啊，多少次，我们奋力狂奔，迎接着狂风骤雨的敲打，却不愿意放慢脚步，从对未来不确定性的焦虑之中跳脱出来，看看自己的来时路，看看昨日之我、今日之我。在忙忙叨叨之中时光便流逝掉了，有时不妨做点 &lt;strong&gt;reflection&lt;/strong&gt;,整理一下杂乱的思绪和没想明白的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;另一个落在实处的动机是我发现我学东西有个特点，就是忘东西很快。如果不留下点东西呢，会忘，然后忘了没有笔记又很难捡起来。&lt;/strong&gt; 所以我想，为什么不在自己对这个领域的内容认识最深刻的时候留下点记忆，寄希望于未来的自己或者或许对机器学习有兴趣的读者能够通过今日的一篇文章了解一些今日之我所思所想的一些内容呢，于是就诞生了这篇文章。&lt;/p&gt;
&lt;p&gt;但这件事怎么看都还是很呆，都考完了，然后在写的过程中肯定又能学到点东西。一位朋友跟我说 &lt;strong&gt;“饭后甜品，你不能指望跟正餐一起”&lt;/strong&gt; ，于是本着一个品味甜品的食客的心态，我决定将这篇文章尽量写的轻量化一点、故事性强一点，穿起一个思考的主线。&lt;/p&gt;
&lt;h1 id=&#34;1-optimization&#34;&gt;1. Optimization&lt;/h1&gt;
&lt;p&gt;优化问题自然而然地出现在许多应用领域中。无论人们做什么，在某些时候，他们都会产生一种想要以最佳方式组织事物的渴望。这种意图，当被转换成数学形式时，就会变成某种类型的优化问题。下面介绍几种优化算法，包括：&lt;em&gt;Gradient Descent&lt;/em&gt;, &lt;em&gt;Stochastic Gradient Descent&lt;/em&gt;,  &lt;em&gt;SVRG&lt;/em&gt;, &lt;em&gt;Mirror Desent&lt;/em&gt;, &lt;em&gt;Linear Coupling&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;11-l-smooth--convex&#34;&gt;1.1 L-Smooth &amp;amp; Convex&lt;/h2&gt;
&lt;p&gt;在优化函数的时候，我们往往需要一些有关函数性质的保障，才能够确保他有好的收敛率。&lt;/p&gt;
&lt;h3 id=&#34;l-smooth&#34;&gt;L-smooth&lt;/h3&gt;
&lt;p&gt;以下三条等价：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$f(x) \leq f(x_0) + \langle \nabla f(x_0), x-x_0 \rangle + \frac{L}{2}||x-x_0||^2$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$|\lambda_{\nabla^2 f(x)}| \leq L$&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
