<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Machine Learning Series: 1.Optimization, Generalization and Supervised Learning | Nemo&#39;s Blog</title>
<meta name="keywords" content="machine-learning, computer-science, optimization, math, artificial-intelligence">
<meta name="description" content="This is the first article in the Machine Learning Series. It covers the basics of optimization(GD,SGD,SVRG,Mirror Descent,Linear Coupling), generalization(No Free Lunch, PAC Learning, VC Dimension), and supervised learning(Linear Regression, Logistic Regression, Compressed Sensing).">
<meta name="author" content="Nemo">
<link rel="canonical" href="http://localhost:1313/blog/posts/ml1/">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.75ecc523c7c1152df7c886f961b47af55f43bbb2ec48d13c8f8959e716f99b35.css" integrity="sha256-dezFI8fBFS33yIb5YbR69V9Du7LsSNE8j4lZ5xb5mzU=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/blog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/posts/ml1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>


<script>
    
    const konamiCode = ['ArrowUp', 'ArrowUp', 'ArrowDown', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'ArrowLeft', 'ArrowRight', 'b', 'a'];
    let konamiCodePosition = 0;
    
    document.addEventListener('keydown', function(e) {
        
        const key = e.key;
        
        
        const expectedKey = konamiCode[konamiCodePosition];
        
        
        if (key.toLowerCase() === expectedKey.toLowerCase()) {
            
            konamiCodePosition++;
            
            
            if (konamiCodePosition === konamiCode.length) {
                
                konamiCodePosition = 0;
                
                
                window.location.href = '/blog/secret-page/';
            }
        } else {
            
            konamiCodePosition = 0;
        }
    });

    
    let clickCount = 0;
    let lastClickTime = 0;
    const CLICK_TIMEOUT = 1000; 
    
    document.addEventListener('click', function(e) {
        
        if (e.button !== 0) return;
        
        const currentTime = new Date().getTime();
        
        
        if (currentTime - lastClickTime > CLICK_TIMEOUT) {
            clickCount = 0;
        }
        
        clickCount++;
        lastClickTime = currentTime;
        
        
        if (clickCount === 5) {
            clickCount = 0;
            window.location.href = '/blog/collaborators/';
        }
    });
</script><meta property="og:url" content="http://localhost:1313/blog/posts/ml1/">
  <meta property="og:site_name" content="Nemo&#39;s Blog">
  <meta property="og:title" content="Machine Learning Series: 1.Optimization, Generalization and Supervised Learning">
  <meta property="og:description" content="This is the first article in the Machine Learning Series. It covers the basics of optimization(GD,SGD,SVRG,Mirror Descent,Linear Coupling), generalization(No Free Lunch, PAC Learning, VC Dimension), and supervised learning(Linear Regression, Logistic Regression, Compressed Sensing).">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-11-09T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-11-09T00:00:00+00:00">
    <meta property="article:tag" content="Machine-Learning">
    <meta property="article:tag" content="Computer-Science">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Math">
    <meta property="article:tag" content="Artificial-Intelligence">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Series: 1.Optimization, Generalization and Supervised Learning">
<meta name="twitter:description" content="This is the first article in the Machine Learning Series. It covers the basics of optimization(GD,SGD,SVRG,Mirror Descent,Linear Coupling), generalization(No Free Lunch, PAC Learning, VC Dimension), and supervised learning(Linear Regression, Logistic Regression, Compressed Sensing).">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning Series: 1.Optimization, Generalization and Supervised Learning",
      "item": "http://localhost:1313/blog/posts/ml1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Machine Learning Series: 1.Optimization, Generalization and Supervised Learning",
  "name": "Machine Learning Series: 1.Optimization, Generalization and Supervised Learning",
  "description": "This is the first article in the Machine Learning Series. It covers the basics of optimization(GD,SGD,SVRG,Mirror Descent,Linear Coupling), generalization(No Free Lunch, PAC Learning, VC Dimension), and supervised learning(Linear Regression, Logistic Regression, Compressed Sensing).",
  "keywords": [
    "machine-learning", "computer-science", "optimization", "math", "artificial-intelligence"
  ],
  "articleBody": "0.é¥­åç”œå“,ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ· Everything should be made as simple as possible, but not simpler. Albert Einstein.\nè®°å¾—é«˜ä¸‰çš„æ—¶å€™å†™è¿‡ä¸€ç¯‡ä½œæ–‡,æ–‡ç« çš„ç«‹æ„å¤§æ¦‚æ˜¯ â€œæ•´é¡¿æ——é¼“å†å‡ºå‘â€ ã€‚æ˜¯å•Š,å¤šå°‘æ¬¡,æˆ‘ä»¬å¥‹åŠ›ç‹‚å¥”,è¿æ¥ç€ç‹‚é£éª¤é›¨çš„æ•²æ‰“,å´ä¸æ„¿æ„æ”¾æ…¢è„šæ­¥,ä»å¯¹æœªæ¥ä¸ç¡®å®šæ€§çš„ç„¦è™‘ä¹‹ä¸­è·³è„±å‡ºæ¥,çœ‹çœ‹è‡ªå·±çš„æ¥æ—¶è·¯,çœ‹çœ‹æ˜¨æ—¥ä¹‹æˆ‘ã€ä»Šæ—¥ä¹‹æˆ‘ã€‚åœ¨å¿™å¿™å¨å¨ä¹‹ä¸­æ—¶å…‰ä¾¿æµé€æ‰äº†,æœ‰æ—¶ä¸å¦¨åšç‚¹ reflection,æ•´ç†ä¸€ä¸‹æ‚ä¹±çš„æ€ç»ªå’Œæ²¡æƒ³æ˜ç™½çš„é—®é¢˜ã€‚\nå¦ä¸€ä¸ªè½åœ¨å®å¤„çš„åŠ¨æœºæ˜¯æˆ‘å‘ç°æˆ‘å­¦ä¸œè¥¿æœ‰ä¸ªç‰¹ç‚¹,å°±æ˜¯å¿˜ä¸œè¥¿å¾ˆå¿«ã€‚å¦‚æœä¸ç•™ä¸‹ç‚¹ä¸œè¥¿å‘¢,ä¼šå¿˜,ç„¶åå¿˜äº†æ²¡æœ‰ç¬”è®°åˆå¾ˆéš¾æ¡èµ·æ¥ã€‚ æ‰€ä»¥æˆ‘æƒ³,ä¸ºä»€ä¹ˆä¸åœ¨è‡ªå·±å¯¹è¿™ä¸ªé¢†åŸŸçš„å†…å®¹è®¤è¯†æœ€æ·±åˆ»çš„æ—¶å€™ç•™ä¸‹ç‚¹è®°å¿†,å¯„å¸Œæœ›äºæœªæ¥çš„è‡ªå·±æˆ–è€…æˆ–è®¸å¯¹æœºå™¨å­¦ä¹ æœ‰å…´è¶£çš„è¯»è€…èƒ½å¤Ÿé€šè¿‡ä»Šæ—¥çš„ä¸€ç¯‡æ–‡ç« äº†è§£ä¸€äº›ä»Šæ—¥ä¹‹æˆ‘æ‰€æ€æ‰€æƒ³çš„ä¸€äº›å†…å®¹å‘¢,äºæ˜¯å°±è¯ç”Ÿäº†è¿™ç¯‡æ–‡ç« ã€‚\nä½†è¿™ä»¶äº‹æ€ä¹ˆçœ‹éƒ½è¿˜æ˜¯å¾ˆå‘†,éƒ½è€ƒå®Œäº†,ç„¶ååœ¨å†™çš„è¿‡ç¨‹ä¸­è‚¯å®šåˆèƒ½å­¦åˆ°ç‚¹ä¸œè¥¿ã€‚ä¸€ä½æœ‹å‹è·Ÿæˆ‘è¯´ â€œé¥­åç”œå“,ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·â€ ,äºæ˜¯æœ¬ç€ä¸€ä¸ªå“å‘³ç”œå“çš„é£Ÿå®¢çš„å¿ƒæ€,æˆ‘å†³å®šå°†è¿™ç¯‡æ–‡ç« å°½é‡å†™çš„è½»é‡åŒ–ä¸€ç‚¹ã€æ•…äº‹æ€§å¼ºä¸€ç‚¹,ç©¿èµ·ä¸€ä¸ªæ€è€ƒçš„ä¸»çº¿ã€‚\n1. Optimization ä¼˜åŒ–é—®é¢˜è‡ªç„¶è€Œç„¶åœ°å‡ºç°åœ¨è®¸å¤šåº”ç”¨é¢†åŸŸä¸­ã€‚æ— è®ºäººä»¬åšä»€ä¹ˆ,åœ¨æŸäº›æ—¶å€™,ä»–ä»¬éƒ½ä¼šäº§ç”Ÿä¸€ç§æƒ³è¦ä»¥æœ€ä½³æ–¹å¼ç»„ç»‡äº‹ç‰©çš„æ¸´æœ›ã€‚è¿™ç§æ„å›¾,å½“è¢«è½¬æ¢æˆæ•°å­¦å½¢å¼æ—¶,å°±ä¼šå˜æˆæŸç§ç±»å‹çš„ä¼˜åŒ–é—®é¢˜ã€‚ä¸‹é¢ä»‹ç»å‡ ç§ä¼˜åŒ–ç®—æ³•,åŒ…æ‹¬ï¼šGradient Descent, Stochastic Gradient Descent, SVRG, Mirror Desent, Linear Coupling.\n1.1 L-Smooth \u0026 Convex åœ¨ä¼˜åŒ–å‡½æ•°çš„æ—¶å€™,æˆ‘ä»¬å¾€å¾€éœ€è¦ä¸€äº›æœ‰å…³å‡½æ•°æ€§è´¨çš„ä¿éšœ,æ‰èƒ½å¤Ÿç¡®ä¿ä»–æœ‰å¥½çš„æ”¶æ•›ç‡ã€‚\nL-smooth ä»¥ä¸‹ä¸‰æ¡ç­‰ä»·ï¼š\n$f(x) \\leq f(x_0) + \\langle \\nabla f(x_0), x-x_0 \\rangle + \\frac{L}{2}||x-x_0||^2$\n$|\\lambda_{\\nabla^2 f(x)}| \\leq L$\n$||\\nabla f(x) - \\nabla f(y)|| \\leq L||x-y||$\næ³¨æ„åˆ°L-smoothå…¶å®å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯æ¢¯åº¦å˜åŒ–ä¸ä¼šå¤ªå¿«,å¦å¤–ä¸€ä¸ªæœ‰è¶£çš„çœ‹æ³•æ˜¯ï¼š\nUpper Bound: $f(x) \\leq f(x_0) + \\langle \\nabla f(x_0), x-x_0 \\rangle + \\frac{L}{2}||x-x_0||^2$ Lower Bound: $f(x) \\geq f(x_0) + \\langle \\nabla f(x_0), x-x_0 \\rangle - \\frac{L}{2}||x-x_0||^2$ ä¹Ÿå°±æ˜¯è¯´ç»™å®šä¸€ä¸ªç‚¹$f(x_0)$çš„é›¶é˜¶å’Œä¸€é˜¶ä¿¡æ¯,æˆ‘ä»¬å°±å¯ä»¥è·å¾—åˆ«çš„ç‚¹çš„å‡½æ•°å€¼çš„ä¸€ä¸ªäºŒæ¬¡å‹çš„ä¸Šä¸‹ç•Œã€‚ Convex ä»¥ä¸‹å››æ¡ç­‰ä»·ï¼š\n$ f(x) \\geq f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle $ $ f(x) \\leq f(x_0) + \\langle \\nabla f(x), x - x_0 \\rangle $ $ \\lambda_{\\min}(\\nabla^2 f(x)) \\geq 0 $ $ \\frac{1}{T} \\sum_{i=1}^{T} f(x_i) \\geq f(\\bar{x}), \\quad \\bar{x} = \\frac{1}{T} \\sum_{i=1}^{T} x_i $ $\\mu$-strongly Convex ä»¥ä¸‹ä¸‰æ¡ç­‰ä»·ï¼š\n$ f(x) \\geq f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{\\mu}{2} |x - x_0|^2 $ $ \\lambda_{\\min}(\\nabla^2 f(x)) \\geq \\mu $ $ |\\nabla f(x) - \\nabla f(y)| \\geq \\mu |x - y| $ Convex \u0026 L-Smooth: åœ¨ä¸€ä¸ªå‡½æ•°åˆconvexåˆL-Smoothçš„æƒ…å†µä¸‹,æˆ‘ä»¬ä¼šæœ‰ä¸€äº›æ›´å¥½çš„æ€§è´¨ï¼š\nThm.1 $$ f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle \\geq \\frac{1}{2L} |\\nabla f(x) - \\nabla f(y)|^2 $$\nè¯æ˜å¦‚ä¸‹:\nä»¤ $h(y) = f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle$\næ³¨æ„åˆ° $$ \\nabla h(y) = \\nabla f(y) - \\nabla f(x) $$ $$ \\nabla^2 h(y) = \\nabla^2 f(y) $$ æ‰€ä»¥è¯´$h(y)$ä¹Ÿæ˜¯convexä¸”L-smoothçš„,è€Œä¸”æœ€å°å€¼ç‚¹åœ¨$y=x$å¤„å–çš„ã€‚ æ‰€ä»¥,\n$$ h(x) \\leq h(y - \\frac{1}{L} \\nabla h(y))\\ $$ $$\\leq h(y) - \\frac{1}{L} |\\nabla h(y)|^2 + \\frac{1}{2L} |\\nabla h(y)|^2 $$ $$ =h(y) - \\frac{1}{2L} |\\nabla h(y)|^2 $$\nå› æ­¤,\n$$ f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle \\geq \\frac{1}{2L} |\\nabla f(y)-\\nabla f(x)|^2 $$\nThm.2 $$ \\langle \\nabla f(x) - \\nabla f(y), x - y \\rangle \\geq \\frac{1}{L} |\\nabla f(x) - \\nabla f(y)|^2$$\nè¿™ä¸ªçš„è¯æ˜å¯ä»¥ç”±Thm.1äº¤æ¢$x,y$æ¬¡åºä¹‹åç›¸åŠ å¾—åˆ°ã€‚\n1.2 Gradient Descent GDçš„update ruleå¦‚ä¸‹: $$x_{t+1}=x_{t}-\\eta \\nabla f(x_t)$$ åœ¨ä»¥ä¸‹ä¸‰ç§æƒ…å†µä¸‹,åˆ†åˆ«æœ‰ä¸åŒçš„æ”¶æ•›ç‡ï¼š\nConvex, L-Smooth $$ x_{t+1} = x_t - \\eta \\nabla f(x_t) $$\n$$ f(x_{t+1}) \\leq f(x_t) + \\langle \\nabla f(x_t), x_{t+1} - x_t \\rangle + \\frac{L}{2} |x_{t+1} - x_t|^2 $$\n$$ = f(x_t) - \\eta |\\nabla f(x_t)|^2 - \\frac{L \\eta^2}{2} |\\nabla f(x_t)|^2 $$\nå–$\\eta \\leq \\frac{1}{L}$:\n$$ f(x_{t+1}) \\leq f(x_t) - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 $$\nç”±convexity:\n$$ f(x_{t+1}) \\leq f(x^*) + \\langle \\nabla f(x_t), x_t - x^* \\rangle - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 $$\n$$ = f(x^*) - \\frac{1}{\\eta} \\langle x_{t+1} - x_t, x_t - x^* \\rangle - \\frac{1}{2\\eta} |x_{t+1} - x_t|^2 $$\n$$ = f(x^*) - \\frac{1}{2\\eta} |x_{t+1} - x^*|^2 + \\frac{1}{2\\eta} |x_t - x^*|^2 $$\næ¥ä¸‹æ¥æˆ‘ä»¬åštelescope:\n$$ \\sum_{t=0}^{T-1} (f(x_{t+1}) - f(x^*)) \\leq \\frac{1}{2\\eta} (|x_0 - x^*|^2 - |x_T - x^*|^2) $$\nå› ä¸º$f(x_t)$æ˜¯å•è°ƒé€’å‡çš„(convexä¿è¯)\n$$ f(x_T) - f(x^*) \\leq \\frac{1}{2\\eta T} |x_0 - x^*|^2 = \\epsilon $$ æ‰€ä»¥è¯´ $$ T = \\frac{|x_0 - x^*|^2}{2\\eta \\epsilon} = O\\left(\\frac{L}{\\epsilon}\\right) $$ åœ¨è¿™ç§æƒ…å†µä¸‹éœ€è¦è¿­ä»£$O(\\frac{1}{\\epsilon})$æ¬¡,æ”¶æ•›ç‡ä¸º$O(\\frac{1}{T})$.\n$\\mu$-strongly Convex \u0026 L-smooth è¿™é‡Œèµ·æ‰‹å¼æˆ‘ä»¬å¡$||x-x^*||$: $$ |x_{t+1} - x^*|^2 = |x_t - \\eta \\nabla f(x_t) - x^*|^2 $$\n$$ = |x_t - x^*|^2 - 2\\eta \\langle \\nabla f(x_t), x_t - x^* \\rangle + \\eta^2 |\\nabla f(x_t)|^2 $$ å› ä¸ºå¼ºå‡¸æ€§ï¼š $$ f(y) \\geq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{\\mu}{2} |y - x|^2 $$\nä»£å…¥ $x = x_t$, $y = x^*$:\n$$ f(x^*) \\geq f(x_t) + \\langle \\nabla f(x_t), x^* - x_t \\rangle + \\frac{\\mu}{2} |x_t - x^*|^2 $$\n$$ \\langle \\nabla f(x_t), x_t - x^* \\rangle \\geq f(x_t) - f(x^*) + \\frac{\\mu}{2} |x_t - x^*|^2 $$ æ‰€ä»¥ $$ |x_{t+1} - x^*|^2 \\leq |x_t - x^*|^2 - 2\\eta (f(x_t) - f(x^*) + \\frac{\\mu}{2} |x_t - x^*|^2ï¼‰ + \\eta^2 |\\nabla f(x_t)|^2 $$ æ ¹æ®ä¹‹å‰çš„Thm.1: $$ \\frac{1}{2L} |\\nabla f(x_t)|^2 \\leq f(x_t) - f(x^*) $$ æ‰€ä»¥ $$ |x_{t+1} - x^*|^2 \\leq (1 - \\eta \\mu) |x_t - x^*|^2 + (2\\eta^2 L - 2\\eta )(f(x_t) - f(x^*)) $$\nå– $\\eta = \\frac{1}{L}$:\n$$ |x_{t+1} - x^*|^2 \\leq (1 - \\frac{\\mu}{L}) |x_t - x^*|^2 $$ æ‰€ä»¥è¯´Linear Convergence, åæ˜ åœ¨$f(x)$ä¸Š: $$f(x_T)\\leq f(x^*)+\\frac{L}{2}||x_T-x^*||^2$$ $$\\leq f(x^*)+\\frac{L}{2}(1 - \\frac{\\mu}{L})^T||x_0-x^*||^2$$ ä¹Ÿå°±æ˜¯è¯´éœ€è¦è¿­ä»£æ¬¡æ•°$O(log(\\frac{1}{\\epsilon}))$, æ”¶æ•›ç‡ä¸ºLinear Convergence.\nRemark: å¯¹äº$\\mu$-strongly Convex \u0026 L-smoothçš„å‡½æ•°æœ‰å¦‚ä¸‹æ€§è´¨ï¼š$ \\frac{\\mu}{2} | \\mathbf{x}^* - \\mathbf{x} |^2 \\leq f(\\mathbf{x}) - f^* \\leq \\frac{L}{2} | \\mathbf{x}^* - \\mathbf{x} |^2 $ $ \\frac{1}{2L} | \\nabla f(\\mathbf{x}) |^2 \\leq f(\\mathbf{x}) - f^* \\leq \\frac{1}{2\\mu} | \\nabla f(\\mathbf{x}) |^2 $æ ¹æ®è¿™äº›æ€§è´¨æœ‰ä¸€ä¸ªæ›´ä¸ºç®€æ´çš„è¯æ˜ã€‚\nL-Smooth æ ¹æ®ç¬¬ä¸€ç§æƒ…å†µä¸‹çš„åˆ†æï¼š $$ f(x_{t+1}) - f(x_t) \\leq -\\frac{\\eta}{2} |\\nabla f(x_t)|^2 $$\nç„¶ååšTelescope:\n$$ \\min_{k \\in [T]} |\\nabla f(x_t)|^2 \\leq \\frac{2L(f(x_0) - f(x^*))}{T} = \\epsilon^2 $$ æ‰€ä»¥è¯´å½“æˆ‘ä»¬æƒ³è·å¾—$|\\nabla f(x_t)|^2\u003c\\epsilon$,æˆ‘ä»¬éœ€è¦ $ T = O\\left(\\frac{1}{\\epsilon^2}\\right) $çš„è¿­ä»£æ¬¡æ•°,æ”¶æ•›ç‡ä¸º $ O\\left(\\frac{1}{\\sqrt{T}}\\right) $ã€‚\nRecap: æ€»ç»“èµ·æ¥å¤§æ¦‚æ˜¯: 1.3 Stochastic Gradient Descent Why SGD GDçœ‹èµ·æ¥ä¸é”™,ä½†æ˜¯æœ‰ä¸¤ä¸ªé—®é¢˜:\nè®¡ç®—ä¸€æ¬¡full gradientå¾ˆè´µ GDä¼šåœ¨local maximumå’Œsaddle pointï¼ˆéç‚¹ï¼‰å¡ä½ äºæ˜¯æˆ‘ä»¬å°±ä¼šå»æƒ³,èƒ½ä¸èƒ½å°‘ç®—å‡ ä¸ªæ•°æ®ç‚¹å¯¹åº”çš„loss function,åŒæ—¶åˆèƒ½æœ‰ä¸€äº›convergence guaranteeå‘¢,SGDä¾¿æ˜¯è¿™æ ·çš„ä¸€ç§ç®—æ³•ã€‚\nAlgorithm SGDçš„update ruleå¦‚ä¸‹æ‰€ç¤º: $$ x_{t+1} = x_t - \\eta G_t, $$ å…¶ä¸­$G_t$æ»¡è¶³: $$ \\mathbb{E}[G_t] = \\nabla f(x_t), \\quad \\text{Var}(G_t) \\leq \\sigma^2 $$\nConvergence ä¸‹é¢æˆ‘ä»¬è¯æ˜SGDåœ¨L-Smooth, Convex, $\\text{Var}(G_t) \\leq \\sigma^2$çš„æ¡ä»¶ä¸‹çš„æ”¶æ•›ç‡:\nå› ä¸ºL-smooth: $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) + \\mathbb{E}[\\langle \\nabla f(x_t), x_{t+1} - x_t \\rangle] + \\frac{L}{2} \\mathbb{E}[|x_{t+1} - x_t|^2] $$\n$$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) - \\eta |\\nabla f(x_t)|^2 + \\frac{L \\eta^2}{2} \\mathbb{E}[|G_t|^2] $$\næ ¹æ®æ–¹å·®çš„å®šä¹‰ï¼š $$\\mathbb{E}[ ||G_t||^2 ] = \\text{Var}(G_t) + ||\\mathbb{E}[G_t]||^2 \\leq \\sigma^2 + |\\nabla f(x_t)|^2$$ æ‰€ä»¥æœ‰ $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) + \\left(\\frac{L \\eta^2}{2} - \\eta\\right) |\\nabla f(x_t)|^2 + \\frac{L \\eta^2}{2} \\sigma^2 $$\nå– $\\eta = \\frac{1}{L}$:\n$$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 + \\frac{\\eta}{2} \\sigma^2 $$\næ ¹æ®convexity:\n$$ f(x_t) \\leq f(x^*) + \\langle \\nabla f(x_t), x_t - x^* \\rangle $$\n$$ \\mathbb{E}[f(x_{t+1})] \\leq f(x^*) + \\mathbb{E}[\\langle G_t, x_t - x^* \\rangle] - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 + \\frac{\\eta}{2} \\sigma^2 $$ åˆå› ä¸º $$ |\\nabla f(x_t)|^2 = \\mathbb{E}[|G_t|^2] - \\text{Var}(G_t) \\geq \\mathbb{E}[|G_t|^2] - \\sigma^2 $$\næ‰€ä»¥ $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x^*) + \\mathbb{E}[\\langle G_t, x_t - x^* \\rangle - \\frac{\\eta}{2} |G_t|^2] + \\eta \\sigma^2 $$ æ³¨æ„åˆ° $$ \\langle G_t, x_t - x^* \\rangle - \\frac{\\eta}{2} |G_t|^2 $$\n$$ = -\\frac{1}{2\\eta} |(x_{t+1} - x_t) - (x^* - x_t)|^2 + \\frac{1}{2\\eta} |x_t - x^*|^2 $$\n$$ = \\frac{1}{2\\eta} (|x_t - x^*|^2 - |x_{t+1} - x^*|^2) $$ ä¹Ÿå°±æ˜¯è¯´ $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x^*) + \\frac{\\eta}{2} \\mathbb{E}[|x_t - x^*|^2 - |x_{t+1} - x^*|^2] + \\eta \\sigma^2 $$\nä» $t = 0$ åˆ° $T-1$æ±‚å’Œ(telescope):\n$$ \\frac{1}{T}\\sum_{t=0}^{T-1} (\\mathbb{E}[f(x_t)] - f(x^*)) \\leq \\frac{1}{2\\eta T} |x_0 - x^*|^2 + \\eta \\sigma^2 $$\nå– $\\eta = \\frac{\\epsilon}{2\\sigma^2} \\leq \\frac{1}{L}$, åˆ™æœ‰:\n$$ T = \\frac{2 \\sigma^2 |x_0 - x^*|^2}{\\epsilon^2} $$\nStochastic Gradient Descent (SGD) çš„æ”¶æ•›ç‡æ˜¯ $ O\\left(\\frac{1}{\\sqrt{T}}\\right) $ã€‚\n1.4 SVRG æˆ‘ä»¬çœ‹åˆ°äº†é€šè¿‡Stochastic Gradientå¯ä»¥å‡å°‘computation cost,ä½†æ˜¯éšä¹‹è€Œæ¥çš„é—®é¢˜æ˜¯å› ä¸º $G_t$æ‹¥æœ‰çš„variance,å¯¼è‡´åŸæ¥$O(\\frac{1}{T})$çš„convergence rateå˜æˆäº†$O(\\frac{1}{\\sqrt{T}})$,äºæ˜¯æˆ‘ä»¬å»æƒ³,æœ‰æ²¡æœ‰ä»€ä¹ˆåŠæ³•èƒ½å¤Ÿåœ¨ä¿æŒcomputation costæ¯”è¾ƒå°çš„æƒ…å†µä¸‹åŒæ—¶æŠŠvarianceé™ä¸‹æ¥,SVRGæ˜¯å…¶ä¸­çš„ä¸€ç§ç®—æ³•,åœ¨strongly-convexå’Œl-smoothçš„æƒ…å†µä¸‹æœ€åèƒ½å¤Ÿè·å¾—å’ŒGDä¸€æ ·çš„convergence rateã€‚\nAlgorithm Procedure SVRG Parameters: update frequency $m$ and learning rate $\\eta$\nInitialize $\\tilde{w}_0$\nIterate: for $s = 1, 2, \\ldots$\n$\\tilde{w} = \\tilde{w}_{s-1}$ $\\tilde{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla l_i(\\tilde{w})$ $w_0 = \\tilde{w}$\nIterate: for $t = 1, 2, \\ldots, m$\ni. Randomly pick $i_t \\in {1, \\ldots, n}$ and update weight\n$ w_t = w_{t-1} - \\eta \\left( \\nabla l_{i_t}(w_{t-1}) - \\nabla l_{i_t}(\\tilde{w}) + \\tilde{\\mu} \\right) $ end\nOption I: set $\\tilde{w}_s = w_m$\nOption II: set $\\tilde{w}_s = w_t$ for randomly chosen $t \\in {0, \\ldots, m - 1}$\nend Convergence Rate å‰æå‡è®¾ï¼š\nL-smooth, $l_i$: convex, $f$: strong-convex\nBound $\\mathbb{E}[||v_t||^2]$:\nä»¤ $v_t = \\nabla l_i(w_{t-1}) - \\nabla l_i(\\tilde{w}) + \\tilde{u}$\n$\\mathbb{E}[||v_t||^{2}] = \\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(\\tilde{w}) + \\tilde{u})^2]$\nå› ä¸º$ (a+b)^2 \\leq 2a^2 + 2b^2 $ï¼š\n$\\leq 2\\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(w^*))^2] + 2\\mathbb{E}[(\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}) + \\tilde{u})^2]$\n$= 2\\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(w^*))^2] $\n$+ 2\\mathbb{E}[\\left((\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}))-\\mathbb{E}[(\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}))]\\right)^2]$\nåˆå› ä¸º$$\\mathbb{E}[(x - \\mathbb{E}[x])^2] = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2 \\leq \\mathbb{E}[x^2]:$$\næ‰€ä»¥$\\mathbb{E}[||v_t||^{2}]$\n$\\leq 2\\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(w^*))^2] + 2\\mathbb{E}[(\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}))^2]$\næ ¹æ®Thm.1:\n$\\leq 4L(f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*))$\nBound $||w_t-w^*||$ $$ \\mathbb{E}[|w_{t} - w^*|^2] = \\mathbb{E}[|w_t - w_{t-1} + w_{t-1} - w^*|^2] $$ $$ = \\mathbb{E}[|w_{t} - w^*|^2] + 2 \\mathbb{E}[\\langle w_t - w_{t-1}, w_{t-1} - w^* \\rangle] + \\mathbb{E}[|w_t - w_{t-1}|^2] $$\n$$ = |w_{t-1} - w^*|^2 - 2\\eta \\mathbb{E}[\\langle v_t, w_{t-1} - w^* \\rangle] + \\eta^2 \\mathbb{E}[v_t^2] $$\n$$ \\leq |w_{t-1} - w^*|^2 - 2\\eta \\mathbb{E}[\\langle v_t, w_{t-1} - w^* \\rangle] + 4\\eta^2 L(f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*)) $$\n$$ = |w_{t} - w^*|^2 - 2\\eta \\langle \\nabla f(w_{t-1}), w_{t+1} - w^* \\rangle + 4L\\eta^2 (f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*)) $$ åˆå› ä¸ºconvexityï¼š $$ f(w_{t-1}) - f(w^*) \\geq \\langle \\nabla f(w_{t-1}), w_{t-1} - w^* \\rangle $$\n$$ \\Rightarrow \\mathbb{E}[|w_{t} - w^*|^2] \\leq |w_{t-1} - w^*|^2 - 2\\eta (f(w_{t-1}) - f(w^*)) + 4L\\eta^2 (f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*)) $$\n$$ = |w_{t+1} - w^*|^2 + 4L\\eta^2 (f(\\tilde{w}) - f(w^*)) + 2\\eta (2L\\eta - 1)(f(w_{t-1}) - f(w^*)) $$\nTelescope ä»$\\sum_{t=1}^{m}$,ç”¨option 2: $$ \\mathbb{E}[|w_m - w^*|^2] \\leq \\mathbb{E}[|\\tilde{w} - w^*|^2] + 4mL\\eta^2 (f(\\tilde{w}) - f(w^*)) + 2m\\eta (2L\\eta - 1) \\mathbb{E}[f(\\tilde{w}_s) - f(w^*)] $$\né‡æ–°æ•´ç†æˆ:\n$$ \\mathbb{E}[|w_m - w^*|^2] + 2m\\eta (1 - 2L\\eta) \\mathbb{E}[f(\\tilde{w}_s) - f(w^*)] $$\n$$ \\leq \\mathbb{E}[|\\tilde{w} - w^*|^2] + 4mL\\eta^2 (f(\\tilde{w}) - f(w^*)) $$\n$$ \\leq \\left(\\frac{2}{u} + 4mL\\eta^2\\right)(f(\\tilde{w}) - f(w^*)) $$\næ‰€ä»¥\n$$ \\mathbb{E}[f(\\tilde{w}_s) - f(w^*)] \\leq (\\frac{1}{u\\eta (1 - 2L\\eta)m} + \\frac{2L\\eta}{1-2L\\eta}) $$\n$$\\cdot \\mathbb{E} [f(\\tilde{w}_{s - 1})-f(w^*)]$$\næ‰€ä»¥æ”¶æ•›ç‡æ˜¯Linear Convergence, $\\frac{L}{u}$å¤§æ—¶æ¯”GDå¿«ã€‚\n1.5 Mirror Descent Algorithm å¯¹äºä¸€ä¸ª1-strongly convexçš„Distance Generating Function$w(x)$,æˆ‘ä»¬å®šä¹‰Bergman Divergence:$$V_x(y)=w(y)-w(x)-\\langle \\nabla w(x),y-x \\rangle$$ ç„¶åæˆ‘ä»¬å®šä¹‰: $$\\text{Mirror}_ {x}(\\zeta) = \\arg \\min_ {y} { V_ {x}(y) + \\langle \\zeta, y - x \\rangle } $$\nä¸€ä¸ªMirror Descentçš„å®šä¹‰æ˜¯ $$ x_{t+1} = \\text{Mirror}_ {x_t} (\\alpha \\nabla f(x_t)) $$\n$$ = \\arg \\min_{y} \\left( w(y) - w(x_t) - \\langle \\nabla w(x_t), y - x_t \\rangle + \\alpha \\langle \\nabla f(x_t), y - x_t \\rangle \\right) $$\nIntuition ç¬¬äºŒç§è§†è§’ç§°ä¸ºé•œåƒç©ºé—´ (Mirror space) è§†è§’,ä¸€ä¸ª Mirror step å¯ä»¥è¢«è§†ä½œå°†å¶ç©ºé—´ä¸Šçš„æ¢¯åº¦ä¸‹é™,å³æœå¦ä¸€ä¸ªæ–°çš„æå€¼ç‚¹è¿›è¡Œæœç´¢ã€‚è¿‡ç¨‹å½¢å¦‚ï¼š\nå°† $x$ é€šè¿‡ Mirror map æ˜ å°„åˆ°å¯¹å¶ç©ºé—´ä¸Šçš„ $\\theta_k$ã€‚ $\\theta_ {k+1} = \\theta_ k - \\alpha \\nabla f(x_k)$ã€‚ å°† $\\theta_ {k+1}$ æ˜ å°„å›åŸç©ºé—´ä¸Šçš„ $\\overline{x} _{k+1}$ã€‚ å°† $\\overline{x}_ {k+1}$ æŠ•å½±åˆ°çº¦æŸé›†,æŠ•å½±ä½¿ç”¨ Bregman divergence ä½œä¸ºå…¶è·ç¦»,å³ $x_ {k+1} = \\arg \\min_ {y} V_ {x_{k+1}}(y)$ã€‚ æŒ‰ç…§ Mirror step çš„å¼å­,å¯ä»¥çœ‹å‡º Mirror map å°±æ˜¯ $\\nabla w(\\cdot)$ã€‚å› æ­¤å®é™…è¿‡ç¨‹ä¸ºï¼š\n$\\theta_k = \\nabla w(x)$ã€‚ $\\theta_{k+1} = \\theta_k - \\alpha \\nabla f(x_k)$ã€‚ $\\overline{x}_{k+1} = (\\nabla w)^{-1}(\\theta{k+1})$ã€‚ $x_{k+1} = \\arg \\min_{y} V_{\\overline{x}_{k+1}}(y)$ã€‚ è¿™ä¸ªè§†è§’æå‡ºäº†ä¸€ç‚¹å‡è®¾,$(\\nabla w)^{-1}(\\overline{x}_{k+1})$ å§‹ç»ˆå­˜åœ¨,å³ ${\\nabla w(x)} = \\mathbb{R}^n$ã€‚\nRelationship between GD \u0026 MD è¿™ä¸ªé—®é¢˜æ›¾å¾ˆé•¿ä¸€æ®µæ—¶é—´è®©ç¬”è€…æ„Ÿåˆ°å›°æƒ‘ã€‚ç¬”è€…å¯¹äºè¿™ä¸€å—å¹¶éå¾ˆæ‡‚,ç¬”è€…ç°åœ¨çš„ç†è§£æ˜¯:\næˆ‘ä»¬çŸ¥é“ä¸€ä¸ªPrimal Spaceå’ŒDual Spaceçš„èŒƒæ•°ä¹‹é—´æ»¡è¶³$\\frac{1}{p}+\\frac{1}{q}=1$\nGDæ˜¯MDåœ¨ $\\alpha=\\frac{1}{L}$,primal spaceå–$||Â·||_2$èŒƒæ•°,Distance Generating Functionå– $w(x)=\\frac{1}{2} x^2$ä¸‹çš„ç‰¹æ®Šæƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹,å› ä¸ºL2-normçš„Dualå°±æ˜¯L2-norm,æ‰€ä»¥è¿™ä¸ªå¯¹å¶ç©ºé—´å°±æ˜¯åŸç©ºé—´ã€‚\nä½†æ˜¯å¦ä¸€ç§ç†è§£æ–¹å¼æ˜¯,MDæ˜¯å…ˆé€šè¿‡æ¢¯åº¦æ˜ å°„åˆ°Dual Spaceä¹‹ååœ¨è¿™ä¸ªç©ºé—´ä¸‹åšGDå†é€†æ˜ å°„åprojectå›åŸæ¥çš„ç©ºé—´ä¸­ã€‚\nConvergence: å‰ææ¡ä»¶: $f(x)$ convex, $w(x)$ 1-strongly convex, $\\nabla f(x)\\leq \\rho$\nBound $f(x_t)-f(x^*)$: å› ä¸ºconvexityï¼š $$ \\alpha (f(x_{t+1}) - f(u)) \\leq \\langle \\alpha \\nabla f(x_t), x_t - u \\rangle $$ åˆå› ä¸ºMDçš„æ›´æ–°è§„åˆ™ï¼š $$ x_{t+1} = \\arg \\min_{y} \\left( V_{x_t}(y) + \\langle \\alpha \\nabla f(x_t), y - x_t \\rangle \\right) $$ æ‰€ä»¥è¯´ç”±æœ€å°å€¼ç‚¹æ¢¯åº¦ç­‰äº0: $$ \\alpha \\nabla f(x_t) = - \\nabla V_{x_t}(x_{t+1}) $$ å› æ­¤ $$ \\alpha (f(x_t) - f(u)) \\leq \\langle \\alpha \\nabla f(x_t), x_t - x_{k+1} \\rangle + \\langle - \\nabla V_{x_t}(x_{k+1}), x_{k+1} - u \\rangle $$ æ¥ä¸‹æ¥æˆ‘ä»¬è¯æ˜ä¸€ä¸ªé‡è¦çš„triangle inequality: $$ \\langle - \\nabla V_{x_t}(y), y - u \\rangle = \\langle \\nabla w(x) - \\nabla w(y), y - u \\rangle $$\n$$ = (w(u) - w(x)) - \\langle \\nabla w(x), u - x \\rangle - (w(y) - w(x) - \\langle \\nabla w(x), y - x \\rangle) $$\n$$ = V_x(u) - V_x(y) - V_y(u) $$ å¸¦å›åŸå¼: $$ \\alpha (f(x_t) - f(u)) \\leq \\langle \\alpha \\nabla f(x_t), x_t - x_{k+1} \\rangle + V_{x_k}(u) - V_{x_k}(x_{k+1}) - V_{x_{k+1}}(u) $$\nç”±äºDGFçš„1-strongly convex:\n$$ \\leq \\langle \\alpha \\nabla f(x_t), x_t - x_{k+1} \\rangle- \\frac{1}{2} |x_{k+1} - x_t|^2 + V_{x_k}(u) - V_{x_{k+1}}(u) $$ è¿™æ­¥æ˜¯å‰ä¸¤é¡¹åšä¸ªé…æ–¹æ³•: $$ \\leq \\frac{\\alpha^2}{2} |\\nabla f(x_t)|^2 + V_{x_k}(u) - V_{x_k}(x_{k+1}) $$\nTelescoping: $$ \\alpha T (f(\\overline{x}) - f(x_t)) \\leq \\sum \\text{LHS} \\leq \\sum \\text{RHS} $$ $$ \\leq \\frac{\\alpha^2 T}{2} \\cdot \\rho^2 + V_{x_0}(x^*) - V_{x_T}(x^*) $$ æ‰€ä»¥è¯´ $$ f(\\overline{x}) - f(x^*) \\leq \\frac{\\alpha}{2} \\rho^2 + \\frac{\\Theta}{\\alpha T} $$ ä»¤$\\alpha = \\sqrt{\\frac{2\\Theta}{T \\rho^2}}$.\næœ‰$f(x_T) - f(x^*) \\leq \\sqrt{\\frac{2\\Theta}{T }}\\rho= \\epsilon$ äºæ˜¯æˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„æ”¶æ•›ç‡ $$ T = \\Omega \\left( \\frac{\\rho^2}{\\epsilon^2} \\right) $$\n1.6 Linear Coupling Wishful Thinking æˆ‘ä»¬é€šè¿‡1.5çš„åˆ†æå·²ç»çŸ¥é“Mirror Descentæœ‰ $ T = O\\left(\\frac{\\rho^2}{\\epsilon^2}\\right) $çš„æ”¶æ•›ç‡\nç„¶åæˆ‘ä»¬çŸ¥é“åœ¨GDä¸­ $$ f(x_{t+1}) - f(x_t) \\leq -\\frac{1}{2L} |\\nabla f(x_t)|^2 $$ æ‰€ä»¥è¯´åœ¨gradientæ¯”è¾ƒå¤§çš„æ—¶å€™: $$ |\\nabla f(x_t)| \u003e \\rho : \\Omega\\left(\\frac{L \\epsilon}{\\rho^2}\\right) \\text{ steps} $$\nåœ¨gradientæ¯”è¾ƒå°çš„æ—¶å€™MD:\n$$ |\\nabla f(x_t)| \u003c \\rho : \\Omega\\left(\\frac{\\rho^2}{\\epsilon^2}\\right) \\text{ steps} $$ æ‰€ä»¥æˆ‘ä»¬æƒ³èƒ½ä¸èƒ½åœ¨æ¢¯åº¦å¤§çš„æ—¶å€™è·‘GD,åœ¨æ¢¯åº¦å°çš„æ—¶å€™è·‘MD,è¿™æ ·ä¼šè·å¾—ä¸€ä¸ªæ›´å¥½çš„æ”¶æ•›ç‡\nCoupling:\n$$\\Omega ( \\max { \\frac{L \\epsilon}{\\rho^2}, \\frac{\\rho^2}{\\epsilon^2} })$$ å–$\\rho = (L \\epsilon^{3})^\\frac{1}{4}$: $$ \\Omega\\left(\\sqrt{\\frac{L}{\\epsilon}}\\right) \\text{ steps} $$\nAlgorithm åˆå§‹åŒ– $$x_0 = y_0 = z_0$$ æ¯ä¸€æ­¥æ›´æ–°,æ›´æ–°$x$: $$ x_{k+1} = \\tau z_k + (1 - \\tau) y_k $$ æ›´æ–°$y$: $$ y_{k+1} = \\arg \\min_{y \\in \\mathcal{Q}} { \\frac{L}{2} |y - x_{k+1}|^2 + \\langle \\nabla f(x_{k+1}), y - x_{k+1} \\rangle } $$\n$$ = x_{k+1} - \\frac{1}{L} \\nabla f(x_{k+1}) \\quad \\text{(GD step)} $$\næ›´æ–°$z$: $$ z_{k+1} = Mirror_{z_k} (\\alpha \\nabla f(x_{k+1})) $$ Convergence æ ¹æ®MDçš„åˆ†æ: $$ \\alpha \\langle \\nabla f(x_{k+1}), z_k - u \\rangle \\leq \\frac{\\alpha^2}{2} |\\nabla f(x_{k+1})|^2 + V_{z_k}(u) - V_{z_{k+1}}(u) $$ ç”±äº $$ f(x_{k+1}) - f(y_{k+1}) \\geq \\frac{1}{2L} |\\nabla f(x_{k+1})|^2$$ æ‰€ä»¥åŸå¼ $$ \\leq \\alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u) $$ åˆå› ä¸ºconvexity: $$ \\alpha (f(x_{k+1}) - f(u)) \\leq \\alpha \\langle \\nabla f(x_{k+1}), x_{k+1} - u \\rangle $$\n$$ = \\alpha \\langle \\nabla f(x_{k+1}), z_k - u \\rangle + \\alpha \\langle \\nabla f(x_{k+1}), x_{k+1} - z_k \\rangle $$ å‰é¢ä¸€é¡¹æˆ‘ä»¬å·²ç»MDåšæ‰äº†,åé¢ä¸€é¡¹ $$ \\alpha \\langle \\nabla f(x_{k+1}), x_{k+1} - z_k \\rangle $$\n$$ = \\frac{(1 - \\tau) \\alpha}{\\tau} \\langle \\nabla f(x_{k+1}), y_k - x_{k+1} \\rangle $$\n$$ \\leq \\frac{(1 - \\tau) \\alpha}{\\tau} (f(y_k) - f(x_{k+1})) $$ æ‰€ä»¥è¯´ $$ \\alpha (f(x_{k+1}) - f(u)) \\leq \\alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + \\frac{(1 - \\tau) \\alpha}{\\tau} (f(y_k) - f(x_{k+1})) $$\n$$+ V_{z_k}(u) - V_{z_{k+1}}(u) $$ ä»¤ $ \\frac{(1 - \\tau) \\alpha}{\\tau} = \\alpha^2 L $, æœ‰ $$ f(x_{k+1}) - f(u) \\leq \\alpha^2 L (f(y_k) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u) $$\nTelescope:\n$$ \\alpha T (f(\\overline{x}) - f(x^*)) \\leq \\alpha^2 L (f(y_0) - f(y_T)) + V_{x_0}(x^*) - V_{z_T}(x^*) $$\nå‡è®¾ $f(y_0) - f(x^*) = d$, $V_{x_0}(x^*) = \\Theta$ æœ‰ $$ f(x_i) - f(x^*) \\leq \\frac{\\alpha dL}{T} + \\frac{\\Theta}{\\alpha T} $$ ä»¤$ \\alpha = \\sqrt{\\frac{\\Theta}{dL}}$, æœ‰ $$ f(\\overline{x}) - f(x^*) \\leq \\frac{2 \\sqrt{\\Theta Ld}}{T}$$\nå– $ T = 4 \\sqrt{\\frac{L\\Theta}{d}}$, æœ‰$$f(\\overline{x})-f(x^*)\\leq \\frac{d}{2}$$ æ‰€ä»¥è¯´æˆ‘ä»¬æ¯ $2\\epsilon\\rightarrow \\epsilon$è¿‡ç¨‹é‡æ–°è°ƒæ•´ä¸€æ¬¡$\\tau,\\alpha$,æœ€åå¾—åˆ°çš„è¿­ä»£æ¬¡æ•°æ˜¯: $$O(\\sqrt{\\frac{L \\Theta}{\\epsilon}})+O(\\sqrt{\\frac{L \\Theta}{2\\epsilon}})+O(\\sqrt{\\frac{L \\Theta}{4\\epsilon}})+â€¦=O(\\sqrt{\\frac{L \\Theta}{\\epsilon}})$$ Nesterovå‘Šè¯‰æˆ‘ä»¬$O(\\frac{1}{T^2})$(aka.$O(\\sqrt{\\frac{L}{\\epsilon}})$)å°±æ˜¯æˆ‘ä»¬å¯¹äºconvexä¸”L-smoothå‡½æ•°èƒ½å¾—åˆ°çš„æœ€å¥½ç»“æœäº†,æ‰€ä»¥Linear Couplingç¡®å®å¾ˆç‰›ã€‚\n1.7 Non-Convex Optimization Matrix Completion $A \\in \\mathbb{R}^{m \\times n}$æ»¡è¶³ä»¥ä¸‹å‡è®¾:\n1Â° $A$ is low rank\n2Â° Known entries are uniformly distributed\n3Â° Incoherence: $$ A = U \\Sigma V^T \\quad \\text{for } i \\in [n], j \\in [m]$$ $$\\exists \\mu: 1 \\leq \\mu \\leq \\frac{min(m,n)}{r}$$$$ |e_i^T U| \\leq \\sqrt{\\frac{\\mu r}{n}}, \\quad |e_j^T V| \\leq \\sqrt{\\frac{\\mu r}{m}}$$\né‚£ä¹ˆæˆ‘ä»¬çš„ç›®æ ‡($P_\\Omega$ä»£è¡¨ä¸çŸ¥é“çš„å…ƒç´ éƒ½maskæ‰): $$ \\min |P_\\Omega(UV^T) - P_\\Omega(A)|_F^2 $$ å¯ä»¥æœ‰ä»¥ä¸‹ç®—æ³•:\nAlgorithm:\nFor $t = 0, 1, 2, \\ldots, T$\n$V^{t+1} \\leftarrow \\arg \\min_V ||P_{\\Omega}(U^t V) - P_{\\Omega}(A)||_F^2$ $U^{t+1} \\leftarrow \\arg \\min_U ||P_{\\Omega}(U V^{t}) - P_{\\Omega}(A)||_F^2$ Escaping Saddle Points SGDåœ¨éå‡¸ä¼˜åŒ–ä¸­æœ‰ä¸€äº›GDä¹‹ç±»ç®—æ³•æ²¡æœ‰çš„å¥½å¤„,è¿™å°±æ˜¯å™ªå£°æ‰€å¸¦æ¥çš„éšæœºæ€§æ‰€å±•ç°çš„ä¼˜åŠ¿:\nThm.If ğ¿ is smooth, bounded and strict saddle (actually more general version, applies to points with small gradients, rather than zero gradients), and Hessian is smooth. If SGD noise has non-negligible variance in every direction with constant probability, SGD will escape all saddle points and local maxima, converge to a local minimum after polynomial number of steps.\nå…¶ä¸­Strict Saddle Pointæ˜¯æŒ‡ä¸€ä¸ªç‚¹$\\nabla f(x)=0$, $\\nabla^2 f(x)$åˆæœ‰æ­£ç‰¹å¾å€¼åˆæœ‰è´Ÿç‰¹å¾å€¼ã€‚Flat Saddle Pointæ˜¯æŒ‡ä¸€ä¸ªç‚¹$\\nabla f(x)=0$, $\\nabla^2 f(x)$çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½å¤§äºç­‰äº0,ä¸”æœ‰ä¸€ä¸ªç­‰äº0çš„ç‰¹å¾å€¼ã€‚\n2.Generalization 2.1 No Free Lunch Thm. Thm. è®¾ $A$ ä¸ºåœ¨å®šä¹‰åŸŸ $\\mathcal{X}$ ä¸Šç›¸å¯¹äº 0-1 æŸå¤±çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡çš„ä»»æ„å­¦ä¹ ç®—æ³•ã€‚è®¾ $m$ ä¸ºå°äº $|\\mathcal{X}|/2$ çš„ä»»æ„æ•°,è¡¨ç¤ºè®­ç»ƒé›†å¤§å°ã€‚åˆ™å­˜åœ¨ä¸€ä¸ªåœ¨ $\\mathcal{X} \\times {0, 1}$ ä¸Šçš„åˆ†å¸ƒ $\\mathcal{D}$ ä½¿å¾—ï¼š\nå­˜åœ¨ä¸€ä¸ªå‡½æ•° $f : \\mathcal{X} \\to {0, 1}$,ä½¿å¾— $L_\\mathcal{D}(f) = 0$ã€‚ ä»¥è‡³å°‘ $1/7$ çš„æ¦‚ç‡,å¯¹äºä» $\\mathcal{D}^m$ ä¸­é€‰å–çš„ $S$,æœ‰ $L_\\mathcal{D}(A(S)) \\geq 1/8$ã€‚ è¿™ä¸ªçš„ç›´è§‰åœ¨äºç”±Markovä¸ç­‰å¼,$\\mathbb{E}_{S \\sim D^m }[L_D(A(S))]\\geq \\frac{1}{4}$,ä¹Ÿå°±æ˜¯è¯´å¯¹äºä¸€ä¸ªå®Œå…¨é èƒŒè¯µçš„ç®—æ³•: å‡å¦‚è§è¿‡$(X,y)$,è¾“å‡º$y$,å‡å¦‚æ²¡è§è¿‡å°±éšæœºè¾“å‡º0æˆ–1ã€‚è¿™æ ·å¯¹äºä¸€ä¸ª$|C|=2m$çš„$X$çš„å­é›†,è¿™æ ·â€œèƒŒè¯µ+çè’™â€çš„loss functionæ˜¯$\\frac{1}{4}$ã€‚ä¹Ÿå°±æ˜¯è¯´,æ²¡æœ‰ä»€ä¹ˆåŠæ³•èƒ½å¤Ÿä»æœŸæœ›ä¸Šæ¯”â€œèƒŒè¯µ+çè’™â€æ•ˆæœæ›´å¥½,ä¹Ÿå°±æ˜¯è¯´å­¦ä¹ ç®—æ³•å¤±è´¥äº†ã€‚\nè¯æ˜:\nä¸ºäº†ç®€æ´æ€§,ä¸å¦¨è®¾$|C| = 2m$.\nè®°$T = 2^{2m}$ã€‚ä»$C$åˆ°${0, 1}$çš„å‡½æ•°ä¸€å…±æœ‰$f_1, \\ldots, f_T$,å…±$T$ä¸ª\nè®° $$ D_i({x, y}) = \\frac{1}{|C|} \\quad \\text{if } y = f_i(x) $$ $$ D_i({x, y}) = 0 \\quad \\text{otherwise.} $$\næ˜¾ç„¶,$L_{D_i}(f_i) = 0$.\næˆ‘ä»¬æ¥ä¸‹æ¥è¯æ˜:\n$$\\max_{i \\in [T]} E_{S \\sim D_{i}^{m}} [ L_{D_i}(A(S)) ] \\geq \\frac{1}{4}$$\nè®°ä¸€å…±æœ‰$k$ä¸ªå¯èƒ½çš„ä»$C$ä¸­å–æ ·å‡ºçš„$m$ä¸ªæ•°æ®ç‚¹$x_i$åºåˆ—: æœ‰$k = (2m)^m$,è®° $S_j = (x_1, \\ldots, x_m)$ ,è®° $S_j^i = \\left( (x_1, f_i(x_1)), \\ldots, (x_m, f_i(x_m)) \\right)$ã€‚\næˆ‘ä»¬åªéœ€è¦å–å‡ºä¸€ä¸ª$i \\in [T]$èƒ½å¤Ÿè®©$E_{S \\sim D_i^m} \\left[ L_{D_i}(A(S)) \\right]\\geq \\frac{1}{4}$,é‚£ä¹ˆå¯¹åº”çš„$D_i$ä¾¿æ˜¯æˆ‘ä»¬åœ¨NFLä¸­æ‰€å¸Œæœ›æ‰¾åˆ°çš„$D$ã€‚ $$ \\max_{i \\in [T]} E_{S \\sim D_i^m} \\left[ L_{D_i}(A(S)) \\right] $$\n$$ = \\max_{i \\in [T]} \\frac{1}{k} \\sum_{j=1}^k L_{D_i}(A(S_j^i)) $$\n$$ \\geq \\frac{1}{T} \\sum_{i=1}^T \\frac{1}{k} \\sum_{j=1}^k L_{D_i}(A(S_j^i)) $$\n$$ = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{T} \\sum_{i=1}^T L_{D_i}(A(S_j^i)) $$\n$$ \\geq \\min_{j \\in [k]} \\frac{1}{T} \\sum_{i=1}^T L_{D_i}(A(S_j^i)) $$ å¯¹äºç»™å®šçš„ $j$:\nä»¤$v_1, \\ldots, v_p$ ä¸º$S_j$ä¸­æ²¡æœ‰å‡ºç°çš„$x\\in C$, æ³¨æ„åˆ°$p \\geq m$ã€‚\n$$ L_{D_i}(A(S_j^i)) = \\frac{1}{2m} \\sum_{x \\in C} \\mathbf{1}[h(x) \\neq f_i(x)] $$\n$$ \\geq \\frac{1}{2m} \\sum_{r=1}^p \\mathbf{1}[h(v_r) \\neq f_i(v_r)] $$\n$$ \\geq \\frac{1}{2p} \\sum_{r=1}^p \\mathbf{1}[h(v_r) \\neq f_i(v_r)] $$\næ‰€ä»¥è¯´\n$$ \\frac{1}{T} \\sum_{i=1}^T L_{D_i}(A(S_j^i)) $$\n$$ \\geq \\frac{1}{T} \\sum_{i=1}^T \\frac{1}{2p} \\sum_{r=1}^p \\mathbf{1}[h(v_r) \\neq f_i(v_r)] $$\næˆ‘ä»¬å¯ä»¥å°† $f_1, \\ldots, f_T$ ä¸­çš„æ‰€æœ‰å‡½æ•°åˆ’åˆ†æˆ $T/2$ å¯¹ä¸ç›¸äº¤çš„å‡½æ•°å¯¹,å…¶ä¸­å¯¹äºæ¯ä¸€å¯¹ $(f_i, f_{iâ€™})$,å¯¹äºä»»æ„ $c \\in C$,éƒ½æœ‰ $f_i(c) \\neq f_{iâ€™}(c)$ã€‚\näºæ˜¯æœ‰ $$ \\frac{1}{2p} \\sum_{r=1}^p \\frac{1}{T} \\sum_{i=1}^T \\mathbf{1}[h(v_r) \\neq f_i(v_r)] = \\frac{1}{4} $$\næ‰€ä»¥è¯´ $$ \\max_{i \\in [T]} E_{S \\sim D_i^m} \\left[ L_{D_i}(A(S)) \\right] \\geq \\frac{1}{4} $$\nä»¤$\\mathcal{D} = D_i$:\nå¦‚æœ\n$$ \\Pr \\left[ L_{\\mathcal{D}}(A(S)) \\geq \\frac{1}{8} \\right] \u003c \\frac{1}{7} $$\né‚£ä¹ˆ\n$$ E_{S \\sim \\mathcal{D}^m} \\left[ L_{\\mathcal{D}}(A(S)) \\right] \u003c \\frac{1}{7} \\cdot 1 + \\frac{6}{7} \\cdot \\frac{1}{8} $$\n$$ = \\frac{1}{7} + \\frac{3}{28} = \\frac{1}{4}.\\quad\\blacksquare $$\n2.2 PAC-Learning ä¸€äº›æ¦‚å¿µ: Hypothesis Class (H) ï¼šèƒ½å¤Ÿé€‰æ‹©çš„å‡è®¾$h$çš„é›†åˆ $ERM_H$ ï¼šé€‰æ‹©å…·æœ‰æœ€å°empirical lossçš„å‡è®¾ $$ ERM_H(S) \\in \\arg\\min_{h \\in H} L_S(h) $$\nRealizability Assumption: å­˜åœ¨ $h^* \\in H$ ä½¿å¾— $L_{D,f}(h^*) = 0$ã€‚è¿™æ„å‘³ç€å¯¹äºæ¯ä¸ªè®­ç»ƒé›† $S$,æˆ‘ä»¬æœ‰ $L_S(h^*) = 0$ã€‚ PAC-Learnable: å¦‚æœå­˜åœ¨ä¸€ä¸ªå‡½æ•° $m_H: (0,1)^2 \\to \\mathbb{N}$ å’Œä¸€ä¸ªlearning algorithm,ä½¿å¾—å¯¹äºä»»æ„çš„ $\\epsilon, \\delta \\in (0,1)$,å¯¹äºå®šä¹‰åœ¨ $X$ ä¸Šçš„ä»»æ„åˆ†å¸ƒ $D$,ä»¥åŠä»»æ„labeling function $f: X \\to {0,1}$,è‹¥Realizability Assumptionåœ¨ $H, D, f$ ä¸‹æˆç«‹,åˆ™å½“åœ¨ç”± $D$ ç”Ÿæˆå¹¶ç”± $f$ æ ‡è®°çš„ $m \\geq m_H(\\epsilon, \\delta)$ ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒæ ·æœ¬ä¸Šè¿è¡Œè¯¥learning algorithmæ—¶,è¯¥ç®—æ³•è¿”å›ä¸€ä¸ªå‡è®¾ $h$,ä½¿å¾—ä»¥è‡³å°‘ $1 - \\delta$ çš„æ¦‚ç‡ï¼ˆåœ¨æ ·æœ¬é€‰æ‹©çš„éšæœºæ€§ä¸Šï¼‰,$L_{D,f}(h) \\leq \\epsilon$ã€‚ Finite Classes are PAC-learnable Thm. ç»™å®š $\\delta \\in (0,1)$, $\\epsilon \u003e 0$, å¦‚æœ $m \\geq \\frac{\\log(|H|/\\delta)}{\\epsilon}$,é‚£ä¹ˆå¦‚æœRealizability Assumptionæˆç«‹, é‚£ä¹ˆå¯¹äºä»»æ„ERM hypothesis $h_S$: $$ \\Pr [ L_D(h_S) \\leq \\epsilon ] \\geq 1 - \\delta $$ Pf. æˆ‘ä»¬æƒ³è¦upper bound\n$$ \\Pr_{S\\sim \\mathcal{D}^m} [ S | L_D(h(S)) \u003e \\epsilon ] $$\nå®šä¹‰æ‰€æœ‰ä¸å¥½çš„å‡è®¾çš„é›†åˆä¸º: $$ H_B := { h \\in H | L_D(f, h) \u003e \\epsilon } $$ å®šä¹‰misleadingçš„å‡è®¾çš„é›†åˆä¸ºï¼š $$ M := { S \\mid \\exists h \\in H_B, L_S(h) = 0 } $$ æœ‰ $$ { S \\mid L_D(h(S)) \u003e \\epsilon } \\subseteq M $$ æ‰€ä»¥ $$ \\Pr \\left[ L_D(h(S)) \u003e \\epsilon \\right] \\leq \\Pr \\left[ S \\in M \\right] \\leq \\sum_{h \\in H_B} \\Pr \\left[ L_S(h) = 0 \\right] $$ åˆå› ä¸º $$ \\Pr \\left[ L_S(h) = 0 \\right] = \\prod_{i=1}^m Pr_{x_i\\sim\\mathcal{D}} \\left[ h(x_i) = f(x_i) \\right] $$\nå› ä¸º $$ Pr_{x_i\\sim\\mathcal{D}} \\left[ h(x_i) = f(x_i) \\right] = 1 - L_D(f, h) \\leq 1 - \\epsilon $$ æ‰€ä»¥\n$$ \\Pr \\left[ L_S(h) = 0 \\right] \\leq (1 - \\epsilon)^m \\leq e^{-m \\epsilon} $$\n$$ |H| \\cdot e^{-m \\epsilon} \\leq \\delta \\implies m = \\frac{\\log(|H|/\\delta)}{\\epsilon}. \\blacksquare $$\nThreshold Functions are PAC-learnable Threshold Functions: $$ \\mathcal{H}={h(x) = \\mathbf{1}[x \u003c a]} $$ æ³¨æ„åˆ°è¿™æ˜¯ä¸€ä¸ªinfinite classã€‚ Thm. è®¾ $H$ ä¸ºThreshold Functionsã€‚åˆ™ $H$ æ˜¯ PAC-learnableçš„,ä½¿ç”¨ ERM ç®—æ³•,å…¶æ ·æœ¬å¤æ‚åº¦ä¸º$$ m_H(\\epsilon, \\delta) \\leq \\frac{\\lceil \\log(2/\\delta) \\rceil}{\\epsilon}$$\nPf. è®°$ h^*(x) = \\mathbf{1}[x \u003c a^*] $s.t.$L_D(h^*)=0$\nå®šä¹‰ $$ b_0 := \\sup {x \\mid (x, 1) \\in S}, \\quad b_1 := \\inf {x \\mid (x, 0) \\in S} $$ æ³¨æ„åˆ° $$ \\Pr \\left[ L_D(h) \u003e \\epsilon \\right] \\leq \\Pr \\left[ b_0 \u003c a_0 \\right] + \\Pr \\left[ b_1 \u003e a_1 \\right] $$ åœ¨$ m = \\frac{\\ln \\left(\\frac{2}{\\delta}\\right)}{\\epsilon} $çš„æƒ…å†µä¸‹: $$ \\Pr \\left[ b_0 \u003c a_0 \\right] = (1 - \\epsilon)^m \\leq e^{-\\epsilon m} = \\frac{\\delta}{2} $$\n$$ \\Pr \\left[ b_1 \u003e a_1 \\right] = (1 - \\epsilon)^m \\leq e^{-\\epsilon m} = \\frac{\\delta}{2}. \\blacksquare $$\n2.3 Agnostic PAC-Learnable æœ‰æ—¶å€™Realizability Assumptionå¤ªå¼ºäº†,æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªåœ¨$\\mathcal{H}$ä¸­æ²¡æœ‰Loss=0çš„hypothesisçš„æƒ…å†µä¸‹è¡¡é‡estimation errorçš„æ‰‹æ®µ:\nAgnostic PAC-Learnable: ä¸€ä¸ªå‡è®¾ç±» $H$ æ˜¯ Agnostic PAC å¯å­¦ä¹ çš„,å¦‚æœå­˜åœ¨ä¸€ä¸ªå‡½æ•° $m_H: (0,1)^2 \\rightarrow \\mathbb{N}$ å’Œä¸€ä¸ªå…·æœ‰ä»¥ä¸‹æ€§è´¨çš„å­¦ä¹ ç®—æ³•ï¼šå¯¹äºæ¯ä¸€ä¸ª $\\epsilon, \\delta \\in (0,1)$,ä»¥åŠå®šä¹‰åœ¨ $X \\times Y$ ä¸Šçš„æ¯ä¸ªåˆ†å¸ƒ $D$,å½“åœ¨ç”± $D$ ç”Ÿæˆçš„ $m \\geq m_H(\\epsilon, \\delta)$ ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰æ ·æœ¬ä¸Šè¿è¡Œè¯¥å­¦ä¹ ç®—æ³•æ—¶,ç®—æ³•ä¼šè¿”å›ä¸€ä¸ªå‡è®¾ $h$,ä½¿å¾—ä»¥è‡³å°‘ $1 - \\delta$ çš„æ¦‚ç‡ï¼ˆå¯¹äº $m$ ä¸ªè®­ç»ƒæ ·æœ¬çš„é€‰æ‹©è€Œè¨€ï¼‰,æ»¡è¶³\n$$ L_D(h) \\leq \\min_{hâ€™ \\in H} L_D(hâ€™) + \\epsilon $$\nError Decomposition: $L_D(h_S) = \\epsilon_{app} + \\epsilon_{est}$ $\\epsilon_{app} = \\min_{h \\in H} L_D(h)$ $\\epsilon_{est} = L_D(h_S) - \\epsilon_{app}$ $\\epsilon_{app} = L_D(BO) + \\min_{h \\in H} L_D(h) - L_D(BO)$ $\\epsilon_{app}$æè¿°çš„æ˜¯è¿™ä¸ªhypothesis classçš„inductive biasçš„å¤šå°‘,è€Œ$\\epsilon_{est}$æ˜¯ä¸sample sizeå’Œsample complexityç›¸å…³çš„(sample complexityä¸hypothesis classçš„representation poweræˆæ­£æ¯”),æ‰€ä»¥è¯´å½“æˆ‘ä»¬æƒ³è¦å‡å°‘$L_D(h_S)$,æˆ‘ä»¬é¢ä¸´ä¸€ä¸ªbias-complexity tradeoffã€‚\nå…¶ä¸­BOæŒ‡ä»£çš„æ˜¯Bayes Optimal Predictorã€‚\nBayes Optimal Predictor ç»™å®šä»»ä½•åœ¨ $X \\times {0,1}$ ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ $D$,ä» $X$ åˆ° ${0,1}$ çš„æœ€ä½³æ ‡ç­¾é¢„æµ‹å‡½æ•°ä¸º\n$$ f_D(x) = 1 \\quad \\text{if } P[y = 1 \\mid x] \\geq \\frac{1}{2} $$ $$ f_D(x) = 0 \\quad \\text{otherwise} $$\nå¾ˆå®¹æ˜“éªŒè¯,å¯¹äºæ¯ä¸ªæ¦‚ç‡åˆ†å¸ƒ $D$,è´å¶æ–¯æœ€ä¼˜é¢„æµ‹å™¨ $f_D$ æ˜¯æœ€ä¼˜çš„,å› ä¸ºæ²¡æœ‰å…¶ä»–åˆ†ç±»å™¨ $g: X \\rightarrow {0,1}$ çš„é”™è¯¯ç‡æ›´ä½ã€‚å³,å¯¹äºæ¯ä¸ªåˆ†ç±»å™¨ $g$,æœ‰\n$$ L_D(f_D) \\leq L_D(g) $$\n2.4 VC-Dim Restriction of $H$ to $C$ è®¾ $H$ æ˜¯ä» $X$ åˆ° ${0,1}$ çš„å‡½æ•°ç±»,$C = {c_1, \\cdots, c_m} \\subseteq X$ã€‚$H$ åœ¨ $C$ ä¸Šçš„é™åˆ¶æ˜¯ä» $C$ åˆ° ${0,1}$ çš„å‡½æ•°é›†åˆ,è¿™äº›å‡½æ•°å¯ä»¥ä» $H$ ä¸­å¯¼å‡ºã€‚å³\n$$ H_C = {(h(c_1), \\cdots, h(c_m)) : h \\in H} $$\næˆ‘ä»¬å°†ä» $C$ åˆ° ${0,1}$ çš„æ¯ä¸ªå‡½æ•°è¡¨ç¤ºä¸º ${0,1}^{|C|}$ ä¸­çš„ä¸€ä¸ªå‘é‡ã€‚\nShattering ä¸€ä¸ªå‡è®¾ç±» $H$ Shatteræœ‰é™é›† $C \\subseteq X$,å¦‚æœ Restriction of $H$ to $C$æ˜¯ä» $C$ åˆ° ${0,1}$ çš„æ‰€æœ‰å‡½æ•°é›†åˆã€‚å³\n$$ |H_C| = 2^{|C|} $$\nNFL Reexpressed è®¾ $H$ æ˜¯ä» $X$ åˆ° ${0,1}$ çš„hypothesis classã€‚ä»¤ $m$ ä¸ºè®­ç»ƒé›†å¤§å°ã€‚å‡è®¾å­˜åœ¨ä¸€ä¸ªå¤§å°ä¸º $2m$ çš„é›†åˆ $C \\subseteq X$,å®ƒè¢« $H$ shatterã€‚åˆ™å¯¹äºä»»æ„å­¦ä¹ ç®—æ³• $A$,å­˜åœ¨ä¸€ä¸ªå®šä¹‰åœ¨ $X \\times {0,1}$ ä¸Šçš„åˆ†å¸ƒ $D$ å’Œä¸€ä¸ªé¢„æµ‹å™¨ $h \\in H$,ä½¿å¾— $L_D(h) = 0$,ä½†ä»¥è‡³å°‘ $\\frac{1}{7}$ çš„æ¦‚ç‡,å¯¹äº $S \\sim D^m$ çš„é€‰æ‹©,æœ‰\n$$ L_D(A(S)) \\geq \\frac{1}{8} $$\nVC-Dimension Hypothesis class $H$ çš„ VC-dimensionï¼ˆè®°ä½œ $\\text{VCdim}(H)$ï¼‰æ˜¯ $H$ å¯ä»¥shatterçš„é›†åˆ $C \\subseteq X$ çš„æœ€å¤§å¤§å°ã€‚å¦‚æœ $H$ å¯ä»¥shatterä»»æ„å¤§çš„é›†åˆ,æˆ‘ä»¬ç§° $\\text{VCdim}(H)=+ \\infty$.\nInifite VC-dim hypothesis classes are not PAC-learnable NFLçš„ç›´æ¥åæœå°±æ˜¯$\\text{VCdim}(H)=+ \\infty$çš„$H$ä¸æ˜¯PAC-learnableçš„ã€‚\n2.5 Fundamental theorem of statistical learning è®¾ $H$ æ˜¯ä»ä¸€ä¸ªåŸŸ $X$ åˆ° ${0,1}$ çš„hypothesis class,å¹¶ä¸”æŸå¤±å‡½æ•°æ˜¯ 0-1 æŸå¤±ã€‚å‡è®¾ $\\text{VCdim}(H) = d \u003c \\infty$ã€‚åˆ™å­˜åœ¨å¸¸æ•° $C_1, C_2$,ä½¿å¾—\n$H$ æ˜¯å…·æœ‰ä»¥ä¸‹æ ·æœ¬å¤æ‚åº¦çš„Agnostic PAC-learnableï¼š\n$$ C_1 \\frac{d + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon^2} \\leq m_H(\\epsilon, \\delta) \\leq C_2 \\frac{d + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon^2} $$\n$H$ æ˜¯å…·æœ‰ä»¥ä¸‹æ ·æœ¬å¤æ‚åº¦çš„ PAC-learnableï¼š\n$$ C_1 \\frac{d + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon} \\leq m_H(\\epsilon, \\delta) \\leq C_2 \\frac{d \\log \\left(\\frac{1}{\\epsilon}\\right) + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon} $$\n3.Supervised Learning å¯¹äºå›å½’é—®é¢˜,æˆ‘ä»¬æ„é€ ä¸€ä¸ªå‡½æ•°$f: X\\rightarrow \\mathbb{R}$ åœ¨åˆ†ç±»é—®é¢˜ä¸­,æˆ‘ä»¬æ„é€ ä¸€ä¸ªå‡½æ•°$f: X\\rightarrow {0,1}$æˆ–è€…${-1,1}$ã€‚ å‰è€…æˆ‘ä»¬çš„loss functionå¾ˆå¥½design,æ¯”å¦‚è¯´Mean Square Loss,ä½†æ˜¯åè€…çš„losså°±ä¸æ˜¯ç‰¹åˆ«å¥½designã€‚ä¸€ç§è‡ªç„¶çš„æƒ³æ³•æ˜¯$f(x)=sign(w^Tx)$,ä½†æ˜¯é—®é¢˜å°±æ˜¯è¿™ä¸ªlossä¸å¯å¯¼,ä¸‹é¢æ˜¯ä¸€ç§åˆ©ç”¨è¿™ç§å‡½æ•°ä½†æ˜¯ä¸éœ€è¦å¯¼æ•°çš„è¿œå¤ç®—æ³•ã€‚\n3.1 Perceptron Algorithm Convergence Thm.\nåˆé€‚ç¼©æ”¾ä½¿å¾— $||x_i|| \\leq 1$ ã€‚å‡è®¾å­˜åœ¨ $w_*$ æ»¡è¶³ $||w_*|| = 1$ ä¸” $y_i w_*^T x_i \u003e \\gamma$ï¼ˆå­˜åœ¨è¿‡åŸç‚¹çš„åˆ’åˆ†å¹³é¢,å®‰å…¨è·ç¦»ä¸º $\\gamma$ï¼‰ã€‚è¯¥ç®—æ³•æ”¶æ•›å‰æœ€å¤šè§¦å‘ $\\frac{1}{\\gamma^2}$ æ¬¡é¢„æµ‹é”™è¯¯ã€‚\nPf. å‡è®¾ç®—æ³•ç¬¬ $t$ æ¬¡çŠ¯é”™æ˜¯ $(x_t, y_t)$,è¿™ä¼šä½¿å¾—\n$$w_{t+1} = w_t + y_t x_t$$\nä¸”æ­¤æ—¶ $\\langle {w}^T, y_t x_t \\rangle \u003c 0$ï¼ˆé”è§’ï¼‰ã€‚è¿™è¯´æ˜ $$ ||w_{t+1}||^2 \\leq ||w_t||^2 + ||y_t x_t||^2 = ||w_t||^2 + 1 \\ ||w_t||^2 \\leq t $$\nå¦ä¸€æ–¹é¢ $$ ||w_{t+1}|| \\geq \\langle w_{t+1}, w_* \\rangle \\geq \\langle w_t, w_* \\rangle + \\gamma \\ ||w_t|| \\geq \\gamma t $$\nç»¼ä¸Š $$ \\gamma^2 t^2 \\leq |w_t|^2 \\leq t $$ è§£å¾— $t \\leq \\frac{1}{\\gamma^2}$ã€‚$\\blacksquare$\n3.2 Logistic Regression ä¸ºäº†è§£å†³ä¸å¯å¯¼çš„é—®é¢˜,æ›´ä¸ºç°ä»£çš„æƒ³æ³•æ˜¯é€šè¿‡sigmoidå‡½æ•°æŠŠ$w^Tx$å‹ç¼©åˆ°$(0,1)$ä¹‹é—´çš„æ¦‚ç‡,å³$$f(x)=\\frac{1}{1+e^{-w^Tx}}.$$ è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡ä¹‹é—´çš„å·®å¼‚,å¯ä»¥ç”¨l1-normæˆ–è€…cross-entropy lossã€‚\nç†µ (Entropy) å¯¹äºç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \\cdots, p_n)$,å®šä¹‰å®ƒçš„ç†µä¸º$$ H(p) = \\sum_{i=1}^{n} p_i \\log \\frac{1}{p_i}$$\näº¤å‰ç†µ (Cross entropy) å®šä¹‰ä¸¤ä¸ªç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \\cdots, p_n)$ å’Œ $(q_1, q_2, \\cdots, q_n)$ çš„äº¤å‰ç†µä¸º$$ XE(p, q) = \\sum_{i=1}^{n} p_i \\log \\frac{1}{q_i}$$\nKL æ•£åº¦ å®šä¹‰ä¸¤ä¸ªç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \\cdots, p_n)$ å’Œ $(q_1, q_2, \\cdots, q_n)$ çš„ KL æ•£åº¦ä¸º$$ KL(p, q) = XE(p, q) - H(p)$$\näº¤å‰ç†µæ¯”l1-norm å¥½åœ¨ï¼š\nl1-normï¼šæä¾›æ’å®šçš„æ¢¯åº¦ã€‚ äº¤å‰ç†µï¼šå·®è·è¶Šå¤§,æ¢¯åº¦è¶Šå¤§ 3.3 Regularization å½“æˆ‘ä»¬æƒ³è¦é™åˆ¶$f$çš„è¡¨è¾¾èƒ½åŠ›æ—¶,ç»å…¸çš„çœ‹æ³•å°±æ˜¯é€šè¿‡åœ¨$||Â·||_2$æˆ–$||Â·||_1$æ„ä¹‰ä¸‹é™åˆ¶$w$çš„å¯èƒ½å–å€¼åŒºé—´ã€‚\nRidge Regression æŠŠloss functionæ”¹ä¸º$$l(w)+\\lambda||w||^2$$ è¿™é‡Œæ˜¯2-norm, è¿™ç›¸å½“äºæ¯ä¸€æ­¥å…ˆGD,ä¹‹åå†è¿›è¡Œäº†ä¸€æ¬¡ $$w_{t+1}=(1-\\eta \\lambda)\\tilde{w}_{t}$$ è¿™è¢«ç§°ä¸ºweight decayã€‚\nLasso Regression æœ‰æ—¶å€™æˆ‘ä»¬æƒ³è¦è·å¾—sparseçš„è§£,å› æ­¤æˆ‘ä»¬æŠŠloss functionæ”¹ä¸º$$l(w)+\\lambda||w||_1^2$$ è¿™ä¸ªç›´è§‰åœ¨äºç”¨diamondå’Œå‡¸é›†çš„äº¤é›†æ›´æœ‰å¯èƒ½æ˜¯sparseçš„ 3.4 Compressed Sensing Nyquist theorem: for a signal with frequency ğ‘“, we need 2ğ‘“ sampling rate to fully reconstruct the signal\nè¿™ä¸ªæ˜¯ä¸€ä¸ªé€šç”¨çš„å®šç†,ä½†æ˜¯å¤§éƒ¨åˆ†æƒ…å†µä¸‹,æˆ‘ä»¬çš„ä¿¡å·å…¶å®æ˜¯å­˜åœ¨ä¸€ç»„åŸºä¸‹çš„ç¨€ç–è¡¨ç¤º,æ‰€ä»¥æˆ‘ä»¬ä¼šå»æƒ³èƒ½ä¸èƒ½é€šè¿‡æ›´å°‘çš„é‡‡æ ·,æ¥é‡æ„å‡ºä¿¡å·,è¿™å°±æ˜¯compressed sensingçš„èƒŒæ™¯ã€‚\nåœ¨Compressed Sensingä¸­,å’Œsupervised learningä¸åŒçš„æ˜¯æˆ‘ä»¬å¯ä»¥è‡ªå·±é€‰æ‹©è‡ªå·±çš„measurement matrix,å³è®­ç»ƒé›†,åœ¨ä¸‹å›¾ä¸­ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰å®š$A$çš„æ¯ä¸€è¡Œ,ç„¶åè·å¾—å¯¹åº”çš„$y$,æœ€ç»ˆæˆ‘ä»¬å¸Œæœ›é€šè¿‡$y$è¿˜åŸå‡º$x$ã€‚\næœ€åçš„å¾—åˆ°çš„ä¸»è¦ç»“è®º,ç”¨è‡ªç„¶è¯­è¨€å»æè¿°,æ˜¯å¦‚ä¸‹ä¸‰æ¡:\nå¦‚æœä¸€ä¸ªç¨€ç–ä¿¡å·é€šè¿‡ $x \\mapsto Wx$ è¿›è¡Œäº†å‹ç¼©,å…¶ä¸­ $W$ æ˜¯æ»¡è¶³$(\\epsilon, s)$-RIP çš„çŸ©é˜µ,é‚£ä¹ˆå¯ä»¥å®Œå…¨é‡æ„ä»»ä½•ç¨€ç–ä¿¡å·ã€‚æ»¡è¶³æ­¤æ€§è´¨çš„çŸ©é˜µä¿è¯äº†ä»»ä½•ç¨€ç–å¯è¡¨ç¤ºå‘é‡çš„èŒƒæ•°distortionè¾ƒå°ã€‚\né€šè¿‡æ±‚è§£çº¿æ€§è§„åˆ’,é‡æ„å¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å†…è®¡ç®—ã€‚\nç»™å®š $n \\times d$ çš„éšæœºçŸ©é˜µ,åœ¨ $n$ å¤§äº $s \\log(d)$ çš„æ•°é‡çº§æ—¶,å®ƒå¾ˆå¯èƒ½æ»¡è¶³ RIP æ¡ä»¶ã€‚\næ¥ä¸‹æ¥è®©æˆ‘formallyç”¨æ•°å­¦çš„è¯­è¨€build upéƒ½ä»¥ä¸Šçš„ç»“è®ºã€‚\nRIP-Condition ä¸€ä¸ªçŸ©é˜µ $W \\in \\mathbb{R}^{n,d}$ æ˜¯ $(\\epsilon, s)$-RIP çš„å½“ä¸”ä»…å½“å¯¹äºæ‰€æœ‰ $x \\neq 0$ ä¸”æ»¡è¶³ $||x||_{0}\\leq s$ çš„ $x$,æˆ‘ä»¬æœ‰ $$ \\left| \\frac{||Wx||_2^2}{||x||_2^2} - 1 \\right| \\leq \\epsilon. $$\nThm.1 Thm.1 è®¾ $\\epsilon \u003c 1$,å¹¶ä¸”è®¾ $W$ ä¸º $(\\epsilon, 2s)$-RIP çŸ©é˜µã€‚è®¾ $x$ ä¸ºä¸€ä¸ªæ»¡è¶³ $||x||_0\\leq s$ çš„å‘é‡,\nä»¤ $y = Wx$ ä¸º $x$ çš„å‹ç¼©ç»“æœ,å¹¶ä¸”ä»¤ $$\\tilde{x} \\in \\arg \\min_{{v}: W{v}=y} ||{v}||_0$$ ä¸ºé‡æ„å‘é‡ã€‚é‚£ä¹ˆ,$\\tilde{x} = x$ã€‚\nè¿™ä¸ªå®šç†å‘Šè¯‰æˆ‘ä»¬å¯¹äºRIPçš„çŸ©é˜µ,å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡æ‰¾åˆ°ç¬¦åˆ$Wv=y$çš„$v$çš„l0-normæœ€å°çš„å‘é‡,æˆ‘ä»¬å°±èƒ½å¤ŸæˆåŠŸçš„(æ— æŸ)é‡å»ºå‡º$x$ã€‚\nPf. ä»¤ $h = \\tilde{x} - x$\n$$ |\\tilde{x}|_0 \\leq |x|_0 \\leq s $$\nå› æ­¤ $h$ æ˜¯ $2s$-sparseçš„ã€‚\n$$ (1 - \\epsilon) |h|^2 \\leq |Wh|^2 \\leq (1 + \\epsilon) |h|^2 $$\nç”±äº $Wh = W(\\tilde{x} - x) = 0$\n$$ \\Rightarrow |h|^2 = 0 $$\nå› æ­¤ $\\tilde{x} = x$.$\\blacksquare$\nä½†é—®é¢˜æ˜¯,æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªpolytimeæ±‚è§£l0-normæœ€å°å€¼çš„ç®—æ³•,æ‰€ä»¥è¿™ä¸ªå®šç†åœ¨å®é™…åº”ç”¨ä¸­æ²¡æœ‰æ„ä¹‰,æˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­å°è¯•å§l0-norm relaxåˆ° l1-norm,ä¸‹é¢çš„thm2å’Œ3ä¾¿æ˜¯l1-normä¸‹é‡å»ºç»“æœç›¸ä¼¼æ€§çš„ä¿è¯ã€‚\nThm.2 Thm.2 å‡è®¾ $W$ ä¸º $(\\epsilon, 2s)$-RIP çŸ©é˜µã€‚$x$ ä¸ºä¸€ä¸ªæ»¡è¶³ $|x|_0 \\leq s$ çš„å‘é‡,\nä»¤ $y = Wx$ ä¸º $x$ çš„å‹ç¼©ç»“æœ,å¹¶ä¸” $\\epsilon \u003c \\frac{1}{1 + \\sqrt{2}}$,é‚£ä¹ˆ,\n$$x=\\arg \\min_{v: Wv = y} ||v||_ {0}=\\arg \\min_{v:Wv = y}||v||_1$$\nè¿™ä¸ªå®šç†è¯´æ˜åœ¨s-sparseçš„æƒ…å†µä¸‹,Relax åˆ°l1-normä¹Ÿå¯ä»¥é‡æ„å‡ºä¸€æ ·çš„å‘é‡ã€‚\näº‹å®ä¸Š,æˆ‘ä»¬å°†è¯æ˜ä¸€ä¸ªæ›´å¼ºçš„ç»“æœ,è¯¥ç»“æœå³ä½¿åœ¨ $x$ ä¸æ˜¯ä¸€ä¸ªç¨€ç–å‘é‡çš„æƒ…å†µä¸‹ä¹Ÿæˆç«‹,å³Thm.3ã€‚\nThm.3 Thm.3 è®¾ $\\epsilon \u003c \\frac{1}{1 + \\sqrt{2}}$ å¹¶ä¸” $W$ æ˜¯ä¸€ä¸ª $(\\epsilon, 2s)$-RIP çŸ©é˜µã€‚è®¾ $x$ æ˜¯ä»»æ„å‘é‡,å¹¶å®šä¹‰ $$x_s \\in \\arg \\min_{v: ||v|| _ 0 \\leq s} ||x - v||_ 1 $$ ä¹Ÿå°±æ˜¯è¯´,$x_s$ æ˜¯ä¸€ä¸ªåœ¨ $x$ çš„ $s$ ä¸ªæœ€å¤§å…ƒç´ å¤„ç­‰äº $x$ å¹¶åœ¨å…¶ä»–åœ°æ–¹ç­‰äº $0$ çš„å‘é‡ã€‚è®¾ $y = Wx$ ,å¹¶ä»¤ $$x^* \\in \\arg \\min_{v: Wv = y} |v|_1$$ ä¸ºé‡æ„çš„å‘é‡ã€‚é‚£ä¹ˆ, $$|x^* - x|_2 \\leq 2 \\frac{1 + \\rho}{1 - \\rho} s^{-1/2} |x - x_s|_1,$$ å…¶ä¸­ $\\rho = \\sqrt{2\\epsilon}/(1 - \\epsilon)$ã€‚\nPf.\nè¿™ä¸ªå®šç†çš„è¯æ˜ç›¸å¯¹æ¯”è¾ƒå¤æ‚,ä¸»è¦æ˜¯è¯æ˜ä»¥ä¸‹ä¸¤ä¸ªClaim:\nClaim 1ï¼š $$ |h_{T_{0,1}}|_ 2 \\leq |h _{T_0}|_2 + 2s^{-1/2}|x - x_s|_1ã€‚ $$\nClaim 2ï¼š $$ |h_{T_{0,1}}|_ 2 \\leq \\frac{2\\rho}{1 - \\rho}s^{-1/2}|x - x_s|_1ã€‚ $$ ç¬¦å·è¯´æ˜ï¼š ç»™å®šä¸€ä¸ªå‘é‡ $v$ å’Œä¸€ç»„ç´¢å¼• $I$,æˆ‘ä»¬ç”¨ $v_I$ è¡¨ç¤ºå‘é‡,å…¶ç¬¬ $i$ ä¸ªå…ƒç´ ä¸º $v_i$ å¦‚æœ $i \\in I$,å¦åˆ™å…¶ç¬¬ $i$ ä¸ªå…ƒç´ ä¸º 0ã€‚ä»¤ $h = x^* - x$ã€‚\næˆ‘ä»¬ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªæŠ€å·§æ˜¯å°†ç´¢å¼•é›†åˆ $[d] = {1, \\ldots, d}$ åˆ’åˆ†ä¸ºå¤§å°ä¸º $s$ çš„ä¸ç›¸äº¤é›†åˆã€‚ä¹Ÿå°±æ˜¯è¯´,æˆ‘ä»¬å†™ä½œ $[d] = T_0 \\cup T_1 \\cup T_2 \\ldots T_{d/s-1}$,å¯¹äºæ‰€æœ‰ $i$,æˆ‘ä»¬æœ‰ $|T_i| = s$,å¹¶ä¸”æˆ‘ä»¬ä¸ºç®€ä¾¿èµ·è§å‡è®¾ $d/s$ æ˜¯ä¸€ä¸ªæ•´æ•°ã€‚æˆ‘ä»¬å¦‚ä¸‹å®šä¹‰åˆ’åˆ†ã€‚åœ¨ $T_0$ ä¸­,æˆ‘ä»¬æ”¾ç½® $s$ ä¸ªå¯¹åº”äº $x$ çš„ç»å¯¹å€¼ä¸­æœ€å¤§çš„å…ƒç´ çš„ç´¢å¼•ï¼ˆå¦‚æœæœ‰å¹¶åˆ—çš„æƒ…å†µ,åˆ™ä»»æ„æ‰“ç ´å¹³å±€ï¼‰ã€‚è®¾ $T_0^c = [d] \\setminus T_0$ã€‚æ¥ä¸‹æ¥,$T_1$ å°†æ˜¯å¯¹åº”äº $h_{T_0^c}$ ç»å¯¹å€¼ä¸­æœ€å¤§çš„ $s$ ä¸ªå…ƒç´ çš„ç´¢å¼•ã€‚è®¾ $T_{0,1} = T_0 \\cup T_1$,å¹¶ä»¤ $T_{0,1}^c = [d] \\setminus T_{0,1}$ã€‚æ¥ä¸‹æ¥,$T_2$ å°†æ˜¯å¯¹åº”äº $h_{T_{0,1}^c}$ ç»å¯¹å€¼ä¸­æœ€å¤§çš„ $s$ ä¸ªå…ƒç´ çš„ç´¢å¼•ã€‚æˆ‘ä»¬å°†ç»§ç»­æ„é€  $T_3, T_4, \\ldots$ ä»¥ç›¸åŒçš„æ–¹å¼ã€‚\nPf of Claim 1,æˆ‘ä»¬ä¸ä½¿ç”¨RIPæ¡ä»¶,ä»…ä»…ä½¿ç”¨$x^*$æœ€å°åŒ–$\\ell_1$èŒƒæ•°è¿™ä¸€äº‹å®ã€‚è®¾$j \u003e 1$ã€‚å¯¹äºæ¯ä¸ª$i \\in T_j$å’Œ$iâ€™ \\in T_{j-1}$,æˆ‘ä»¬æœ‰$|h_i| \\leq |h_{iâ€™}|$ã€‚å› æ­¤,$|h_ {T_j}|_ \\infty \\leq |h_ {T_ {j-1}}|_ 1/s$ã€‚ç”±æ­¤å¯ä»¥å¾—åˆ°ï¼š\n$$ ||h_{T_j}||_ 2 \\leq s^{-1/2} ||h_{T_{j-1}}||_1 $$\nå¯¹$j = 2, 3, \\ldots$æ±‚å’Œ,å¹¶ä½¿ç”¨ä¸‰è§’ä¸ç­‰å¼,å¯ä»¥å¾—åˆ°ï¼š\n$$ ||h_{T_{0,1}^c}||_ 2 \\leq \\sum_{j \\geq 2} ||h_{T_j}||_ 2 \\leq s^{-1/2} ||h_{T_{0,1}^c}||_1 $$\næ¥ä¸‹æ¥,æˆ‘ä»¬è¯æ˜$|h_{T_0}|_1$ä¸èƒ½å¤ªå¤§ã€‚å®é™…ä¸Š,ç”±äº$x^* = x + h$å…·æœ‰æœ€å°çš„$\\ell_1$èŒƒæ•°,å¹¶ä¸”$x$æ»¡è¶³$x^*$çš„å®šä¹‰ä¸­çš„çº¦æŸæ¡ä»¶,æˆ‘ä»¬æœ‰$|x|_1 \\geq |x + h|_1$ã€‚å› æ­¤,åˆ©ç”¨ä¸‰è§’ä¸ç­‰å¼æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š\n$$ ||x||_ 1 \\geq \\sum_{i \\in T_0} |x_i + h_i| + \\sum_{i \\in T_{0,1}^c} |x_i + h_i| \\geq ||x_{T_0}||_ 1 - ||h_{T_0}||_ 1 + ||x_{T_{0,1}^c}||_ 1 - ||h_{T_{0,1}^c}||_1 $$\nç”±äº$|x_ {T_ {0,1}^c}|_ 1 = |x - x_s|_ 1 = |x|_ 1 - |x_ {T_ 0}|_ 1$,æˆ‘ä»¬å¾—åˆ°ï¼š\n$$ |h_{T_0}|_ 1 \\leq |h_{T_0}|_ 1 + 2|x_{T_{0,1}^c}|_1ã€‚ $$\nç»“åˆä¸Šè¿°ç­‰å¼å¯ä»¥å¾—åˆ°ï¼š\n$$ |h_{T_{0,1}^c}|_ 2 \\leq s^{-1/2} (|h_{T_0}|_ 1 + 2|x_{T_{0,1}^c}|_1)ã€‚\\blacksquare $$\nPf of Claim 2\nå¯¹äº2s-ç¨€ç–çš„å‘é‡$h_{T_{0,1}}$,æˆ‘ä»¬æœ‰ï¼š\n$$(1 - \\epsilon) ||h_{T_{0,1}}||_ 2^2 \\leq ||Wh_{T_{0,1}}||_2^2$$\nè€Œ\n$$Wh_{T_{0,1}} = Wh - \\sum_{j \\geq 2} Wh_{T_j} = -\\sum_{j \\geq 2} Wh_{T_j}$$\nå› æ­¤\n$$||Wh_{T_{0,1}}||_ 2^2 = -\\sum_{j \\geq 2} \\langle Wh_{T_{0,1}}, Wh_{T_j} \\rangle$$\nLemmaï¼šå¦‚æœ$W$æ˜¯$(\\epsilon, 2s)$-RIPçŸ©é˜µ,å¯¹äºä»»æ„ä¸ç›¸äº¤çš„$I, J$é›†åˆ,è‹¥$|I| \\leq s, |J| \\leq s$,åˆ™ $$ \\langle W u_{I}, W u_{J} \\rangle \\leq \\epsilon |u_{I}| |u_{J}| $$\nPf.\n$$\\langle W u_{I}, W u_{J} \\rangle = \\frac{|W(u_I + u_J)|^2 - |W(u_I - u_J)|^2}{4}$$\n$$ \\leq \\frac{(1 + \\epsilon) |u_I + u_J|^2 - (1 - \\epsilon) |u_I - u_J|^2}{4} $$\nç”±äº$I, J$æ˜¯ä¸ç›¸äº¤çš„é›†åˆï¼š\n$$ = \\frac{(1 + \\epsilon) (|u_I|^2 + |u_J|^2) - (1 - \\epsilon) (|u_I|^2 + |u_J|^2)}{4} $$\n$$ = \\frac{\\epsilon}{2} ((|u_I|^2 + |u_J|^2) \\leq \\epsilon |u_I||u_J|.\\blacksquare $$\nåŸå¼ä»£å…¥Lemma,æˆ‘ä»¬æœ‰ï¼š\n$$||Wh_{T_{0,1}}||_ 2^2 \\leq \\epsilon (||h_{T_0}||_ 2 + ||h_{T_{1}}||_ 2) \\cdot \\sum_{j \\geq 2} ||h_{T_j}||_ 2 $$ åˆ©ç”¨$2(a^2 + b^2) \\geq (a + b)^2$: $$||h_{T_0}||_ 2 + ||h_{T_1}||_ 2 \\leq \\sqrt{2} ||h_ {T_{0,1}}|| _2$$\næ‰€ä»¥\n$$ |Wh_{T_{0,1}}|_ 2^2 \\leq \\sqrt{2} \\epsilon |h_{T_{0,1}}| _ 2 \\cdot \\sum_{j \\geq 2} |h_{T_j}|_ 2 $$\n$$ \\leq \\sqrt{2} \\epsilon \\cdot s^{-1/2} |h_{T_ {0,1}}| _ 2 \\cdot |h _{T _{0,1}^C}| _1 $$\nå› æ­¤\n$$ |h_{T_0,1}|_ 2 \\leq \\frac{\\sqrt{2} \\epsilon}{1 - \\epsilon} s^{-1/2} |h_{T_0^C}|_1 $$\n$$ |h_{T_0,1}|_ 2 \\leq \\frac{\\sqrt{2} \\epsilon}{1 - \\epsilon} s^{-1/2} (|h_{T_0}|_ 1 + 2|x_{T_0^C}|_1) $$\n$$ \\leq \\rho ||h_ {T_{0}}|| _{2} + 2 \\rho s^{-1/2} ||x _{T _{0}^{C}}|| _{1} $$\nç”±äº\n$$||h_{T_ {1}}|| _ 2 \\leq ||h_{T _{0,1}}|| _2$$\nå› æ­¤\n$$ ||h_{T_{0,1}}||_2 \\leq \\frac{2 \\rho}{1 - \\rho} s^{-1/2} ||x - x_s||_1\\blacksquare $$\nå›åˆ°Thm.3çš„è¯æ˜:\n$$ |h|_ 2 \\leq |h_{T _{0,1}}| _2 + |h _{T _{0,1}^C}| _2 $$\n$$ \\leq 2 |h_{T_0,1}|_2 + 2s^{-1/2} |x - x_s|_1 $$\n$$ \\leq \\left( \\frac{4 \\rho}{1 - \\rho} s^{-1/2} + 2s^{-1/2} \\right) |x - x_s|_1 $$\n$$ = 2 \\frac{1 + \\rho}{1 - \\rho} s^{-1/2} |x - x_s|_1. \\blacksquare $$\nThm.4 æœ€åæˆ‘ä»¬å°±å‰©ä¸‹Thm.4äº†,\nThm.4\nè®¾ $U$ ä¸ºä»»æ„å›ºå®šçš„ $d \\times d$ æ­£äº¤çŸ©é˜µ,è®¾ $\\epsilon, \\delta$ ä¸ºåœ¨ $(0, 1)$ ä¹‹é—´çš„æ ‡é‡,è®¾ $s$ æ˜¯ $[d]$ ä¸­çš„ä¸€ä¸ªæ•´æ•°,ä¸”è®¾ $n$ ä¸ºæ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„æ•´æ•° $$ n \\geq 100 \\frac{s \\ln(40d/(\\delta \\epsilon))}{\\epsilon^2}.$$ è®¾ $W \\in \\mathbb{R}^{n, d}$ ä¸ºä¸€ä¸ªçŸ©é˜µ,å…¶æ¯ä¸ªå…ƒç´ å‡ä»¥é›¶å‡å€¼å’Œæ–¹å·® $1/n$ æ­£æ€åˆ†å¸ƒã€‚åˆ™,å¯¹äºè‡³å°‘ $1 - \\delta$ çš„æ¦‚ç‡è€Œè¨€,çŸ©é˜µ $WU$ æ˜¯ $(\\epsilon, s)$-RIPã€‚\nè¿™é‡Œçš„å¸¸æ•°é¡¹å¯èƒ½æœ‰ä¸€äº›é—®é¢˜,è¯æ˜ä¹Ÿæ¯”è¾ƒå¤æ‚,è¿™é‡Œå°±ä¸å±•å¼€äº†ã€‚å¤§ä½“çš„Proof Sketchæ˜¯:\nå°†è¿ç»­ç©ºé—´æ˜ å°„åˆ°æœ‰é™ä¸ªç‚¹ä¸Š\nè€ƒè™‘ä¸€ä¸ªç‰¹å®šçš„å¤§å°ä¸º $s$çš„ç´¢å¼•é›† $I$\nä½¿ç”¨è¿™ä¸ªç´¢å¼•é›†è¿›å…¥ç¨€ç–ç©ºé—´\nå¯¹æ‰€æœ‰å¯èƒ½çš„ $I$ åº”ç”¨union bound å…·ä½“å¯ä»¥å‚è€ƒShai Shalev-Shwartzçš„paper: Compressed Sensing: Basic results and self contained proofs*.\n4. åè®° â€œThe people who are crazy enough to think they can change the world, are the ones who do.â€\næœŸä¸­ä¹‹å‰çš„å†…å®¹å¤§æ¦‚æ˜¯è¿™äº›ã€‚åœ¨å†™ä½œçš„è¿‡ç¨‹ä¸­,æˆ‘å‘ç°æˆ‘å¾€å¾€ä¼šå¿½ç•¥ä¸€äº›æˆ‘ä¸é‚£ä¹ˆæ„Ÿå…´è¶£çš„éƒ¨åˆ†è€Œåªæ˜¯å»å†™è‡ªè®¤ä¸ºæœ‰è¶£çš„éƒ¨åˆ†,è¿™ä¸€ç‚¹äº¦å¦‚æˆ‘çš„å¤ä¹ ,å…¶ä¸­æ¤å…¥äº†å¤ªå¤šçš„ä¸ªäººç†è§£è€Œå¿½è§†æ‰äº†è€å¸ˆæˆ–è€…å­¦ç•Œä¸»æµæƒ³è®©äººå…³æ³¨çš„æ¡†æ¶,å½¢æˆçš„Map of Machine Learning Worldè‡ªç„¶ä¹Ÿä¼šæ˜¯ä¸åŒçš„ã€‚è¿™å¤§æŠµä¹Ÿèƒ½è§£é‡Šè€ƒè¯•ä¸ºä»€ä¹ˆä¼šå¯„çš„ä¸€éƒ¨åˆ†åŸå› å§ã€‚ååŠå­¦æœŸäº‰å–è®©è‡ªå·±å­¦ä¼šçš„ä¸œè¥¿çš„åˆ†å¸ƒå’Œè¯¾ä¸Šçš„åˆ†å¸ƒæ¥è¿‘ä¸€äº›,æˆ–è€…æä¸€ä¸ªgenerative model,ä»è‡ªå·±çš„åˆ†å¸ƒé‡Œé‡‡æ ·,ç»è¿‡ä¸€äº›å˜æ¢èƒ½å¤Ÿæ¥è¿‘ä»–çš„åˆ†å¸ƒå§ã€‚ ",
  "wordCount" : "4487",
  "inLanguage": "en",
  "datePublished": "2024-11-09T00:00:00Z",
  "dateModified": "2024-11-09T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Nemo"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blog/posts/ml1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nemo's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/blog/" accesskey="h" title="Nemo&#39;s Blog (Alt + H)">Nemo&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/blog/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://knightnemo.github.io" title="About Me">
                    <span>About Me</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/blog/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/blog/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Machine Learning Series: 1.Optimization, Generalization and Supervised Learning
    </h1>
    <div class="post-meta"><span title='2024-11-09 00:00:00 +0000 UTC'>November 9, 2024</span>&nbsp;Â·&nbsp;22 min&nbsp;Â·&nbsp;Nemo


      <div class="meta-item">&nbspÂ·&nbsp
        <span id="busuanzi_container_page_pv" > 
          Reads: <span class="page-pv" id="busuanzi_value_page_pv">0</span> times
        </span>
      </div>
    </div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#0%e9%a5%ad%e5%90%8e%e7%94%9c%e5%93%81%e4%bd%a0%e4%b8%8d%e8%83%bd%e6%8c%87%e6%9c%9b%e8%b7%9f%e6%ad%a3%e9%a4%90%e4%b8%80%e8%b5%b7" aria-label="0.é¥­åç”œå“,ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·">0.é¥­åç”œå“,ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·</a></li>
                <li>
                    <a href="#1-optimization" aria-label="1. Optimization">1. Optimization</a><ul>
                        
                <li>
                    <a href="#11-l-smooth--convex" aria-label="1.1 L-Smooth &amp; Convex">1.1 L-Smooth &amp; Convex</a><ul>
                        
                <li>
                    <a href="#l-smooth" aria-label="L-smooth">L-smooth</a></li>
                <li>
                    <a href="#convex" aria-label="Convex">Convex</a></li>
                <li>
                    <a href="#mu-strongly-convex" aria-label="$\mu$-strongly Convex">$\mu$-strongly Convex</a></li>
                <li>
                    <a href="#convex--l-smooth" aria-label="Convex &amp; L-Smooth:">Convex &amp; L-Smooth:</a></li></ul>
                </li>
                <li>
                    <a href="#12-gradient-descent" aria-label="1.2 Gradient Descent">1.2 Gradient Descent</a><ul>
                        
                <li>
                    <a href="#convex-l-smooth" aria-label="Convex, L-Smooth">Convex, L-Smooth</a></li>
                <li>
                    <a href="#mu-strongly-convex--l-smooth" aria-label="$\mu$-strongly Convex &amp; L-smooth">$\mu$-strongly Convex &amp; L-smooth</a></li>
                <li>
                    <a href="#l-smooth-1" aria-label="L-Smooth">L-Smooth</a></li>
                <li>
                    <a href="#recap" aria-label="Recap:">Recap:</a></li></ul>
                </li>
                <li>
                    <a href="#13-stochastic-gradient-descent" aria-label="1.3 Stochastic Gradient Descent">1.3 Stochastic Gradient Descent</a><ul>
                        
                <li>
                    <a href="#why-sgd" aria-label="Why SGD">Why SGD</a></li>
                <li>
                    <a href="#algorithm" aria-label="Algorithm">Algorithm</a></li>
                <li>
                    <a href="#convergence" aria-label="Convergence">Convergence</a></li></ul>
                </li>
                <li>
                    <a href="#14-svrg" aria-label="1.4 SVRG">1.4 SVRG</a><ul>
                        
                <li>
                    <a href="#algorithm-1" aria-label="Algorithm">Algorithm</a></li>
                <li>
                    <a href="#procedure-svrg" aria-label="Procedure SVRG">Procedure SVRG</a></li>
                <li>
                    <a href="#convergence-rate" aria-label="Convergence Rate">Convergence Rate</a></li></ul>
                </li>
                <li>
                    <a href="#15-mirror-descent" aria-label="1.5 Mirror Descent">1.5 Mirror Descent</a><ul>
                        
                <li>
                    <a href="#algorithm-2" aria-label="Algorithm">Algorithm</a></li>
                <li>
                    <a href="#intuition" aria-label="Intuition">Intuition</a></li>
                <li>
                    <a href="#relationship-between-gd--md" aria-label="Relationship between GD &amp; MD">Relationship between GD &amp; MD</a></li>
                <li>
                    <a href="#convergence-1" aria-label="Convergence:">Convergence:</a></li></ul>
                </li>
                <li>
                    <a href="#16-linear-coupling" aria-label="1.6 Linear Coupling">1.6 Linear Coupling</a><ul>
                        
                <li>
                    <a href="#wishful-thinking" aria-label="Wishful Thinking">Wishful Thinking</a></li>
                <li>
                    <a href="#algorithm-3" aria-label="Algorithm">Algorithm</a></li>
                <li>
                    <a href="#convergence-2" aria-label="Convergence">Convergence</a></li></ul>
                </li>
                <li>
                    <a href="#17-non-convex-optimization" aria-label="1.7 Non-Convex Optimization">1.7 Non-Convex Optimization</a><ul>
                        
                <li>
                    <a href="#matrix-completion" aria-label="Matrix Completion">Matrix Completion</a></li>
                <li>
                    <a href="#escaping-saddle-points" aria-label="Escaping Saddle Points">Escaping Saddle Points</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#2generalization" aria-label="2.Generalization">2.Generalization</a><ul>
                        
                <li>
                    <a href="#21-no-free-lunch-thm" aria-label="2.1 No Free Lunch Thm.">2.1 No Free Lunch Thm.</a></li>
                <li>
                    <a href="#22-pac-learning" aria-label="2.2 PAC-Learning">2.2 PAC-Learning</a><ul>
                        
                <li>
                    <a href="#%e4%b8%80%e4%ba%9b%e6%a6%82%e5%bf%b5" aria-label="ä¸€äº›æ¦‚å¿µ:">ä¸€äº›æ¦‚å¿µ:</a></li>
                <li>
                    <a href="#finite-classes-are-pac-learnable" aria-label="Finite Classes are PAC-learnable">Finite Classes are PAC-learnable</a></li>
                <li>
                    <a href="#threshold-functions-are-pac-learnable" aria-label="Threshold Functions are PAC-learnable">Threshold Functions are PAC-learnable</a></li></ul>
                </li>
                <li>
                    <a href="#23-agnostic-pac-learnable" aria-label="2.3 Agnostic PAC-Learnable">2.3 Agnostic PAC-Learnable</a><ul>
                        
                <li>
                    <a href="#agnostic-pac-learnable" aria-label="Agnostic PAC-Learnable:">Agnostic PAC-Learnable:</a></li>
                <li>
                    <a href="#error-decomposition" aria-label="Error Decomposition:">Error Decomposition:</a></li>
                <li>
                    <a href="#bayes-optimal-predictor" aria-label="Bayes Optimal Predictor">Bayes Optimal Predictor</a></li></ul>
                </li>
                <li>
                    <a href="#24-vc-dim" aria-label="2.4 VC-Dim">2.4 VC-Dim</a><ul>
                        
                <li>
                    <a href="#restriction-of-h-to-c" aria-label="Restriction of $H$ to $C$">Restriction of $H$ to $C$</a></li>
                <li>
                    <a href="#shattering" aria-label="Shattering">Shattering</a></li>
                <li>
                    <a href="#nfl-reexpressed" aria-label="NFL Reexpressed">NFL Reexpressed</a></li>
                <li>
                    <a href="#vc-dimension" aria-label="VC-Dimension">VC-Dimension</a></li>
                <li>
                    <a href="#inifite-vc-dim-hypothesis-classes-are-not-pac-learnable" aria-label="Inifite VC-dim hypothesis classes are not PAC-learnable">Inifite VC-dim hypothesis classes are not PAC-learnable</a></li></ul>
                </li>
                <li>
                    <a href="#25-fundamental-theorem-of-statistical-learning" aria-label="2.5 Fundamental theorem of statistical learning">2.5 Fundamental theorem of statistical learning</a></li></ul>
                </li>
                <li>
                    <a href="#3supervised-learning" aria-label="3.Supervised Learning">3.Supervised Learning</a><ul>
                        
                <li>
                    <a href="#31-perceptron" aria-label="3.1 Perceptron">3.1 Perceptron</a><ul>
                        
                <li>
                    <a href="#algorithm-4" aria-label="Algorithm">Algorithm</a></li>
                <li>
                    <a href="#convergence-3" aria-label="Convergence">Convergence</a></li></ul>
                </li>
                <li>
                    <a href="#32-logistic-regression" aria-label="3.2 Logistic Regression">3.2 Logistic Regression</a></li>
                <li>
                    <a href="#33-regularization" aria-label="3.3 Regularization">3.3 Regularization</a><ul>
                        
                <li>
                    <a href="#ridge-regression" aria-label="Ridge Regression">Ridge Regression</a></li>
                <li>
                    <a href="#lasso-regression" aria-label="Lasso Regression">Lasso Regression</a></li></ul>
                </li>
                <li>
                    <a href="#34-compressed-sensing" aria-label="3.4 Compressed Sensing">3.4 Compressed Sensing</a><ul>
                        
                <li>
                    <a href="#rip-condition" aria-label="RIP-Condition">RIP-Condition</a></li>
                <li>
                    <a href="#thm1" aria-label="Thm.1">Thm.1</a></li>
                <li>
                    <a href="#thm2" aria-label="Thm.2">Thm.2</a></li>
                <li>
                    <a href="#thm3" aria-label="Thm.3">Thm.3</a></li>
                <li>
                    <a href="#thm4" aria-label="Thm.4">Thm.4</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-%e5%90%8e%e8%ae%b0" aria-label="4. åè®°">4. åè®°</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="0é¥­åç”œå“ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·">0.é¥­åç”œå“,ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·<a hidden class="anchor" aria-hidden="true" href="#0é¥­åç”œå“ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·">#</a></h1>
<blockquote>
<p><em>Everything should be made as simple as possible, but not simpler.</em>
<!-- raw HTML omitted --><!-- raw HTML omitted --> Albert Einstein.<!-- raw HTML omitted --></p>
</blockquote>
<p>è®°å¾—é«˜ä¸‰çš„æ—¶å€™å†™è¿‡ä¸€ç¯‡ä½œæ–‡,æ–‡ç« çš„ç«‹æ„å¤§æ¦‚æ˜¯ <strong>â€œæ•´é¡¿æ——é¼“å†å‡ºå‘â€</strong> ã€‚æ˜¯å•Š,å¤šå°‘æ¬¡,æˆ‘ä»¬å¥‹åŠ›ç‹‚å¥”,è¿æ¥ç€ç‹‚é£éª¤é›¨çš„æ•²æ‰“,å´ä¸æ„¿æ„æ”¾æ…¢è„šæ­¥,ä»å¯¹æœªæ¥ä¸ç¡®å®šæ€§çš„ç„¦è™‘ä¹‹ä¸­è·³è„±å‡ºæ¥,çœ‹çœ‹è‡ªå·±çš„æ¥æ—¶è·¯,çœ‹çœ‹æ˜¨æ—¥ä¹‹æˆ‘ã€ä»Šæ—¥ä¹‹æˆ‘ã€‚åœ¨å¿™å¿™å¨å¨ä¹‹ä¸­æ—¶å…‰ä¾¿æµé€æ‰äº†,æœ‰æ—¶ä¸å¦¨åšç‚¹ <strong>reflection</strong>,æ•´ç†ä¸€ä¸‹æ‚ä¹±çš„æ€ç»ªå’Œæ²¡æƒ³æ˜ç™½çš„é—®é¢˜ã€‚</p>
<p><strong>å¦ä¸€ä¸ªè½åœ¨å®å¤„çš„åŠ¨æœºæ˜¯æˆ‘å‘ç°æˆ‘å­¦ä¸œè¥¿æœ‰ä¸ªç‰¹ç‚¹,å°±æ˜¯å¿˜ä¸œè¥¿å¾ˆå¿«ã€‚å¦‚æœä¸ç•™ä¸‹ç‚¹ä¸œè¥¿å‘¢,ä¼šå¿˜,ç„¶åå¿˜äº†æ²¡æœ‰ç¬”è®°åˆå¾ˆéš¾æ¡èµ·æ¥ã€‚</strong> æ‰€ä»¥æˆ‘æƒ³,ä¸ºä»€ä¹ˆä¸åœ¨è‡ªå·±å¯¹è¿™ä¸ªé¢†åŸŸçš„å†…å®¹è®¤è¯†æœ€æ·±åˆ»çš„æ—¶å€™ç•™ä¸‹ç‚¹è®°å¿†,å¯„å¸Œæœ›äºæœªæ¥çš„è‡ªå·±æˆ–è€…æˆ–è®¸å¯¹æœºå™¨å­¦ä¹ æœ‰å…´è¶£çš„è¯»è€…èƒ½å¤Ÿé€šè¿‡ä»Šæ—¥çš„ä¸€ç¯‡æ–‡ç« äº†è§£ä¸€äº›ä»Šæ—¥ä¹‹æˆ‘æ‰€æ€æ‰€æƒ³çš„ä¸€äº›å†…å®¹å‘¢,äºæ˜¯å°±è¯ç”Ÿäº†è¿™ç¯‡æ–‡ç« ã€‚</p>
<p>ä½†è¿™ä»¶äº‹æ€ä¹ˆçœ‹éƒ½è¿˜æ˜¯å¾ˆå‘†,éƒ½è€ƒå®Œäº†,ç„¶ååœ¨å†™çš„è¿‡ç¨‹ä¸­è‚¯å®šåˆèƒ½å­¦åˆ°ç‚¹ä¸œè¥¿ã€‚ä¸€ä½æœ‹å‹è·Ÿæˆ‘è¯´ <strong>â€œé¥­åç”œå“,ä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·â€</strong> ,äºæ˜¯æœ¬ç€ä¸€ä¸ªå“å‘³ç”œå“çš„é£Ÿå®¢çš„å¿ƒæ€,æˆ‘å†³å®šå°†è¿™ç¯‡æ–‡ç« å°½é‡å†™çš„è½»é‡åŒ–ä¸€ç‚¹ã€æ•…äº‹æ€§å¼ºä¸€ç‚¹,ç©¿èµ·ä¸€ä¸ªæ€è€ƒçš„ä¸»çº¿ã€‚</p>
<h1 id="1-optimization">1. Optimization<a hidden class="anchor" aria-hidden="true" href="#1-optimization">#</a></h1>
<p>ä¼˜åŒ–é—®é¢˜è‡ªç„¶è€Œç„¶åœ°å‡ºç°åœ¨è®¸å¤šåº”ç”¨é¢†åŸŸä¸­ã€‚æ— è®ºäººä»¬åšä»€ä¹ˆ,åœ¨æŸäº›æ—¶å€™,ä»–ä»¬éƒ½ä¼šäº§ç”Ÿä¸€ç§æƒ³è¦ä»¥æœ€ä½³æ–¹å¼ç»„ç»‡äº‹ç‰©çš„æ¸´æœ›ã€‚è¿™ç§æ„å›¾,å½“è¢«è½¬æ¢æˆæ•°å­¦å½¢å¼æ—¶,å°±ä¼šå˜æˆæŸç§ç±»å‹çš„ä¼˜åŒ–é—®é¢˜ã€‚ä¸‹é¢ä»‹ç»å‡ ç§ä¼˜åŒ–ç®—æ³•,åŒ…æ‹¬ï¼š<em>Gradient Descent</em>, <em>Stochastic Gradient Descent</em>,  <em>SVRG</em>, <em>Mirror Desent</em>, <em>Linear Coupling</em>.</p>
<h2 id="11-l-smooth--convex">1.1 L-Smooth &amp; Convex<a hidden class="anchor" aria-hidden="true" href="#11-l-smooth--convex">#</a></h2>
<p>åœ¨ä¼˜åŒ–å‡½æ•°çš„æ—¶å€™,æˆ‘ä»¬å¾€å¾€éœ€è¦ä¸€äº›æœ‰å…³å‡½æ•°æ€§è´¨çš„ä¿éšœ,æ‰èƒ½å¤Ÿç¡®ä¿ä»–æœ‰å¥½çš„æ”¶æ•›ç‡ã€‚</p>
<h3 id="l-smooth">L-smooth<a hidden class="anchor" aria-hidden="true" href="#l-smooth">#</a></h3>
<p>ä»¥ä¸‹ä¸‰æ¡ç­‰ä»·ï¼š</p>
<ul>
<li>
<p>$f(x) \leq f(x_0) + \langle \nabla f(x_0), x-x_0 \rangle + \frac{L}{2}||x-x_0||^2$</p>
</li>
<li>
<p>$|\lambda_{\nabla^2 f(x)}| \leq L$</p>
</li>
<li>
<p>$||\nabla f(x) - \nabla f(y)|| \leq L||x-y||$</p>
</li>
</ul>
<p>æ³¨æ„åˆ°L-smoothå…¶å®å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯æ¢¯åº¦å˜åŒ–ä¸ä¼šå¤ªå¿«,å¦å¤–ä¸€ä¸ªæœ‰è¶£çš„çœ‹æ³•æ˜¯ï¼š</p>
<ul>
<li>Upper Bound:
$f(x) \leq f(x_0) + \langle \nabla f(x_0), x-x_0 \rangle + \frac{L}{2}||x-x_0||^2$</li>
<li>Lower Bound:
$f(x) \geq f(x_0) + \langle \nabla f(x_0), x-x_0 \rangle - \frac{L}{2}||x-x_0||^2$</li>
</ul>
<p>ä¹Ÿå°±æ˜¯è¯´ç»™å®šä¸€ä¸ªç‚¹$f(x_0)$çš„é›¶é˜¶å’Œä¸€é˜¶ä¿¡æ¯,æˆ‘ä»¬å°±å¯ä»¥è·å¾—åˆ«çš„ç‚¹çš„å‡½æ•°å€¼çš„ä¸€ä¸ªäºŒæ¬¡å‹çš„ä¸Šä¸‹ç•Œã€‚
<img alt="figure1" loading="lazy" src="../img/ml1/image.png"></p>
<h3 id="convex">Convex<a hidden class="anchor" aria-hidden="true" href="#convex">#</a></h3>
<p>ä»¥ä¸‹å››æ¡ç­‰ä»·ï¼š</p>
<ul>
<li>$ f(x) \geq f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle $</li>
<li>$ f(x) \leq f(x_0) + \langle \nabla f(x), x - x_0 \rangle $</li>
<li>$ \lambda_{\min}(\nabla^2 f(x)) \geq 0 $</li>
<li>$
\frac{1}{T} \sum_{i=1}^{T} f(x_i) \geq f(\bar{x}), \quad \bar{x} = \frac{1}{T} \sum_{i=1}^{T} x_i
$</li>
</ul>
<h3 id="mu-strongly-convex">$\mu$-strongly Convex<a hidden class="anchor" aria-hidden="true" href="#mu-strongly-convex">#</a></h3>
<p>ä»¥ä¸‹ä¸‰æ¡ç­‰ä»·ï¼š</p>
<ul>
<li>$
f(x) \geq f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle + \frac{\mu}{2} |x - x_0|^2
$</li>
<li>$ \lambda_{\min}(\nabla^2 f(x)) \geq \mu $</li>
<li>$
|\nabla f(x) - \nabla f(y)| \geq \mu |x - y|
$</li>
</ul>
<h3 id="convex--l-smooth">Convex &amp; L-Smooth:<a hidden class="anchor" aria-hidden="true" href="#convex--l-smooth">#</a></h3>
<p>åœ¨ä¸€ä¸ªå‡½æ•°åˆconvexåˆL-Smoothçš„æƒ…å†µä¸‹,æˆ‘ä»¬ä¼šæœ‰ä¸€äº›æ›´å¥½çš„æ€§è´¨ï¼š</p>
<blockquote>
<p><strong>Thm.1</strong> $$
f(y) - f(x) - \langle \nabla f(x), y - x \rangle \geq \frac{1}{2L} |\nabla f(x) - \nabla f(y)|^2 $$</p>
</blockquote>
<p>è¯æ˜å¦‚ä¸‹:</p>
<p>ä»¤
$h(y) = f(y) - f(x) - \langle \nabla f(x), y - x \rangle$</p>
<p>æ³¨æ„åˆ°
$$
\nabla h(y) = \nabla f(y) - \nabla f(x)
$$
$$
\nabla^2 h(y) = \nabla^2 f(y)
$$
æ‰€ä»¥è¯´$h(y)$ä¹Ÿæ˜¯convexä¸”L-smoothçš„,è€Œä¸”æœ€å°å€¼ç‚¹åœ¨$y=x$å¤„å–çš„ã€‚
æ‰€ä»¥,<br>
$$
h(x) \leq h(y - \frac{1}{L} \nabla h(y))\
$$
$$\leq h(y) - \frac{1}{L} |\nabla h(y)|^2 + \frac{1}{2L} |\nabla h(y)|^2
$$
$$
=h(y) - \frac{1}{2L} |\nabla h(y)|^2
$$</p>
<p>å› æ­¤,<br>
$$
f(y) - f(x) - \langle \nabla f(x), y - x \rangle \geq \frac{1}{2L} |\nabla f(y)-\nabla f(x)|^2
$$</p>
<blockquote>
<p><strong>Thm.2</strong>
$$
\langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \frac{1}{L} |\nabla f(x) - \nabla f(y)|^2$$</p>
</blockquote>
<p>è¿™ä¸ªçš„è¯æ˜å¯ä»¥ç”±Thm.1äº¤æ¢$x,y$æ¬¡åºä¹‹åç›¸åŠ å¾—åˆ°ã€‚</p>
<h2 id="12-gradient-descent">1.2 Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#12-gradient-descent">#</a></h2>
<p>GDçš„update ruleå¦‚ä¸‹:
$$x_{t+1}=x_{t}-\eta \nabla f(x_t)$$
åœ¨ä»¥ä¸‹ä¸‰ç§æƒ…å†µä¸‹,åˆ†åˆ«æœ‰ä¸åŒçš„æ”¶æ•›ç‡ï¼š</p>
<h3 id="convex-l-smooth">Convex, L-Smooth<a hidden class="anchor" aria-hidden="true" href="#convex-l-smooth">#</a></h3>
<p>$$
x_{t+1} = x_t - \eta \nabla f(x_t)
$$</p>
<p>$$
f(x_{t+1}) \leq f(x_t) + \langle \nabla f(x_t), x_{t+1} - x_t \rangle + \frac{L}{2} |x_{t+1} - x_t|^2
$$</p>
<p>$$
= f(x_t) - \eta |\nabla f(x_t)|^2 - \frac{L \eta^2}{2} |\nabla f(x_t)|^2
$$</p>
<p>å–$\eta \leq \frac{1}{L}$:</p>
<p>$$ f(x_{t+1}) \leq f(x_t) - \frac{\eta}{2} |\nabla f(x_t)|^2 $$</p>
<p>ç”±convexity:</p>
<p>$$
f(x_{t+1}) \leq f(x^*) + \langle \nabla f(x_t), x_t - x^* \rangle - \frac{\eta}{2} |\nabla f(x_t)|^2
$$</p>
<p>$$
= f(x^*) - \frac{1}{\eta} \langle x_{t+1} - x_t, x_t - x^* \rangle - \frac{1}{2\eta} |x_{t+1} - x_t|^2
$$</p>
<p>$$
= f(x^*) - \frac{1}{2\eta} |x_{t+1} - x^*|^2 + \frac{1}{2\eta} |x_t - x^*|^2
$$</p>
<p>æ¥ä¸‹æ¥æˆ‘ä»¬åštelescope:</p>
<p>$$
\sum_{t=0}^{T-1} (f(x_{t+1}) - f(x^*)) \leq \frac{1}{2\eta} (|x_0 - x^*|^2 - |x_T - x^*|^2)
$$</p>
<p>å› ä¸º$f(x_t)$æ˜¯å•è°ƒé€’å‡çš„(convexä¿è¯)</p>
<p>$$ f(x_T) - f(x^*) \leq \frac{1}{2\eta T} |x_0 - x^*|^2 = \epsilon $$
æ‰€ä»¥è¯´
$$ T = \frac{|x_0 - x^*|^2}{2\eta \epsilon} = O\left(\frac{L}{\epsilon}\right) $$
åœ¨è¿™ç§æƒ…å†µä¸‹éœ€è¦è¿­ä»£$O(\frac{1}{\epsilon})$æ¬¡,æ”¶æ•›ç‡ä¸º$O(\frac{1}{T})$.</p>
<h3 id="mu-strongly-convex--l-smooth">$\mu$-strongly Convex &amp; L-smooth<a hidden class="anchor" aria-hidden="true" href="#mu-strongly-convex--l-smooth">#</a></h3>
<p>è¿™é‡Œèµ·æ‰‹å¼æˆ‘ä»¬å¡$||x-x^*||$:
$$
|x_{t+1} - x^*|^2 = |x_t - \eta \nabla f(x_t) - x^*|^2
$$</p>
<p>$$
= |x_t - x^*|^2 - 2\eta \langle \nabla f(x_t), x_t - x^* \rangle + \eta^2 |\nabla f(x_t)|^2
$$
å› ä¸ºå¼ºå‡¸æ€§ï¼š
$$
f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle + \frac{\mu}{2} |y - x|^2
$$</p>
<p>ä»£å…¥ $x = x_t$, $y = x^*$:</p>
<p>$$
f(x^*) \geq f(x_t) + \langle \nabla f(x_t), x^* - x_t \rangle + \frac{\mu}{2} |x_t - x^*|^2
$$</p>
<p>$$
\langle \nabla f(x_t), x_t - x^* \rangle \geq f(x_t) - f(x^*) + \frac{\mu}{2} |x_t - x^*|^2
$$
æ‰€ä»¥
$$
|x_{t+1} - x^*|^2 \leq |x_t - x^*|^2 - 2\eta (f(x_t) - f(x^*) + \frac{\mu}{2} |x_t - x^*|^2ï¼‰ + \eta^2 |\nabla f(x_t)|^2
$$
æ ¹æ®ä¹‹å‰çš„Thm.1:
$$
\frac{1}{2L} |\nabla f(x_t)|^2 \leq f(x_t) - f(x^*)
$$
æ‰€ä»¥
$$
|x_{t+1} - x^*|^2 \leq (1 - \eta \mu) |x_t - x^*|^2 + (2\eta^2 L - 2\eta )(f(x_t) - f(x^*))
$$</p>
<p>å– $\eta = \frac{1}{L}$:</p>
<p>$$
|x_{t+1} - x^*|^2 \leq (1 - \frac{\mu}{L}) |x_t - x^*|^2
$$
æ‰€ä»¥è¯´Linear Convergence, åæ˜ åœ¨$f(x)$ä¸Š:
$$f(x_T)\leq f(x^*)+\frac{L}{2}||x_T-x^*||^2$$
$$\leq f(x^*)+\frac{L}{2}(1 - \frac{\mu}{L})^T||x_0-x^*||^2$$
ä¹Ÿå°±æ˜¯è¯´éœ€è¦è¿­ä»£æ¬¡æ•°$O(log(\frac{1}{\epsilon}))$, æ”¶æ•›ç‡ä¸ºLinear Convergence.</p>
<blockquote>
<p><em>Remark</em>: <!-- raw HTML omitted -->
å¯¹äº$\mu$-strongly Convex &amp; L-smoothçš„å‡½æ•°æœ‰å¦‚ä¸‹æ€§è´¨ï¼š$
\frac{\mu}{2} | \mathbf{x}^* - \mathbf{x} |^2 \leq f(\mathbf{x}) - f^* \leq \frac{L}{2} | \mathbf{x}^* - \mathbf{x} |^2
$
$
\frac{1}{2L} | \nabla f(\mathbf{x}) |^2 \leq f(\mathbf{x}) - f^* \leq \frac{1}{2\mu} | \nabla f(\mathbf{x}) |^2
$æ ¹æ®è¿™äº›æ€§è´¨æœ‰ä¸€ä¸ªæ›´ä¸ºç®€æ´çš„è¯æ˜ã€‚</p>
</blockquote>
<h3 id="l-smooth-1">L-Smooth<a hidden class="anchor" aria-hidden="true" href="#l-smooth-1">#</a></h3>
<p>æ ¹æ®ç¬¬ä¸€ç§æƒ…å†µä¸‹çš„åˆ†æï¼š
$$
f(x_{t+1}) - f(x_t) \leq -\frac{\eta}{2} |\nabla f(x_t)|^2
$$</p>
<p>ç„¶ååšTelescope:</p>
<p>$$
\min_{k \in [T]} |\nabla f(x_t)|^2 \leq \frac{2L(f(x_0) - f(x^*))}{T} = \epsilon^2
$$
æ‰€ä»¥è¯´å½“æˆ‘ä»¬æƒ³è·å¾—$|\nabla f(x_t)|^2&lt;\epsilon$,æˆ‘ä»¬éœ€è¦
$
T = O\left(\frac{1}{\epsilon^2}\right)
$çš„è¿­ä»£æ¬¡æ•°,æ”¶æ•›ç‡ä¸º
$
O\left(\frac{1}{\sqrt{T}}\right)
$ã€‚</p>
<h3 id="recap">Recap:<a hidden class="anchor" aria-hidden="true" href="#recap">#</a></h3>
<p>æ€»ç»“èµ·æ¥å¤§æ¦‚æ˜¯:
<img alt="table" loading="lazy" src="../img/ml1/image2.png"></p>
<h2 id="13-stochastic-gradient-descent">1.3 Stochastic Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#13-stochastic-gradient-descent">#</a></h2>
<h3 id="why-sgd">Why SGD<a hidden class="anchor" aria-hidden="true" href="#why-sgd">#</a></h3>
<p>GDçœ‹èµ·æ¥ä¸é”™,ä½†æ˜¯æœ‰ä¸¤ä¸ªé—®é¢˜:</p>
<ul>
<li>è®¡ç®—ä¸€æ¬¡full gradientå¾ˆè´µ</li>
<li>GDä¼šåœ¨local maximumå’Œsaddle pointï¼ˆéç‚¹ï¼‰å¡ä½</li>
</ul>
<p>äºæ˜¯æˆ‘ä»¬å°±ä¼šå»æƒ³,èƒ½ä¸èƒ½å°‘ç®—å‡ ä¸ªæ•°æ®ç‚¹å¯¹åº”çš„loss function,åŒæ—¶åˆèƒ½æœ‰ä¸€äº›convergence guaranteeå‘¢,SGDä¾¿æ˜¯è¿™æ ·çš„ä¸€ç§ç®—æ³•ã€‚</p>
<h3 id="algorithm">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm">#</a></h3>
<p>SGDçš„update ruleå¦‚ä¸‹æ‰€ç¤º:
$$
x_{t+1} = x_t - \eta G_t, $$
å…¶ä¸­$G_t$æ»¡è¶³:
$$ \mathbb{E}[G_t] = \nabla f(x_t), \quad \text{Var}(G_t) \leq \sigma^2
$$</p>
<h3 id="convergence">Convergence<a hidden class="anchor" aria-hidden="true" href="#convergence">#</a></h3>
<p>ä¸‹é¢æˆ‘ä»¬è¯æ˜SGDåœ¨L-Smooth, Convex, $\text{Var}(G_t) \leq \sigma^2$çš„æ¡ä»¶ä¸‹çš„æ”¶æ•›ç‡:</p>
<p>å› ä¸ºL-smooth:
$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) + \mathbb{E}[\langle \nabla f(x_t), x_{t+1} - x_t \rangle] + \frac{L}{2} \mathbb{E}[|x_{t+1} - x_t|^2]
$$</p>
<p>$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) - \eta |\nabla f(x_t)|^2 + \frac{L \eta^2}{2} \mathbb{E}[|G_t|^2]
$$</p>
<p>æ ¹æ®æ–¹å·®çš„å®šä¹‰ï¼š
$$\mathbb{E}[ ||G_t||^2 ] = \text{Var}(G_t) + ||\mathbb{E}[G_t]||^2 \leq \sigma^2 + |\nabla f(x_t)|^2$$
æ‰€ä»¥æœ‰
$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) + \left(\frac{L \eta^2}{2} - \eta\right) |\nabla f(x_t)|^2 + \frac{L \eta^2}{2} \sigma^2
$$</p>
<p>å– $\eta = \frac{1}{L}$:</p>
<p>$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) - \frac{\eta}{2} |\nabla f(x_t)|^2 + \frac{\eta}{2} \sigma^2
$$</p>
<p>æ ¹æ®convexity:</p>
<p>$$
f(x_t) \leq f(x^*) + \langle \nabla f(x_t), x_t - x^* \rangle
$$</p>
<p>$$
\mathbb{E}[f(x_{t+1})] \leq f(x^*) + \mathbb{E}[\langle G_t, x_t - x^* \rangle] - \frac{\eta}{2} |\nabla f(x_t)|^2 + \frac{\eta}{2} \sigma^2
$$
åˆå› ä¸º
$$
|\nabla f(x_t)|^2 = \mathbb{E}[|G_t|^2] - \text{Var}(G_t) \geq \mathbb{E}[|G_t|^2] - \sigma^2
$$</p>
<p>æ‰€ä»¥
$$
\mathbb{E}[f(x_{t+1})] \leq f(x^*) + \mathbb{E}[\langle G_t, x_t - x^* \rangle - \frac{\eta}{2} |G_t|^2] + \eta \sigma^2
$$
æ³¨æ„åˆ°
$$
\langle G_t, x_t - x^* \rangle - \frac{\eta}{2} |G_t|^2
$$</p>
<p>$$
= -\frac{1}{2\eta} |(x_{t+1} - x_t) - (x^* - x_t)|^2 + \frac{1}{2\eta} |x_t - x^*|^2
$$</p>
<p>$$
= \frac{1}{2\eta} (|x_t - x^*|^2 - |x_{t+1} - x^*|^2)
$$
ä¹Ÿå°±æ˜¯è¯´
$$
\mathbb{E}[f(x_{t+1})] \leq f(x^*) + \frac{\eta}{2} \mathbb{E}[|x_t - x^*|^2 - |x_{t+1} - x^*|^2] + \eta \sigma^2
$$</p>
<p>ä» $t = 0$ åˆ° $T-1$æ±‚å’Œ(telescope):</p>
<p>$$
\frac{1}{T}\sum_{t=0}^{T-1} (\mathbb{E}[f(x_t)] - f(x^*)) \leq \frac{1}{2\eta T} |x_0 - x^*|^2 + \eta \sigma^2
$$</p>
<p>å– $\eta = \frac{\epsilon}{2\sigma^2} \leq \frac{1}{L}$, åˆ™æœ‰:</p>
<p>$$
T = \frac{2 \sigma^2 |x_0 - x^*|^2}{\epsilon^2}
$$</p>
<p>Stochastic Gradient Descent (SGD) çš„æ”¶æ•›ç‡æ˜¯
$
O\left(\frac{1}{\sqrt{T}}\right)
$ã€‚</p>
<h2 id="14-svrg">1.4 SVRG<a hidden class="anchor" aria-hidden="true" href="#14-svrg">#</a></h2>
<p>æˆ‘ä»¬çœ‹åˆ°äº†é€šè¿‡Stochastic Gradientå¯ä»¥å‡å°‘computation cost,ä½†æ˜¯éšä¹‹è€Œæ¥çš„é—®é¢˜æ˜¯å› ä¸º $G_t$æ‹¥æœ‰çš„variance,å¯¼è‡´åŸæ¥$O(\frac{1}{T})$çš„convergence rateå˜æˆäº†$O(\frac{1}{\sqrt{T}})$,äºæ˜¯æˆ‘ä»¬å»æƒ³,æœ‰æ²¡æœ‰ä»€ä¹ˆåŠæ³•èƒ½å¤Ÿåœ¨ä¿æŒcomputation costæ¯”è¾ƒå°çš„æƒ…å†µä¸‹åŒæ—¶æŠŠvarianceé™ä¸‹æ¥,SVRGæ˜¯å…¶ä¸­çš„ä¸€ç§ç®—æ³•,åœ¨strongly-convexå’Œl-smoothçš„æƒ…å†µä¸‹æœ€åèƒ½å¤Ÿè·å¾—å’ŒGDä¸€æ ·çš„convergence rateã€‚</p>
<h3 id="algorithm-1">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-1">#</a></h3>
<blockquote>
<h3 id="procedure-svrg">Procedure SVRG<a hidden class="anchor" aria-hidden="true" href="#procedure-svrg">#</a></h3>
<p><strong>Parameters</strong>: update frequency $m$ and learning rate $\eta$<br>
<strong>Initialize</strong> $\tilde{w}_0$<br>
<strong>Iterate</strong>: for $s = 1, 2, \ldots$</p>
<ol>
<li>$\tilde{w} = \tilde{w}_{s-1}$</li>
<li>$\tilde{\mu} = \frac{1}{n} \sum_{i=1}^{n} \nabla l_i(\tilde{w})$</li>
<li>$w_0 = \tilde{w}$<br>
<strong>Iterate</strong>: for $t = 1, 2, \ldots, m$<br>
i. Randomly pick $i_t \in {1, \ldots, n}$ and update weight<br>
$
w_t = w_{t-1} - \eta \left( \nabla l_{i_t}(w_{t-1}) - \nabla l_{i_t}(\tilde{w}) + \tilde{\mu} \right)
$
<strong>end</strong><br>
<strong>Option I</strong>: set $\tilde{w}_s = w_m$<br>
<strong>Option II</strong>: set $\tilde{w}_s = w_t$ for randomly chosen $t \in {0, \ldots, m - 1}$<br>
<strong>end</strong></li>
</ol>
</blockquote>
<h3 id="convergence-rate">Convergence Rate<a hidden class="anchor" aria-hidden="true" href="#convergence-rate">#</a></h3>
<ul>
<li>
<p>å‰æå‡è®¾ï¼š</p>
<p>L-smooth, $l_i$: convex, $f$: strong-convex</p>
</li>
<li>
<p>Bound $\mathbb{E}[||v_t||^2]$:</p>
</li>
</ul>
<p>ä»¤  $v_t = \nabla l_i(w_{t-1}) - \nabla l_i(\tilde{w}) + \tilde{u}$</p>
<p>$\mathbb{E}[||v_t||^{2}] = \mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(\tilde{w}) + \tilde{u})^2]$</p>
<p>å› ä¸º$  (a+b)^2 \leq 2a^2 + 2b^2 $ï¼š</p>
<p>$\leq 2\mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(w^*))^2] + 2\mathbb{E}[(\nabla l_i(w^*) - \nabla l_i(\tilde{w}) + \tilde{u})^2]$</p>
<p>$= 2\mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(w^*))^2] $</p>
<p>$+ 2\mathbb{E}[\left((\nabla l_i(w^*) - \nabla l_i(\tilde{w}))-\mathbb{E}[(\nabla l_i(w^*) - \nabla l_i(\tilde{w}))]\right)^2]$</p>
<p>åˆå› ä¸º$$\mathbb{E}[(x - \mathbb{E}[x])^2] = \mathbb{E}[x^2] - (\mathbb{E}[x])^2 \leq \mathbb{E}[x^2]:$$</p>
<p>æ‰€ä»¥$\mathbb{E}[||v_t||^{2}]$</p>
<p>$\leq 2\mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(w^*))^2] + 2\mathbb{E}[(\nabla l_i(w^*) - \nabla l_i(\tilde{w}))^2]$</p>
<p>æ ¹æ®Thm.1:</p>
<p>$\leq 4L(f(w_{t-1}) - f(w^*) + f(\tilde{w}) - f(w^*))$</p>
<ul>
<li>Bound $||w_t-w^*||$
$$
\mathbb{E}[|w_{t} - w^*|^2] = \mathbb{E}[|w_t - w_{t-1} + w_{t-1} - w^*|^2]
$$</li>
</ul>
<p>$$
= \mathbb{E}[|w_{t} - w^*|^2] + 2 \mathbb{E}[\langle w_t - w_{t-1}, w_{t-1} - w^* \rangle] + \mathbb{E}[|w_t - w_{t-1}|^2]
$$</p>
<p>$$
= |w_{t-1} - w^*|^2 - 2\eta \mathbb{E}[\langle v_t, w_{t-1} - w^* \rangle] + \eta^2 \mathbb{E}[v_t^2]
$$</p>
<p>$$
\leq |w_{t-1} - w^*|^2 - 2\eta \mathbb{E}[\langle v_t, w_{t-1} - w^* \rangle] + 4\eta^2 L(f(w_{t-1}) - f(w^*) + f(\tilde{w}) - f(w^*))
$$</p>
<p>$$
= |w_{t} - w^*|^2 - 2\eta \langle \nabla f(w_{t-1}), w_{t+1} - w^* \rangle + 4L\eta^2 (f(w_{t-1}) - f(w^*) + f(\tilde{w}) - f(w^*))
$$
åˆå› ä¸ºconvexityï¼š
$$
f(w_{t-1}) - f(w^*) \geq \langle \nabla f(w_{t-1}), w_{t-1} - w^* \rangle
$$</p>
<p>$$
\Rightarrow \mathbb{E}[|w_{t} - w^*|^2] \leq |w_{t-1} - w^*|^2 - 2\eta (f(w_{t-1}) - f(w^*)) + 4L\eta^2 (f(w_{t-1}) - f(w^*) + f(\tilde{w}) - f(w^*))
$$</p>
<p>$$
= |w_{t+1} - w^*|^2 + 4L\eta^2 (f(\tilde{w}) - f(w^*)) + 2\eta (2L\eta - 1)(f(w_{t-1}) - f(w^*))
$$</p>
<ul>
<li>Telescope</li>
</ul>
<p>ä»$\sum_{t=1}^{m}$,ç”¨option 2:
$$
\mathbb{E}[|w_m - w^*|^2] \leq \mathbb{E}[|\tilde{w} - w^*|^2] + 4mL\eta^2 (f(\tilde{w}) - f(w^*)) + 2m\eta (2L\eta - 1) \mathbb{E}[f(\tilde{w}_s) - f(w^*)]
$$</p>
<p>é‡æ–°æ•´ç†æˆ:</p>
<p>$$
\mathbb{E}[|w_m - w^*|^2] + 2m\eta  (1 - 2L\eta) \mathbb{E}[f(\tilde{w}_s) - f(w^*)]
$$</p>
<p>$$
\leq \mathbb{E}[|\tilde{w} - w^*|^2] + 4mL\eta^2 (f(\tilde{w}) - f(w^*))
$$</p>
<p>$$
\leq \left(\frac{2}{u} + 4mL\eta^2\right)(f(\tilde{w}) - f(w^*))
$$</p>
<p>æ‰€ä»¥</p>
<p>$$
\mathbb{E}[f(\tilde{w}_s) - f(w^*)] \leq (\frac{1}{u\eta (1 - 2L\eta)m} + \frac{2L\eta}{1-2L\eta}) $$</p>
<p>$$\cdot \mathbb{E} [f(\tilde{w}_{s - 1})-f(w^*)]$$</p>
<p>æ‰€ä»¥æ”¶æ•›ç‡æ˜¯Linear Convergence, $\frac{L}{u}$å¤§æ—¶æ¯”GDå¿«ã€‚</p>
<h2 id="15-mirror-descent">1.5 Mirror Descent<a hidden class="anchor" aria-hidden="true" href="#15-mirror-descent">#</a></h2>
<p><img loading="lazy" src="../img/ml1/image3.png"></p>
<h3 id="algorithm-2">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-2">#</a></h3>
<p>å¯¹äºä¸€ä¸ª1-strongly convexçš„Distance Generating Function$w(x)$,æˆ‘ä»¬å®šä¹‰Bergman Divergence:$$V_x(y)=w(y)-w(x)-\langle \nabla w(x),y-x \rangle$$
ç„¶åæˆ‘ä»¬å®šä¹‰:
$$\text{Mirror}_ {x}(\zeta) = \arg \min_ {y} { V_ {x}(y) + \langle \zeta, y - x \rangle } $$</p>
<p>ä¸€ä¸ªMirror Descentçš„å®šä¹‰æ˜¯
$$
x_{t+1} = \text{Mirror}_ {x_t} (\alpha \nabla f(x_t))
$$</p>
<p>$$
= \arg \min_{y} \left( w(y) - w(x_t) - \langle \nabla w(x_t), y - x_t \rangle + \alpha \langle \nabla f(x_t), y - x_t \rangle \right)
$$</p>
<h3 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h3>
<p>ç¬¬äºŒç§è§†è§’ç§°ä¸ºé•œåƒç©ºé—´ (Mirror space) è§†è§’,ä¸€ä¸ª Mirror step å¯ä»¥è¢«è§†ä½œå°†å¶ç©ºé—´ä¸Šçš„æ¢¯åº¦ä¸‹é™,å³æœå¦ä¸€ä¸ªæ–°çš„æå€¼ç‚¹è¿›è¡Œæœç´¢ã€‚è¿‡ç¨‹å½¢å¦‚ï¼š</p>
<ul>
<li>å°† $x$ é€šè¿‡ Mirror map æ˜ å°„åˆ°å¯¹å¶ç©ºé—´ä¸Šçš„ $\theta_k$ã€‚</li>
<li>$\theta_ {k+1} = \theta_ k - \alpha \nabla f(x_k)$ã€‚</li>
<li>å°† $\theta_ {k+1}$ æ˜ å°„å›åŸç©ºé—´ä¸Šçš„ $\overline{x} _{k+1}$ã€‚</li>
<li>å°† $\overline{x}_ {k+1}$ æŠ•å½±åˆ°çº¦æŸé›†,æŠ•å½±ä½¿ç”¨ Bregman divergence ä½œä¸ºå…¶è·ç¦»,å³ $x_ {k+1} = \arg \min_ {y} V_ {x_{k+1}}(y)$ã€‚</li>
</ul>
<p>æŒ‰ç…§ Mirror step çš„å¼å­,å¯ä»¥çœ‹å‡º Mirror map å°±æ˜¯ $\nabla w(\cdot)$ã€‚å› æ­¤å®é™…è¿‡ç¨‹ä¸ºï¼š</p>
<ul>
<li>$\theta_k = \nabla w(x)$ã€‚</li>
<li>$\theta_{k+1} = \theta_k - \alpha \nabla f(x_k)$ã€‚</li>
<li>$\overline{x}_{k+1} = (\nabla w)^{-1}(\theta{k+1})$ã€‚</li>
<li>$x_{k+1} = \arg \min_{y} V_{\overline{x}_{k+1}}(y)$ã€‚</li>
</ul>
<p>è¿™ä¸ªè§†è§’æå‡ºäº†ä¸€ç‚¹å‡è®¾,$(\nabla w)^{-1}(\overline{x}_{k+1})$ å§‹ç»ˆå­˜åœ¨,å³ ${\nabla w(x)} = \mathbb{R}^n$ã€‚</p>
<h3 id="relationship-between-gd--md">Relationship between GD &amp; MD<a hidden class="anchor" aria-hidden="true" href="#relationship-between-gd--md">#</a></h3>
<p>è¿™ä¸ªé—®é¢˜æ›¾å¾ˆé•¿ä¸€æ®µæ—¶é—´è®©ç¬”è€…æ„Ÿåˆ°å›°æƒ‘ã€‚ç¬”è€…å¯¹äºè¿™ä¸€å—å¹¶éå¾ˆæ‡‚,ç¬”è€…ç°åœ¨çš„ç†è§£æ˜¯:</p>
<p>æˆ‘ä»¬çŸ¥é“ä¸€ä¸ªPrimal Spaceå’ŒDual Spaceçš„èŒƒæ•°ä¹‹é—´æ»¡è¶³$\frac{1}{p}+\frac{1}{q}=1$</p>
<p>GDæ˜¯MDåœ¨ $\alpha=\frac{1}{L}$,primal spaceå–$||Â·||_2$èŒƒæ•°,Distance Generating Functionå– $w(x)=\frac{1}{2} x^2$ä¸‹çš„ç‰¹æ®Šæƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹,å› ä¸ºL2-normçš„Dualå°±æ˜¯L2-norm,æ‰€ä»¥è¿™ä¸ªå¯¹å¶ç©ºé—´å°±æ˜¯åŸç©ºé—´ã€‚</p>
<p>ä½†æ˜¯å¦ä¸€ç§ç†è§£æ–¹å¼æ˜¯,MDæ˜¯å…ˆé€šè¿‡æ¢¯åº¦æ˜ å°„åˆ°Dual Spaceä¹‹ååœ¨è¿™ä¸ªç©ºé—´ä¸‹åšGDå†é€†æ˜ å°„åprojectå›åŸæ¥çš„ç©ºé—´ä¸­ã€‚</p>
<h3 id="convergence-1">Convergence:<a hidden class="anchor" aria-hidden="true" href="#convergence-1">#</a></h3>
<ul>
<li>å‰ææ¡ä»¶:</li>
</ul>
<p>$f(x)$ convex, $w(x)$ 1-strongly convex, $\nabla f(x)\leq \rho$</p>
<ul>
<li>Bound $f(x_t)-f(x^*)$:</li>
</ul>
<p>å› ä¸ºconvexityï¼š
$$
\alpha (f(x_{t+1}) - f(u)) \leq \langle \alpha \nabla f(x_t), x_t - u \rangle $$
åˆå› ä¸ºMDçš„æ›´æ–°è§„åˆ™ï¼š
$$
x_{t+1} = \arg \min_{y} \left( V_{x_t}(y) + \langle \alpha \nabla f(x_t), y - x_t \rangle \right)
$$
æ‰€ä»¥è¯´ç”±æœ€å°å€¼ç‚¹æ¢¯åº¦ç­‰äº0:
$$
\alpha \nabla f(x_t) = - \nabla V_{x_t}(x_{t+1})
$$
å› æ­¤
$$
\alpha (f(x_t) - f(u)) \leq \langle \alpha \nabla f(x_t), x_t - x_{k+1} \rangle + \langle - \nabla V_{x_t}(x_{k+1}), x_{k+1} - u \rangle
$$
æ¥ä¸‹æ¥æˆ‘ä»¬è¯æ˜ä¸€ä¸ªé‡è¦çš„triangle inequality:
$$
\langle - \nabla V_{x_t}(y), y - u \rangle = \langle \nabla w(x) - \nabla w(y), y - u \rangle
$$</p>
<hr>
<p>$$
= (w(u) - w(x)) - \langle \nabla w(x), u - x \rangle - (w(y) - w(x) - \langle \nabla w(x), y - x \rangle)
$$</p>
<p>$$
= V_x(u) - V_x(y) - V_y(u)
$$
å¸¦å›åŸå¼:
$$
\alpha (f(x_t) - f(u))
\leq \langle \alpha \nabla f(x_t), x_t - x_{k+1} \rangle + V_{x_k}(u) - V_{x_k}(x_{k+1}) - V_{x_{k+1}}(u)
$$</p>
<p>ç”±äºDGFçš„1-strongly convex:</p>
<p>$$
\leq \langle \alpha \nabla f(x_t), x_t - x_{k+1} \rangle- \frac{1}{2} |x_{k+1} - x_t|^2 + V_{x_k}(u) - V_{x_{k+1}}(u)
$$
è¿™æ­¥æ˜¯å‰ä¸¤é¡¹åšä¸ªé…æ–¹æ³•:
$$
\leq \frac{\alpha^2}{2} |\nabla f(x_t)|^2 + V_{x_k}(u) - V_{x_k}(x_{k+1})
$$</p>
<ul>
<li>Telescoping:</li>
</ul>
<p>$$
\alpha T (f(\overline{x}) - f(x_t)) \leq \sum \text{LHS} \leq \sum \text{RHS}
$$
$$
\leq \frac{\alpha^2 T}{2} \cdot \rho^2 + V_{x_0}(x^*) - V_{x_T}(x^*)
$$
æ‰€ä»¥è¯´
$$
f(\overline{x}) - f(x^*) \leq \frac{\alpha}{2} \rho^2 + \frac{\Theta}{\alpha T}
$$
ä»¤$\alpha = \sqrt{\frac{2\Theta}{T \rho^2}}$.</p>
<p>æœ‰$f(x_T) - f(x^*) \leq \sqrt{\frac{2\Theta}{T }}\rho= \epsilon$
äºæ˜¯æˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„æ”¶æ•›ç‡
$$
T = \Omega \left( \frac{\rho^2}{\epsilon^2} \right)
$$</p>
<h2 id="16-linear-coupling">1.6 Linear Coupling<a hidden class="anchor" aria-hidden="true" href="#16-linear-coupling">#</a></h2>
<h3 id="wishful-thinking">Wishful Thinking<a hidden class="anchor" aria-hidden="true" href="#wishful-thinking">#</a></h3>
<p>æˆ‘ä»¬é€šè¿‡1.5çš„åˆ†æå·²ç»çŸ¥é“Mirror Descentæœ‰
$
T = O\left(\frac{\rho^2}{\epsilon^2}\right)
$çš„æ”¶æ•›ç‡</p>
<p>ç„¶åæˆ‘ä»¬çŸ¥é“åœ¨GDä¸­
$$
f(x_{t+1}) - f(x_t) \leq -\frac{1}{2L} |\nabla f(x_t)|^2
$$
æ‰€ä»¥è¯´åœ¨gradientæ¯”è¾ƒå¤§çš„æ—¶å€™:
$$
|\nabla f(x_t)| &gt; \rho : \Omega\left(\frac{L \epsilon}{\rho^2}\right) \text{ steps}
$$</p>
<p>åœ¨gradientæ¯”è¾ƒå°çš„æ—¶å€™MD:</p>
<p>$$
|\nabla f(x_t)| &lt; \rho : \Omega\left(\frac{\rho^2}{\epsilon^2}\right) \text{ steps}
$$
æ‰€ä»¥æˆ‘ä»¬æƒ³èƒ½ä¸èƒ½åœ¨æ¢¯åº¦å¤§çš„æ—¶å€™è·‘GD,åœ¨æ¢¯åº¦å°çš„æ—¶å€™è·‘MD,è¿™æ ·ä¼šè·å¾—ä¸€ä¸ªæ›´å¥½çš„æ”¶æ•›ç‡</p>
<p>Coupling:</p>
<p>$$\Omega ( \max { \frac{L \epsilon}{\rho^2}, \frac{\rho^2}{\epsilon^2} })$$
å–$\rho = (L \epsilon^{3})^\frac{1}{4}$:
$$ \Omega\left(\sqrt{\frac{L}{\epsilon}}\right) \text{ steps}
$$</p>
<h3 id="algorithm-3">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-3">#</a></h3>
<ul>
<li>åˆå§‹åŒ–
$$x_0 = y_0 = z_0$$</li>
<li>æ¯ä¸€æ­¥æ›´æ–°,æ›´æ–°$x$:
$$
x_{k+1} = \tau z_k + (1 - \tau) y_k
$$</li>
<li>æ›´æ–°$y$:</li>
</ul>
<p>$$
y_{k+1} = \arg \min_{y \in \mathcal{Q}} { \frac{L}{2} |y - x_{k+1}|^2 + \langle \nabla f(x_{k+1}), y - x_{k+1} \rangle }
$$</p>
<p>$$
= x_{k+1} - \frac{1}{L} \nabla f(x_{k+1}) \quad \text{(GD step)}
$$</p>
<ul>
<li>æ›´æ–°$z$:
$$
z_{k+1} = Mirror_{z_k} (\alpha \nabla f(x_{k+1}))
$$</li>
</ul>
<h3 id="convergence-2">Convergence<a hidden class="anchor" aria-hidden="true" href="#convergence-2">#</a></h3>
<p>æ ¹æ®MDçš„åˆ†æ:
$$
\alpha \langle \nabla f(x_{k+1}), z_k - u \rangle \leq \frac{\alpha^2}{2} |\nabla f(x_{k+1})|^2 + V_{z_k}(u) - V_{z_{k+1}}(u)
$$
ç”±äº
$$
f(x_{k+1}) - f(y_{k+1}) \geq \frac{1}{2L} |\nabla f(x_{k+1})|^2$$
æ‰€ä»¥åŸå¼
$$ \leq \alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u)
$$
åˆå› ä¸ºconvexity:
$$
\alpha (f(x_{k+1}) - f(u)) \leq \alpha \langle \nabla f(x_{k+1}), x_{k+1} - u \rangle
$$</p>
<p>$$
= \alpha \langle \nabla f(x_{k+1}), z_k - u \rangle + \alpha \langle \nabla f(x_{k+1}), x_{k+1} - z_k \rangle
$$
å‰é¢ä¸€é¡¹æˆ‘ä»¬å·²ç»MDåšæ‰äº†,åé¢ä¸€é¡¹
$$
\alpha \langle \nabla f(x_{k+1}), x_{k+1} - z_k \rangle
$$</p>
<p>$$
= \frac{(1 - \tau) \alpha}{\tau} \langle \nabla f(x_{k+1}), y_k - x_{k+1} \rangle
$$</p>
<p>$$
\leq \frac{(1 - \tau) \alpha}{\tau} (f(y_k) - f(x_{k+1}))
$$
æ‰€ä»¥è¯´
$$
\alpha (f(x_{k+1}) - f(u)) \leq \alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + \frac{(1 - \tau) \alpha}{\tau} (f(y_k) - f(x_{k+1}))
$$</p>
<p>$$+ V_{z_k}(u) - V_{z_{k+1}}(u)
$$
ä»¤
$
\frac{(1 - \tau) \alpha}{\tau} = \alpha^2 L
$,
æœ‰
$$
f(x_{k+1}) - f(u) \leq \alpha^2 L (f(y_k) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u)
$$</p>
<p>Telescope:</p>
<p>$$
\alpha T (f(\overline{x}) - f(x^*)) \leq \alpha^2 L (f(y_0) - f(y_T)) + V_{x_0}(x^*) - V_{z_T}(x^*)
$$</p>
<p>å‡è®¾ $f(y_0) - f(x^*) = d$, $V_{x_0}(x^*) = \Theta$
æœ‰
$$
f(x_i) - f(x^*) \leq \frac{\alpha dL}{T} + \frac{\Theta}{\alpha T}
$$
ä»¤$
\alpha = \sqrt{\frac{\Theta}{dL}}$,
æœ‰
$$ f(\overline{x}) - f(x^*) \leq \frac{2 \sqrt{\Theta Ld}}{T}$$</p>
<p>å– $ T = 4 \sqrt{\frac{L\Theta}{d}}$,
æœ‰$$f(\overline{x})-f(x^*)\leq \frac{d}{2}$$
æ‰€ä»¥è¯´æˆ‘ä»¬æ¯ $2\epsilon\rightarrow \epsilon$è¿‡ç¨‹é‡æ–°è°ƒæ•´ä¸€æ¬¡$\tau,\alpha$,æœ€åå¾—åˆ°çš„è¿­ä»£æ¬¡æ•°æ˜¯:
$$O(\sqrt{\frac{L \Theta}{\epsilon}})+O(\sqrt{\frac{L \Theta}{2\epsilon}})+O(\sqrt{\frac{L \Theta}{4\epsilon}})+&hellip;=O(\sqrt{\frac{L \Theta}{\epsilon}})$$
Nesterovå‘Šè¯‰æˆ‘ä»¬$O(\frac{1}{T^2})$(aka.$O(\sqrt{\frac{L}{\epsilon}})$)å°±æ˜¯æˆ‘ä»¬å¯¹äºconvexä¸”L-smoothå‡½æ•°èƒ½å¾—åˆ°çš„æœ€å¥½ç»“æœäº†,æ‰€ä»¥Linear Couplingç¡®å®å¾ˆç‰›ã€‚</p>
<h2 id="17-non-convex-optimization">1.7 Non-Convex Optimization<a hidden class="anchor" aria-hidden="true" href="#17-non-convex-optimization">#</a></h2>
<h3 id="matrix-completion">Matrix Completion<a hidden class="anchor" aria-hidden="true" href="#matrix-completion">#</a></h3>
<p>$A \in \mathbb{R}^{m \times n}$æ»¡è¶³ä»¥ä¸‹å‡è®¾:</p>
<blockquote>
<p>1Â° $A$ is low rank</p>
<p>2Â° Known entries are uniformly distributed</p>
<p>3Â° <strong>Incoherence</strong>: $$
A = U \Sigma V^T \quad \text{for } i \in [n], j \in [m]$$ $$\exists \mu: 1 \leq \mu \leq \frac{min(m,n)}{r}$$$$
|e_i^T U| \leq \sqrt{\frac{\mu r}{n}}, \quad |e_j^T V| \leq \sqrt{\frac{\mu r}{m}}$$</p>
</blockquote>
<p>é‚£ä¹ˆæˆ‘ä»¬çš„ç›®æ ‡($P_\Omega$ä»£è¡¨ä¸çŸ¥é“çš„å…ƒç´ éƒ½maskæ‰):
$$
\min |P_\Omega(UV^T) - P_\Omega(A)|_F^2
$$
å¯ä»¥æœ‰ä»¥ä¸‹ç®—æ³•:</p>
<blockquote>
<p>Algorithm:</p>
<p>For $t = 0, 1, 2, \ldots, T$</p>
<ul>
<li>$V^{t+1} \leftarrow \arg \min_V ||P_{\Omega}(U^t V) - P_{\Omega}(A)||_F^2$</li>
<li>$U^{t+1} \leftarrow \arg \min_U ||P_{\Omega}(U V^{t}) - P_{\Omega}(A)||_F^2$</li>
</ul>
</blockquote>
<h3 id="escaping-saddle-points">Escaping Saddle Points<a hidden class="anchor" aria-hidden="true" href="#escaping-saddle-points">#</a></h3>
<p>SGDåœ¨éå‡¸ä¼˜åŒ–ä¸­æœ‰ä¸€äº›GDä¹‹ç±»ç®—æ³•æ²¡æœ‰çš„å¥½å¤„,è¿™å°±æ˜¯å™ªå£°æ‰€å¸¦æ¥çš„éšæœºæ€§æ‰€å±•ç°çš„ä¼˜åŠ¿:</p>
<blockquote>
<p><strong>Thm.</strong><!-- raw HTML omitted -->If ğ¿ is <strong>smooth, bounded and strict saddle</strong> (actually more general version, applies to points with small gradients, rather than zero gradients), and <strong>Hessian is smooth</strong>. If <strong>SGD noise has non-negligible variance in every direction with constant probability</strong>, SGD will <strong>escape all saddle points and local maxima, converge to a local minimum after polynomial number of steps.</strong></p>
</blockquote>
<p>å…¶ä¸­Strict Saddle Pointæ˜¯æŒ‡ä¸€ä¸ªç‚¹$\nabla f(x)=0$, $\nabla^2 f(x)$åˆæœ‰æ­£ç‰¹å¾å€¼åˆæœ‰è´Ÿç‰¹å¾å€¼ã€‚Flat Saddle Pointæ˜¯æŒ‡ä¸€ä¸ªç‚¹$\nabla f(x)=0$, $\nabla^2 f(x)$çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½å¤§äºç­‰äº0,ä¸”æœ‰ä¸€ä¸ªç­‰äº0çš„ç‰¹å¾å€¼ã€‚</p>
<h1 id="2generalization">2.Generalization<a hidden class="anchor" aria-hidden="true" href="#2generalization">#</a></h1>
<h2 id="21-no-free-lunch-thm">2.1 No Free Lunch Thm.<a hidden class="anchor" aria-hidden="true" href="#21-no-free-lunch-thm">#</a></h2>
<blockquote>
<p><em>Thm.</em>
è®¾ $A$ ä¸ºåœ¨å®šä¹‰åŸŸ $\mathcal{X}$ ä¸Šç›¸å¯¹äº 0-1 æŸå¤±çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡çš„ä»»æ„å­¦ä¹ ç®—æ³•ã€‚è®¾ $m$ ä¸ºå°äº $|\mathcal{X}|/2$ çš„ä»»æ„æ•°,è¡¨ç¤ºè®­ç»ƒé›†å¤§å°ã€‚åˆ™å­˜åœ¨ä¸€ä¸ªåœ¨ $\mathcal{X} \times {0, 1}$ ä¸Šçš„åˆ†å¸ƒ $\mathcal{D}$ ä½¿å¾—ï¼š</p>
<ol>
<li>å­˜åœ¨ä¸€ä¸ªå‡½æ•° $f : \mathcal{X} \to {0, 1}$,ä½¿å¾— $L_\mathcal{D}(f) = 0$ã€‚</li>
<li>ä»¥è‡³å°‘ $1/7$ çš„æ¦‚ç‡,å¯¹äºä» $\mathcal{D}^m$ ä¸­é€‰å–çš„ $S$,æœ‰ $L_\mathcal{D}(A(S)) \geq 1/8$ã€‚</li>
</ol>
</blockquote>
<p>è¿™ä¸ªçš„ç›´è§‰åœ¨äºç”±Markovä¸ç­‰å¼,$\mathbb{E}_{S \sim D^m }[L_D(A(S))]\geq \frac{1}{4}$,ä¹Ÿå°±æ˜¯è¯´å¯¹äºä¸€ä¸ªå®Œå…¨é èƒŒè¯µçš„ç®—æ³•: å‡å¦‚è§è¿‡$(X,y)$,è¾“å‡º$y$,å‡å¦‚æ²¡è§è¿‡å°±éšæœºè¾“å‡º0æˆ–1ã€‚è¿™æ ·å¯¹äºä¸€ä¸ª$|C|=2m$çš„$X$çš„å­é›†,è¿™æ ·â€œèƒŒè¯µ+çè’™â€çš„loss functionæ˜¯$\frac{1}{4}$ã€‚ä¹Ÿå°±æ˜¯è¯´,æ²¡æœ‰ä»€ä¹ˆåŠæ³•èƒ½å¤Ÿä»æœŸæœ›ä¸Šæ¯”â€œèƒŒè¯µ+çè’™â€æ•ˆæœæ›´å¥½,ä¹Ÿå°±æ˜¯è¯´å­¦ä¹ ç®—æ³•å¤±è´¥äº†ã€‚</p>
<p><strong>è¯æ˜</strong>:</p>
<p>ä¸ºäº†ç®€æ´æ€§,ä¸å¦¨è®¾$|C| = 2m$.</p>
<p>è®°$T = 2^{2m}$ã€‚ä»$C$åˆ°${0, 1}$çš„å‡½æ•°ä¸€å…±æœ‰$f_1, \ldots, f_T$,å…±$T$ä¸ª</p>
<p>è®°
$$
D_i({x, y}) =
\frac{1}{|C|} \quad \text{if } y = f_i(x)
$$
$$
D_i({x, y}) = 0 \quad \text{otherwise.}
$$</p>
<p>æ˜¾ç„¶,$L_{D_i}(f_i) = 0$.</p>
<p>æˆ‘ä»¬æ¥ä¸‹æ¥è¯æ˜:</p>
<p>$$\max_{i \in [T]} E_{S \sim D_{i}^{m}} [ L_{D_i}(A(S)) ] \geq \frac{1}{4}$$</p>
<hr>
<p>è®°ä¸€å…±æœ‰$k$ä¸ªå¯èƒ½çš„ä»$C$ä¸­å–æ ·å‡ºçš„$m$ä¸ªæ•°æ®ç‚¹$x_i$åºåˆ—:
æœ‰$k = (2m)^m$,è®°
$S_j = (x_1, \ldots, x_m)$
,è®°
$S_j^i = \left( (x_1, f_i(x_1)), \ldots, (x_m, f_i(x_m)) \right)$ã€‚</p>
<p>æˆ‘ä»¬åªéœ€è¦å–å‡ºä¸€ä¸ª$i \in [T]$èƒ½å¤Ÿè®©$E_{S \sim D_i^m} \left[ L_{D_i}(A(S)) \right]\geq \frac{1}{4}$,é‚£ä¹ˆå¯¹åº”çš„$D_i$ä¾¿æ˜¯æˆ‘ä»¬åœ¨NFLä¸­æ‰€å¸Œæœ›æ‰¾åˆ°çš„$D$ã€‚
$$
\max_{i \in [T]} E_{S \sim D_i^m} \left[ L_{D_i}(A(S)) \right]
$$</p>
<p>$$
= \max_{i \in [T]} \frac{1}{k} \sum_{j=1}^k L_{D_i}(A(S_j^i))
$$</p>
<p>$$
\geq \frac{1}{T} \sum_{i=1}^T \frac{1}{k} \sum_{j=1}^k L_{D_i}(A(S_j^i))
$$</p>
<p>$$
= \frac{1}{k} \sum_{j=1}^k \frac{1}{T} \sum_{i=1}^T L_{D_i}(A(S_j^i))
$$</p>
<p>$$
\geq \min_{j \in [k]} \frac{1}{T} \sum_{i=1}^T L_{D_i}(A(S_j^i))
$$
å¯¹äºç»™å®šçš„ $j$:</p>
<p>ä»¤$v_1, \ldots, v_p$ ä¸º$S_j$ä¸­æ²¡æœ‰å‡ºç°çš„$x\in C$, æ³¨æ„åˆ°$p \geq m$ã€‚</p>
<p>$$
L_{D_i}(A(S_j^i)) = \frac{1}{2m} \sum_{x \in C} \mathbf{1}[h(x) \neq f_i(x)]
$$</p>
<p>$$
\geq \frac{1}{2m} \sum_{r=1}^p \mathbf{1}[h(v_r) \neq f_i(v_r)]
$$</p>
<p>$$
\geq \frac{1}{2p} \sum_{r=1}^p \mathbf{1}[h(v_r) \neq f_i(v_r)]
$$</p>
<p>æ‰€ä»¥è¯´</p>
<p>$$
\frac{1}{T} \sum_{i=1}^T L_{D_i}(A(S_j^i))
$$</p>
<p>$$
\geq \frac{1}{T} \sum_{i=1}^T \frac{1}{2p} \sum_{r=1}^p \mathbf{1}[h(v_r) \neq f_i(v_r)]
$$</p>
<p>æˆ‘ä»¬å¯ä»¥å°† $f_1, \ldots, f_T$ ä¸­çš„æ‰€æœ‰å‡½æ•°åˆ’åˆ†æˆ $T/2$ å¯¹ä¸ç›¸äº¤çš„å‡½æ•°å¯¹,å…¶ä¸­å¯¹äºæ¯ä¸€å¯¹ $(f_i, f_{i&rsquo;})$,å¯¹äºä»»æ„ $c \in C$,éƒ½æœ‰ $f_i(c) \neq f_{i&rsquo;}(c)$ã€‚</p>
<p>äºæ˜¯æœ‰
$$
\frac{1}{2p} \sum_{r=1}^p \frac{1}{T} \sum_{i=1}^T \mathbf{1}[h(v_r) \neq f_i(v_r)] = \frac{1}{4}
$$</p>
<p>æ‰€ä»¥è¯´
$$
\max_{i \in [T]} E_{S \sim D_i^m} \left[ L_{D_i}(A(S)) \right] \geq \frac{1}{4}
$$</p>
<hr>
<p>ä»¤$\mathcal{D} = D_i$:</p>
<p>å¦‚æœ</p>
<p>$$
\Pr \left[ L_{\mathcal{D}}(A(S)) \geq \frac{1}{8} \right] &lt; \frac{1}{7}
$$</p>
<p>é‚£ä¹ˆ</p>
<p>$$
E_{S \sim \mathcal{D}^m} \left[ L_{\mathcal{D}}(A(S)) \right] &lt; \frac{1}{7} \cdot 1 + \frac{6}{7} \cdot \frac{1}{8}
$$</p>
<p>$$
= \frac{1}{7} + \frac{3}{28} = \frac{1}{4}.\quad\blacksquare
$$</p>
<h2 id="22-pac-learning">2.2 PAC-Learning<a hidden class="anchor" aria-hidden="true" href="#22-pac-learning">#</a></h2>
<h3 id="ä¸€äº›æ¦‚å¿µ">ä¸€äº›æ¦‚å¿µ:<a hidden class="anchor" aria-hidden="true" href="#ä¸€äº›æ¦‚å¿µ">#</a></h3>
<ul>
<li><strong>Hypothesis Class (H)</strong> ï¼šèƒ½å¤Ÿé€‰æ‹©çš„å‡è®¾$h$çš„é›†åˆ</li>
<li><strong>$ERM_H$</strong> ï¼šé€‰æ‹©å…·æœ‰æœ€å°empirical lossçš„å‡è®¾</li>
</ul>
<p>$$
ERM_H(S) \in \arg\min_{h \in H} L_S(h)
$$</p>
<ul>
<li><strong>Realizability Assumption</strong>: å­˜åœ¨ $h^* \in H$ ä½¿å¾— $L_{D,f}(h^*) = 0$ã€‚è¿™æ„å‘³ç€å¯¹äºæ¯ä¸ªè®­ç»ƒé›† $S$,æˆ‘ä»¬æœ‰ $L_S(h^*) = 0$ã€‚</li>
<li><strong>PAC-Learnable</strong>: å¦‚æœå­˜åœ¨ä¸€ä¸ªå‡½æ•° $m_H: (0,1)^2 \to \mathbb{N}$ å’Œä¸€ä¸ªlearning algorithm,ä½¿å¾—å¯¹äºä»»æ„çš„ $\epsilon, \delta \in (0,1)$,å¯¹äºå®šä¹‰åœ¨ $X$ ä¸Šçš„ä»»æ„åˆ†å¸ƒ $D$,ä»¥åŠä»»æ„labeling function $f: X \to {0,1}$,è‹¥Realizability Assumptionåœ¨ $H, D, f$ ä¸‹æˆç«‹,åˆ™å½“åœ¨ç”± $D$ ç”Ÿæˆå¹¶ç”± $f$ æ ‡è®°çš„ $m \geq m_H(\epsilon, \delta)$ ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒæ ·æœ¬ä¸Šè¿è¡Œè¯¥learning algorithmæ—¶,è¯¥ç®—æ³•è¿”å›ä¸€ä¸ªå‡è®¾ $h$,ä½¿å¾—ä»¥è‡³å°‘ $1 - \delta$ çš„æ¦‚ç‡ï¼ˆåœ¨æ ·æœ¬é€‰æ‹©çš„éšæœºæ€§ä¸Šï¼‰,$L_{D,f}(h) \leq \epsilon$ã€‚</li>
</ul>
<h3 id="finite-classes-are-pac-learnable">Finite Classes are PAC-learnable<a hidden class="anchor" aria-hidden="true" href="#finite-classes-are-pac-learnable">#</a></h3>
<blockquote>
<p><strong>Thm.</strong> ç»™å®š $\delta \in (0,1)$, $\epsilon &gt; 0$, å¦‚æœ $m \geq \frac{\log(|H|/\delta)}{\epsilon}$,é‚£ä¹ˆå¦‚æœRealizability Assumptionæˆç«‹, é‚£ä¹ˆå¯¹äºä»»æ„ERM hypothesis $h_S$:
$$
\Pr [ L_D(h_S) \leq \epsilon ] \geq 1 - \delta
$$
<strong>Pf.</strong> æˆ‘ä»¬æƒ³è¦upper bound</p>
</blockquote>
<p>$$
\Pr_{S\sim \mathcal{D}^m} [ S | L_D(h(S)) &gt; \epsilon ]
$$</p>
<p>å®šä¹‰æ‰€æœ‰ä¸å¥½çš„å‡è®¾çš„é›†åˆä¸º:
$$
H_B := { h \in H | L_D(f, h) &gt; \epsilon }
$$
å®šä¹‰misleadingçš„å‡è®¾çš„é›†åˆä¸ºï¼š
$$
M := { S \mid \exists h \in H_B, L_S(h) = 0 }
$$
æœ‰
$$
{ S \mid L_D(h(S)) &gt; \epsilon } \subseteq M
$$
æ‰€ä»¥
$$
\Pr \left[ L_D(h(S)) &gt; \epsilon \right] \leq \Pr \left[ S \in M \right] \leq \sum_{h \in H_B} \Pr \left[ L_S(h) = 0 \right]
$$
åˆå› ä¸º
$$
\Pr \left[ L_S(h) = 0 \right] = \prod_{i=1}^m Pr_{x_i\sim\mathcal{D}} \left[ h(x_i) = f(x_i) \right]
$$</p>
<p>å› ä¸º
$$
Pr_{x_i\sim\mathcal{D}} \left[ h(x_i) = f(x_i) \right] = 1 - L_D(f, h) \leq 1 - \epsilon
$$
æ‰€ä»¥</p>
<p>$$
\Pr \left[ L_S(h) = 0 \right] \leq (1 - \epsilon)^m \leq e^{-m \epsilon}
$$</p>
<p>$$
|H| \cdot e^{-m \epsilon} \leq \delta \implies m = \frac{\log(|H|/\delta)}{\epsilon}. \blacksquare
$$</p>
<h3 id="threshold-functions-are-pac-learnable">Threshold Functions are PAC-learnable<a hidden class="anchor" aria-hidden="true" href="#threshold-functions-are-pac-learnable">#</a></h3>
<ul>
<li>Threshold Functions:
$$
\mathcal{H}={h(x) = \mathbf{1}[x &lt; a]}
$$
<img loading="lazy" src="../img/ml1/image4.png">
æ³¨æ„åˆ°è¿™æ˜¯ä¸€ä¸ªinfinite classã€‚</li>
</ul>
<blockquote>
<p><strong>Thm.</strong> è®¾ $H$ ä¸ºThreshold Functionsã€‚åˆ™ $H$ æ˜¯ PAC-learnableçš„,ä½¿ç”¨ ERM ç®—æ³•,å…¶æ ·æœ¬å¤æ‚åº¦ä¸º$$
m_H(\epsilon, \delta) \leq \frac{\lceil \log(2/\delta) \rceil}{\epsilon}$$</p>
</blockquote>
<ul>
<li><strong>Pf.</strong></li>
</ul>
<p>è®°$
h^*(x) = \mathbf{1}[x &lt; a^*]
$s.t.$L_D(h^*)=0$</p>
<p>å®šä¹‰
$$
b_0 := \sup {x \mid (x, 1) \in S}, \quad b_1 := \inf {x \mid (x, 0) \in S}
$$
<img loading="lazy" src="../img/ml1/image11.png">
æ³¨æ„åˆ°
$$
\Pr \left[ L_D(h) &gt; \epsilon \right] \leq \Pr \left[ b_0 &lt; a_0 \right] + \Pr \left[ b_1 &gt; a_1 \right]
$$
åœ¨$
m = \frac{\ln \left(\frac{2}{\delta}\right)}{\epsilon}
$çš„æƒ…å†µä¸‹:
$$
\Pr \left[ b_0 &lt; a_0 \right] = (1 - \epsilon)^m \leq e^{-\epsilon m} = \frac{\delta}{2}
$$</p>
<p>$$
\Pr \left[ b_1 &gt; a_1 \right] = (1 - \epsilon)^m \leq e^{-\epsilon m} = \frac{\delta}{2}. \blacksquare
$$</p>
<h2 id="23-agnostic-pac-learnable">2.3 Agnostic PAC-Learnable<a hidden class="anchor" aria-hidden="true" href="#23-agnostic-pac-learnable">#</a></h2>
<p>æœ‰æ—¶å€™Realizability Assumptionå¤ªå¼ºäº†,æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªåœ¨$\mathcal{H}$ä¸­æ²¡æœ‰Loss=0çš„hypothesisçš„æƒ…å†µä¸‹è¡¡é‡estimation errorçš„æ‰‹æ®µ:</p>
<h3 id="agnostic-pac-learnable"><strong>Agnostic PAC-Learnable</strong>:<a hidden class="anchor" aria-hidden="true" href="#agnostic-pac-learnable">#</a></h3>
<p>ä¸€ä¸ªå‡è®¾ç±» $H$ æ˜¯ Agnostic PAC å¯å­¦ä¹ çš„,å¦‚æœå­˜åœ¨ä¸€ä¸ªå‡½æ•° $m_H: (0,1)^2 \rightarrow \mathbb{N}$ å’Œä¸€ä¸ªå…·æœ‰ä»¥ä¸‹æ€§è´¨çš„å­¦ä¹ ç®—æ³•ï¼šå¯¹äºæ¯ä¸€ä¸ª $\epsilon, \delta \in (0,1)$,ä»¥åŠå®šä¹‰åœ¨ $X \times Y$ ä¸Šçš„æ¯ä¸ªåˆ†å¸ƒ $D$,å½“åœ¨ç”± $D$ ç”Ÿæˆçš„ $m \geq m_H(\epsilon, \delta)$ ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰æ ·æœ¬ä¸Šè¿è¡Œè¯¥å­¦ä¹ ç®—æ³•æ—¶,ç®—æ³•ä¼šè¿”å›ä¸€ä¸ªå‡è®¾ $h$,ä½¿å¾—ä»¥è‡³å°‘ $1 - \delta$ çš„æ¦‚ç‡ï¼ˆå¯¹äº $m$ ä¸ªè®­ç»ƒæ ·æœ¬çš„é€‰æ‹©è€Œè¨€ï¼‰,æ»¡è¶³</p>
<p>$$
L_D(h) \leq \min_{h&rsquo; \in H} L_D(h&rsquo;) + \epsilon
$$</p>
<h3 id="error-decomposition"><strong>Error Decomposition</strong>:<a hidden class="anchor" aria-hidden="true" href="#error-decomposition">#</a></h3>
<ul>
<li>$L_D(h_S) = \epsilon_{app} + \epsilon_{est}$</li>
<li>$\epsilon_{app} = \min_{h \in H} L_D(h)$</li>
<li>$\epsilon_{est} = L_D(h_S) - \epsilon_{app}$</li>
<li>$\epsilon_{app} = L_D(BO) + \min_{h \in H} L_D(h) - L_D(BO)$</li>
</ul>
<p>$\epsilon_{app}$æè¿°çš„æ˜¯è¿™ä¸ªhypothesis classçš„inductive biasçš„å¤šå°‘,è€Œ$\epsilon_{est}$æ˜¯ä¸sample sizeå’Œsample complexityç›¸å…³çš„(sample complexityä¸hypothesis classçš„representation poweræˆæ­£æ¯”),æ‰€ä»¥è¯´å½“æˆ‘ä»¬æƒ³è¦å‡å°‘$L_D(h_S)$,æˆ‘ä»¬é¢ä¸´ä¸€ä¸ªbias-complexity tradeoffã€‚</p>
<p>å…¶ä¸­BOæŒ‡ä»£çš„æ˜¯Bayes Optimal Predictorã€‚</p>
<h3 id="bayes-optimal-predictor">Bayes Optimal Predictor<a hidden class="anchor" aria-hidden="true" href="#bayes-optimal-predictor">#</a></h3>
<p>ç»™å®šä»»ä½•åœ¨ $X \times {0,1}$ ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ $D$,ä» $X$ åˆ° ${0,1}$ çš„æœ€ä½³æ ‡ç­¾é¢„æµ‹å‡½æ•°ä¸º</p>
<p>$$
f_D(x) = 1 \quad \text{if } P[y = 1 \mid x] \geq \frac{1}{2}
$$
$$
f_D(x) = 0 \quad \text{otherwise}
$$</p>
<p>å¾ˆå®¹æ˜“éªŒè¯,å¯¹äºæ¯ä¸ªæ¦‚ç‡åˆ†å¸ƒ $D$,è´å¶æ–¯æœ€ä¼˜é¢„æµ‹å™¨ $f_D$ æ˜¯æœ€ä¼˜çš„,å› ä¸ºæ²¡æœ‰å…¶ä»–åˆ†ç±»å™¨ $g: X \rightarrow {0,1}$ çš„é”™è¯¯ç‡æ›´ä½ã€‚å³,å¯¹äºæ¯ä¸ªåˆ†ç±»å™¨ $g$,æœ‰</p>
<p>$$
L_D(f_D) \leq L_D(g)
$$</p>
<h2 id="24-vc-dim">2.4 VC-Dim<a hidden class="anchor" aria-hidden="true" href="#24-vc-dim">#</a></h2>
<h3 id="restriction-of-h-to-c">Restriction of $H$ to $C$<a hidden class="anchor" aria-hidden="true" href="#restriction-of-h-to-c">#</a></h3>
<p>è®¾ $H$ æ˜¯ä» $X$ åˆ° ${0,1}$ çš„å‡½æ•°ç±»,$C = {c_1, \cdots, c_m} \subseteq X$ã€‚$H$ åœ¨ $C$ ä¸Šçš„é™åˆ¶æ˜¯ä» $C$ åˆ° ${0,1}$ çš„å‡½æ•°é›†åˆ,è¿™äº›å‡½æ•°å¯ä»¥ä» $H$ ä¸­å¯¼å‡ºã€‚å³</p>
<p>$$
H_C = {(h(c_1), \cdots, h(c_m)) : h \in H}
$$</p>
<p>æˆ‘ä»¬å°†ä» $C$ åˆ° ${0,1}$ çš„æ¯ä¸ªå‡½æ•°è¡¨ç¤ºä¸º ${0,1}^{|C|}$ ä¸­çš„ä¸€ä¸ªå‘é‡ã€‚</p>
<h3 id="shattering">Shattering<a hidden class="anchor" aria-hidden="true" href="#shattering">#</a></h3>
<p>ä¸€ä¸ªå‡è®¾ç±» $H$ Shatteræœ‰é™é›† $C \subseteq X$,å¦‚æœ Restriction of $H$ to $C$æ˜¯ä» $C$ åˆ° ${0,1}$ çš„æ‰€æœ‰å‡½æ•°é›†åˆã€‚å³</p>
<p>$$
|H_C| = 2^{|C|}
$$</p>
<h3 id="nfl-reexpressed">NFL Reexpressed<a hidden class="anchor" aria-hidden="true" href="#nfl-reexpressed">#</a></h3>
<p>è®¾ $H$ æ˜¯ä» $X$ åˆ° ${0,1}$ çš„hypothesis classã€‚ä»¤ $m$ ä¸ºè®­ç»ƒé›†å¤§å°ã€‚å‡è®¾å­˜åœ¨ä¸€ä¸ªå¤§å°ä¸º $2m$ çš„é›†åˆ $C \subseteq X$,å®ƒè¢« $H$ shatterã€‚åˆ™å¯¹äºä»»æ„å­¦ä¹ ç®—æ³• $A$,å­˜åœ¨ä¸€ä¸ªå®šä¹‰åœ¨ $X \times {0,1}$ ä¸Šçš„åˆ†å¸ƒ $D$ å’Œä¸€ä¸ªé¢„æµ‹å™¨ $h \in H$,ä½¿å¾— $L_D(h) = 0$,ä½†ä»¥è‡³å°‘ $\frac{1}{7}$ çš„æ¦‚ç‡,å¯¹äº $S \sim D^m$ çš„é€‰æ‹©,æœ‰</p>
<p>$$
L_D(A(S)) \geq \frac{1}{8}
$$</p>
<h3 id="vc-dimension">VC-Dimension<a hidden class="anchor" aria-hidden="true" href="#vc-dimension">#</a></h3>
<p>Hypothesis class $H$ çš„ VC-dimensionï¼ˆè®°ä½œ $\text{VCdim}(H)$ï¼‰æ˜¯ $H$ å¯ä»¥shatterçš„é›†åˆ $C \subseteq X$ çš„æœ€å¤§å¤§å°ã€‚å¦‚æœ $H$ å¯ä»¥shatterä»»æ„å¤§çš„é›†åˆ,æˆ‘ä»¬ç§° $\text{VCdim}(H)=+ \infty$.</p>
<h3 id="inifite-vc-dim-hypothesis-classes-are-not-pac-learnable">Inifite VC-dim hypothesis classes are not PAC-learnable<a hidden class="anchor" aria-hidden="true" href="#inifite-vc-dim-hypothesis-classes-are-not-pac-learnable">#</a></h3>
<p>NFLçš„ç›´æ¥åæœå°±æ˜¯$\text{VCdim}(H)=+ \infty$çš„$H$ä¸æ˜¯PAC-learnableçš„ã€‚</p>
<h2 id="25-fundamental-theorem-of-statistical-learning">2.5 Fundamental theorem of statistical learning<a hidden class="anchor" aria-hidden="true" href="#25-fundamental-theorem-of-statistical-learning">#</a></h2>
<p>è®¾ $H$ æ˜¯ä»ä¸€ä¸ªåŸŸ $X$ åˆ° ${0,1}$ çš„hypothesis class,å¹¶ä¸”æŸå¤±å‡½æ•°æ˜¯ 0-1 æŸå¤±ã€‚å‡è®¾ $\text{VCdim}(H) = d &lt; \infty$ã€‚åˆ™å­˜åœ¨å¸¸æ•° $C_1, C_2$,ä½¿å¾—</p>
<p>$H$ æ˜¯å…·æœ‰ä»¥ä¸‹æ ·æœ¬å¤æ‚åº¦çš„Agnostic PAC-learnableï¼š</p>
<p>$$
C_1 \frac{d + \log \left(\frac{1}{\delta}\right)}{\epsilon^2} \leq m_H(\epsilon, \delta) \leq C_2 \frac{d + \log \left(\frac{1}{\delta}\right)}{\epsilon^2}
$$</p>
<p>$H$ æ˜¯å…·æœ‰ä»¥ä¸‹æ ·æœ¬å¤æ‚åº¦çš„ PAC-learnableï¼š</p>
<p>$$
C_1 \frac{d + \log \left(\frac{1}{\delta}\right)}{\epsilon} \leq m_H(\epsilon, \delta) \leq C_2 \frac{d \log \left(\frac{1}{\epsilon}\right) + \log \left(\frac{1}{\delta}\right)}{\epsilon}
$$</p>
<h1 id="3supervised-learning">3.Supervised Learning<a hidden class="anchor" aria-hidden="true" href="#3supervised-learning">#</a></h1>
<ul>
<li>å¯¹äºå›å½’é—®é¢˜,æˆ‘ä»¬æ„é€ ä¸€ä¸ªå‡½æ•°$f: X\rightarrow \mathbb{R}$</li>
<li>åœ¨åˆ†ç±»é—®é¢˜ä¸­,æˆ‘ä»¬æ„é€ ä¸€ä¸ªå‡½æ•°$f: X\rightarrow {0,1}$æˆ–è€…${-1,1}$ã€‚</li>
</ul>
<p>å‰è€…æˆ‘ä»¬çš„loss functionå¾ˆå¥½design,æ¯”å¦‚è¯´Mean Square Loss,ä½†æ˜¯åè€…çš„losså°±ä¸æ˜¯ç‰¹åˆ«å¥½designã€‚ä¸€ç§è‡ªç„¶çš„æƒ³æ³•æ˜¯$f(x)=sign(w^Tx)$,ä½†æ˜¯é—®é¢˜å°±æ˜¯è¿™ä¸ªlossä¸å¯å¯¼,ä¸‹é¢æ˜¯ä¸€ç§åˆ©ç”¨è¿™ç§å‡½æ•°ä½†æ˜¯ä¸éœ€è¦å¯¼æ•°çš„è¿œå¤ç®—æ³•ã€‚</p>
<h2 id="31-perceptron">3.1 Perceptron<a hidden class="anchor" aria-hidden="true" href="#31-perceptron">#</a></h2>
<h3 id="algorithm-4">Algorithm<a hidden class="anchor" aria-hidden="true" href="#algorithm-4">#</a></h3>
<p><img loading="lazy" src="../img/ml1/image10.png"></p>
<h3 id="convergence-3">Convergence<a hidden class="anchor" aria-hidden="true" href="#convergence-3">#</a></h3>
<blockquote>
<p>Thm.</p>
<p>åˆé€‚ç¼©æ”¾ä½¿å¾— $||x_i|| \leq 1$ ã€‚å‡è®¾å­˜åœ¨ $w_*$ æ»¡è¶³ $||w_*|| = 1$ ä¸” $y_i w_*^T x_i &gt; \gamma$ï¼ˆå­˜åœ¨è¿‡åŸç‚¹çš„åˆ’åˆ†å¹³é¢,å®‰å…¨è·ç¦»ä¸º $\gamma$ï¼‰ã€‚è¯¥ç®—æ³•æ”¶æ•›å‰æœ€å¤šè§¦å‘ $\frac{1}{\gamma^2}$ æ¬¡é¢„æµ‹é”™è¯¯ã€‚</p>
</blockquote>
<p><strong>Pf.</strong>
å‡è®¾ç®—æ³•ç¬¬ $t$ æ¬¡çŠ¯é”™æ˜¯ $(x_t, y_t)$,è¿™ä¼šä½¿å¾—</p>
<p>$$w_{t+1} = w_t + y_t x_t$$</p>
<p>ä¸”æ­¤æ—¶ $\langle {w}^T, y_t x_t \rangle &lt; 0$ï¼ˆé”è§’ï¼‰ã€‚è¿™è¯´æ˜
$$
||w_{t+1}||^2 \leq ||w_t||^2 + ||y_t x_t||^2 = ||w_t||^2 + 1 \
||w_t||^2 \leq t
$$</p>
<p>å¦ä¸€æ–¹é¢
$$
||w_{t+1}|| \geq \langle w_{t+1}, w_* \rangle \geq \langle w_t, w_* \rangle + \gamma \
||w_t|| \geq \gamma t
$$</p>
<p>ç»¼ä¸Š
$$
\gamma^2 t^2 \leq |w_t|^2 \leq t
$$
è§£å¾— $t \leq \frac{1}{\gamma^2}$ã€‚$\blacksquare$</p>
<h2 id="32-logistic-regression">3.2 Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#32-logistic-regression">#</a></h2>
<p>ä¸ºäº†è§£å†³ä¸å¯å¯¼çš„é—®é¢˜,æ›´ä¸ºç°ä»£çš„æƒ³æ³•æ˜¯é€šè¿‡sigmoidå‡½æ•°æŠŠ$w^Tx$å‹ç¼©åˆ°$(0,1)$ä¹‹é—´çš„æ¦‚ç‡,å³$$f(x)=\frac{1}{1+e^{-w^Tx}}.$$
è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡ä¹‹é—´çš„å·®å¼‚,å¯ä»¥ç”¨l1-normæˆ–è€…cross-entropy lossã€‚</p>
<p><img loading="lazy" src="../img/ml1/image9.png"></p>
<blockquote>
<p><strong>ç†µ (Entropy)</strong><!-- raw HTML omitted -->
å¯¹äºç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \cdots, p_n)$,å®šä¹‰å®ƒçš„ç†µä¸º$$
H(p) = \sum_{i=1}^{n} p_i \log \frac{1}{p_i}$$</p>
</blockquote>
<blockquote>
<p><strong>äº¤å‰ç†µ (Cross entropy)</strong><!-- raw HTML omitted -->
å®šä¹‰ä¸¤ä¸ªç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \cdots, p_n)$ å’Œ $(q_1, q_2, \cdots, q_n)$ çš„äº¤å‰ç†µä¸º$$
XE(p, q) = \sum_{i=1}^{n} p_i \log \frac{1}{q_i}$$</p>
</blockquote>
<blockquote>
<p><strong>KL æ•£åº¦</strong><!-- raw HTML omitted -->
å®šä¹‰ä¸¤ä¸ªç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \cdots, p_n)$ å’Œ $(q_1, q_2, \cdots, q_n)$ çš„ KL æ•£åº¦ä¸º$$
KL(p, q) = XE(p, q) - H(p)$$</p>
</blockquote>
<p>äº¤å‰ç†µæ¯”l1-norm å¥½åœ¨ï¼š</p>
<ul>
<li>l1-normï¼šæä¾›æ’å®šçš„æ¢¯åº¦ã€‚</li>
<li>äº¤å‰ç†µï¼šå·®è·è¶Šå¤§,æ¢¯åº¦è¶Šå¤§</li>
</ul>
<h2 id="33-regularization">3.3 Regularization<a hidden class="anchor" aria-hidden="true" href="#33-regularization">#</a></h2>
<p>å½“æˆ‘ä»¬æƒ³è¦é™åˆ¶$f$çš„è¡¨è¾¾èƒ½åŠ›æ—¶,ç»å…¸çš„çœ‹æ³•å°±æ˜¯é€šè¿‡åœ¨$||Â·||_2$æˆ–$||Â·||_1$æ„ä¹‰ä¸‹é™åˆ¶$w$çš„å¯èƒ½å–å€¼åŒºé—´ã€‚</p>
<h3 id="ridge-regression">Ridge Regression<a hidden class="anchor" aria-hidden="true" href="#ridge-regression">#</a></h3>
<p>æŠŠloss functionæ”¹ä¸º$$l(w)+\lambda||w||^2$$
è¿™é‡Œæ˜¯2-norm, è¿™ç›¸å½“äºæ¯ä¸€æ­¥å…ˆGD,ä¹‹åå†è¿›è¡Œäº†ä¸€æ¬¡
$$w_{t+1}=(1-\eta \lambda)\tilde{w}_{t}$$
è¿™è¢«ç§°ä¸ºweight decayã€‚</p>
<h3 id="lasso-regression">Lasso Regression<a hidden class="anchor" aria-hidden="true" href="#lasso-regression">#</a></h3>
<p>æœ‰æ—¶å€™æˆ‘ä»¬æƒ³è¦è·å¾—sparseçš„è§£,å› æ­¤æˆ‘ä»¬æŠŠloss functionæ”¹ä¸º$$l(w)+\lambda||w||_1^2$$
è¿™ä¸ªç›´è§‰åœ¨äºç”¨diamondå’Œå‡¸é›†çš„äº¤é›†æ›´æœ‰å¯èƒ½æ˜¯sparseçš„
<img loading="lazy" src="../img/ml1/image8.png"></p>
<h2 id="34-compressed-sensing">3.4 Compressed Sensing<a hidden class="anchor" aria-hidden="true" href="#34-compressed-sensing">#</a></h2>
<blockquote>
<p><strong>Nyquist theorem</strong>: <!-- raw HTML omitted -->for a signal with frequency ğ‘“, we need 2ğ‘“ sampling rate to fully reconstruct the signal</p>
</blockquote>
<p>è¿™ä¸ªæ˜¯ä¸€ä¸ªé€šç”¨çš„å®šç†,ä½†æ˜¯å¤§éƒ¨åˆ†æƒ…å†µä¸‹,æˆ‘ä»¬çš„ä¿¡å·å…¶å®æ˜¯å­˜åœ¨ä¸€ç»„åŸºä¸‹çš„ç¨€ç–è¡¨ç¤º,æ‰€ä»¥æˆ‘ä»¬ä¼šå»æƒ³èƒ½ä¸èƒ½é€šè¿‡æ›´å°‘çš„é‡‡æ ·,æ¥é‡æ„å‡ºä¿¡å·,è¿™å°±æ˜¯compressed sensingçš„èƒŒæ™¯ã€‚</p>
<p><img loading="lazy" src="../img/ml1/image7.png"></p>
<p>åœ¨Compressed Sensingä¸­,å’Œsupervised learningä¸åŒçš„æ˜¯æˆ‘ä»¬å¯ä»¥è‡ªå·±é€‰æ‹©è‡ªå·±çš„measurement matrix,å³è®­ç»ƒé›†,åœ¨ä¸‹å›¾ä¸­ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰å®š$A$çš„æ¯ä¸€è¡Œ,ç„¶åè·å¾—å¯¹åº”çš„$y$,æœ€ç»ˆæˆ‘ä»¬å¸Œæœ›é€šè¿‡$y$è¿˜åŸå‡º$x$ã€‚</p>
<p><img loading="lazy" src="../img/ml1/image6.png">
æœ€åçš„å¾—åˆ°çš„ä¸»è¦ç»“è®º,ç”¨è‡ªç„¶è¯­è¨€å»æè¿°,æ˜¯å¦‚ä¸‹ä¸‰æ¡:</p>
<ol>
<li>
<p>å¦‚æœä¸€ä¸ªç¨€ç–ä¿¡å·é€šè¿‡ $x \mapsto Wx$ è¿›è¡Œäº†å‹ç¼©,å…¶ä¸­ $W$ æ˜¯æ»¡è¶³$(\epsilon, s)$-RIP çš„çŸ©é˜µ,é‚£ä¹ˆå¯ä»¥å®Œå…¨é‡æ„ä»»ä½•ç¨€ç–ä¿¡å·ã€‚æ»¡è¶³æ­¤æ€§è´¨çš„çŸ©é˜µä¿è¯äº†ä»»ä½•ç¨€ç–å¯è¡¨ç¤ºå‘é‡çš„èŒƒæ•°distortionè¾ƒå°ã€‚</p>
</li>
<li>
<p>é€šè¿‡æ±‚è§£çº¿æ€§è§„åˆ’,é‡æ„å¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å†…è®¡ç®—ã€‚</p>
</li>
<li>
<p>ç»™å®š $n \times d$ çš„éšæœºçŸ©é˜µ,åœ¨ $n$ å¤§äº $s \log(d)$ çš„æ•°é‡çº§æ—¶,å®ƒå¾ˆå¯èƒ½æ»¡è¶³ RIP æ¡ä»¶ã€‚</p>
</li>
</ol>
<p>æ¥ä¸‹æ¥è®©æˆ‘formallyç”¨æ•°å­¦çš„è¯­è¨€build upéƒ½ä»¥ä¸Šçš„ç»“è®ºã€‚</p>
<h3 id="rip-condition">RIP-Condition<a hidden class="anchor" aria-hidden="true" href="#rip-condition">#</a></h3>
<p>ä¸€ä¸ªçŸ©é˜µ $W \in \mathbb{R}^{n,d}$ æ˜¯ $(\epsilon, s)$-RIP çš„å½“ä¸”ä»…å½“å¯¹äºæ‰€æœ‰ $x \neq 0$ ä¸”æ»¡è¶³ $||x||_{0}\leq s$ çš„ $x$,æˆ‘ä»¬æœ‰
$$
\left| \frac{||Wx||_2^2}{||x||_2^2} - 1 \right| \leq \epsilon.
$$</p>
<h3 id="thm1">Thm.1<a hidden class="anchor" aria-hidden="true" href="#thm1">#</a></h3>
<blockquote>
<p><strong>Thm.1</strong> è®¾ $\epsilon &lt; 1$,å¹¶ä¸”è®¾ $W$ ä¸º $(\epsilon, 2s)$-RIP çŸ©é˜µã€‚è®¾ $x$ ä¸ºä¸€ä¸ªæ»¡è¶³ $||x||_0\leq s$ çš„å‘é‡,</p>
<p>ä»¤ $y = Wx$ ä¸º $x$ çš„å‹ç¼©ç»“æœ,å¹¶ä¸”ä»¤
$$\tilde{x} \in \arg \min_{{v}: W{v}=y} ||{v}||_0$$ ä¸ºé‡æ„å‘é‡ã€‚é‚£ä¹ˆ,$\tilde{x} = x$ã€‚</p>
</blockquote>
<p>è¿™ä¸ªå®šç†å‘Šè¯‰æˆ‘ä»¬å¯¹äºRIPçš„çŸ©é˜µ,å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡æ‰¾åˆ°ç¬¦åˆ$Wv=y$çš„$v$çš„l0-normæœ€å°çš„å‘é‡,æˆ‘ä»¬å°±èƒ½å¤ŸæˆåŠŸçš„(æ— æŸ)é‡å»ºå‡º$x$ã€‚</p>
<p><strong>Pf.</strong>
ä»¤ $h = \tilde{x} - x$</p>
<p>$$
|\tilde{x}|_0 \leq |x|_0 \leq s
$$</p>
<p>å› æ­¤ $h$ æ˜¯ $2s$-sparseçš„ã€‚</p>
<p>$$
(1 - \epsilon) |h|^2 \leq |Wh|^2 \leq (1 + \epsilon) |h|^2
$$</p>
<p>ç”±äº $Wh = W(\tilde{x} - x) = 0$</p>
<p>$$
\Rightarrow |h|^2 = 0
$$</p>
<p>å› æ­¤ $\tilde{x} = x$.$\blacksquare$</p>
<p>ä½†é—®é¢˜æ˜¯,æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªpolytimeæ±‚è§£l0-normæœ€å°å€¼çš„ç®—æ³•,æ‰€ä»¥è¿™ä¸ªå®šç†åœ¨å®é™…åº”ç”¨ä¸­æ²¡æœ‰æ„ä¹‰,æˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­å°è¯•å§l0-norm relaxåˆ° l1-norm,ä¸‹é¢çš„thm2å’Œ3ä¾¿æ˜¯l1-normä¸‹é‡å»ºç»“æœç›¸ä¼¼æ€§çš„ä¿è¯ã€‚</p>
<h3 id="thm2">Thm.2<a hidden class="anchor" aria-hidden="true" href="#thm2">#</a></h3>
<blockquote>
<p><strong>Thm.2</strong> å‡è®¾ $W$ ä¸º $(\epsilon, 2s)$-RIP çŸ©é˜µã€‚$x$ ä¸ºä¸€ä¸ªæ»¡è¶³ $|x|_0 \leq s$ çš„å‘é‡,</p>
<p>ä»¤ $y = Wx$ ä¸º $x$ çš„å‹ç¼©ç»“æœ,å¹¶ä¸” $\epsilon &lt; \frac{1}{1 + \sqrt{2}}$,é‚£ä¹ˆ,</p>
<p>$$x=\arg \min_{v: Wv = y} ||v||_ {0}=\arg \min_{v:Wv = y}||v||_1$$</p>
</blockquote>
<p>è¿™ä¸ªå®šç†è¯´æ˜åœ¨s-sparseçš„æƒ…å†µä¸‹,Relax åˆ°l1-normä¹Ÿå¯ä»¥é‡æ„å‡ºä¸€æ ·çš„å‘é‡ã€‚</p>
<p>äº‹å®ä¸Š,æˆ‘ä»¬å°†è¯æ˜ä¸€ä¸ªæ›´å¼ºçš„ç»“æœ,è¯¥ç»“æœå³ä½¿åœ¨ $x$ ä¸æ˜¯ä¸€ä¸ªç¨€ç–å‘é‡çš„æƒ…å†µä¸‹ä¹Ÿæˆç«‹,å³Thm.3ã€‚</p>
<h3 id="thm3">Thm.3<a hidden class="anchor" aria-hidden="true" href="#thm3">#</a></h3>
<blockquote>
<p><strong>Thm.3</strong> è®¾ $\epsilon &lt; \frac{1}{1 + \sqrt{2}}$ å¹¶ä¸” $W$ æ˜¯ä¸€ä¸ª $(\epsilon, 2s)$-RIP çŸ©é˜µã€‚è®¾ $x$ æ˜¯ä»»æ„å‘é‡,å¹¶å®šä¹‰
$$x_s \in \arg \min_{v: ||v|| _ 0 \leq s} ||x - v||_ 1 $$
ä¹Ÿå°±æ˜¯è¯´,$x_s$ æ˜¯ä¸€ä¸ªåœ¨ $x$ çš„ $s$ ä¸ªæœ€å¤§å…ƒç´ å¤„ç­‰äº $x$ å¹¶åœ¨å…¶ä»–åœ°æ–¹ç­‰äº $0$ çš„å‘é‡ã€‚è®¾ $y = Wx$ ,å¹¶ä»¤
$$x^* \in \arg \min_{v: Wv = y} |v|_1$$
ä¸ºé‡æ„çš„å‘é‡ã€‚é‚£ä¹ˆ,
$$|x^* - x|_2 \leq 2 \frac{1 + \rho}{1 - \rho} s^{-1/2} |x - x_s|_1,$$
å…¶ä¸­ $\rho = \sqrt{2\epsilon}/(1 - \epsilon)$ã€‚</p>
</blockquote>
<p><strong>Pf.</strong></p>
<p>è¿™ä¸ªå®šç†çš„è¯æ˜ç›¸å¯¹æ¯”è¾ƒå¤æ‚,ä¸»è¦æ˜¯è¯æ˜ä»¥ä¸‹ä¸¤ä¸ªClaim:</p>
<blockquote>
<p><strong>Claim 1ï¼š</strong>
$$
|h_{T_{0,1}}|_ 2 \leq |h _{T_0}|_2 + 2s^{-1/2}|x - x_s|_1ã€‚
$$</p>
</blockquote>
<blockquote>
<p><strong>Claim 2ï¼š</strong>
$$
|h_{T_{0,1}}|_ 2 \leq \frac{2\rho}{1 - \rho}s^{-1/2}|x - x_s|_1ã€‚
$$
<strong>ç¬¦å·è¯´æ˜ï¼š</strong> ç»™å®šä¸€ä¸ªå‘é‡ $v$ å’Œä¸€ç»„ç´¢å¼• $I$,æˆ‘ä»¬ç”¨ $v_I$ è¡¨ç¤ºå‘é‡,å…¶ç¬¬ $i$ ä¸ªå…ƒç´ ä¸º $v_i$ å¦‚æœ $i \in I$,å¦åˆ™å…¶ç¬¬ $i$ ä¸ªå…ƒç´ ä¸º 0ã€‚ä»¤ $h = x^* - x$ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªæŠ€å·§æ˜¯å°†ç´¢å¼•é›†åˆ $[d] = {1, \ldots, d}$ åˆ’åˆ†ä¸ºå¤§å°ä¸º $s$ çš„ä¸ç›¸äº¤é›†åˆã€‚ä¹Ÿå°±æ˜¯è¯´,æˆ‘ä»¬å†™ä½œ $[d] = T_0 \cup T_1 \cup T_2 \ldots T_{d/s-1}$,å¯¹äºæ‰€æœ‰ $i$,æˆ‘ä»¬æœ‰ $|T_i| = s$,å¹¶ä¸”æˆ‘ä»¬ä¸ºç®€ä¾¿èµ·è§å‡è®¾ $d/s$ æ˜¯ä¸€ä¸ªæ•´æ•°ã€‚æˆ‘ä»¬å¦‚ä¸‹å®šä¹‰åˆ’åˆ†ã€‚åœ¨ $T_0$ ä¸­,æˆ‘ä»¬æ”¾ç½® $s$ ä¸ªå¯¹åº”äº $x$ çš„ç»å¯¹å€¼ä¸­æœ€å¤§çš„å…ƒç´ çš„ç´¢å¼•ï¼ˆå¦‚æœæœ‰å¹¶åˆ—çš„æƒ…å†µ,åˆ™ä»»æ„æ‰“ç ´å¹³å±€ï¼‰ã€‚è®¾ $T_0^c = [d] \setminus T_0$ã€‚æ¥ä¸‹æ¥,$T_1$ å°†æ˜¯å¯¹åº”äº $h_{T_0^c}$ ç»å¯¹å€¼ä¸­æœ€å¤§çš„ $s$ ä¸ªå…ƒç´ çš„ç´¢å¼•ã€‚è®¾ $T_{0,1} = T_0 \cup T_1$,å¹¶ä»¤ $T_{0,1}^c = [d] \setminus T_{0,1}$ã€‚æ¥ä¸‹æ¥,$T_2$ å°†æ˜¯å¯¹åº”äº $h_{T_{0,1}^c}$ ç»å¯¹å€¼ä¸­æœ€å¤§çš„ $s$ ä¸ªå…ƒç´ çš„ç´¢å¼•ã€‚æˆ‘ä»¬å°†ç»§ç»­æ„é€  $T_3, T_4, \ldots$ ä»¥ç›¸åŒçš„æ–¹å¼ã€‚</p>
<p><strong>Pf of Claim 1</strong>,æˆ‘ä»¬ä¸ä½¿ç”¨RIPæ¡ä»¶,ä»…ä»…ä½¿ç”¨$x^*$æœ€å°åŒ–$\ell_1$èŒƒæ•°è¿™ä¸€äº‹å®ã€‚è®¾$j &gt; 1$ã€‚å¯¹äºæ¯ä¸ª$i \in T_j$å’Œ$i&rsquo; \in T_{j-1}$,æˆ‘ä»¬æœ‰$|h_i| \leq |h_{i&rsquo;}|$ã€‚å› æ­¤,$|h_ {T_j}|_ \infty \leq |h_ {T_ {j-1}}|_ 1/s$ã€‚ç”±æ­¤å¯ä»¥å¾—åˆ°ï¼š</p>
<p>$$
||h_{T_j}||_ 2 \leq s^{-1/2} ||h_{T_{j-1}}||_1
$$</p>
<p>å¯¹$j = 2, 3, \ldots$æ±‚å’Œ,å¹¶ä½¿ç”¨ä¸‰è§’ä¸ç­‰å¼,å¯ä»¥å¾—åˆ°ï¼š</p>
<p>$$
||h_{T_{0,1}^c}||_ 2 \leq \sum_{j \geq 2} ||h_{T_j}||_ 2 \leq s^{-1/2} ||h_{T_{0,1}^c}||_1
$$</p>
<p>æ¥ä¸‹æ¥,æˆ‘ä»¬è¯æ˜$|h_{T_0}|_1$ä¸èƒ½å¤ªå¤§ã€‚å®é™…ä¸Š,ç”±äº$x^* = x + h$å…·æœ‰æœ€å°çš„$\ell_1$èŒƒæ•°,å¹¶ä¸”$x$æ»¡è¶³$x^*$çš„å®šä¹‰ä¸­çš„çº¦æŸæ¡ä»¶,æˆ‘ä»¬æœ‰$|x|_1 \geq |x + h|_1$ã€‚å› æ­¤,åˆ©ç”¨ä¸‰è§’ä¸ç­‰å¼æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p>
<p>$$
||x||_ 1 \geq \sum_{i \in T_0} |x_i + h_i| + \sum_{i \in T_{0,1}^c} |x_i + h_i| \geq ||x_{T_0}||_ 1 - ||h_{T_0}||_ 1 + ||x_{T_{0,1}^c}||_ 1 - ||h_{T_{0,1}^c}||_1
$$</p>
<p>ç”±äº$|x_ {T_ {0,1}^c}|_ 1 = |x - x_s|_ 1 = |x|_ 1 - |x_ {T_ 0}|_ 1$,æˆ‘ä»¬å¾—åˆ°ï¼š</p>
<p>$$
|h_{T_0}|_ 1 \leq |h_{T_0}|_ 1 + 2|x_{T_{0,1}^c}|_1ã€‚
$$</p>
<p>ç»“åˆä¸Šè¿°ç­‰å¼å¯ä»¥å¾—åˆ°ï¼š</p>
<p>$$
|h_{T_{0,1}^c}|_ 2 \leq s^{-1/2} (|h_{T_0}|_ 1 + 2|x_{T_{0,1}^c}|_1)ã€‚\blacksquare
$$</p>
<p><strong>Pf of Claim 2</strong></p>
<p>å¯¹äº2s-ç¨€ç–çš„å‘é‡$h_{T_{0,1}}$,æˆ‘ä»¬æœ‰ï¼š</p>
<p>$$(1 - \epsilon) ||h_{T_{0,1}}||_ 2^2 \leq ||Wh_{T_{0,1}}||_2^2$$</p>
<p>è€Œ</p>
<p>$$Wh_{T_{0,1}} = Wh - \sum_{j \geq 2} Wh_{T_j} = -\sum_{j \geq 2} Wh_{T_j}$$</p>
<p>å› æ­¤</p>
<p>$$||Wh_{T_{0,1}}||_ 2^2 = -\sum_{j \geq 2} \langle Wh_{T_{0,1}}, Wh_{T_j} \rangle$$</p>
<blockquote>
<p><strong>Lemma</strong>ï¼šå¦‚æœ$W$æ˜¯$(\epsilon, 2s)$-RIPçŸ©é˜µ,å¯¹äºä»»æ„ä¸ç›¸äº¤çš„$I, J$é›†åˆ,è‹¥$|I| \leq s, |J| \leq s$,åˆ™
$$
\langle W u_{I}, W u_{J} \rangle \leq \epsilon |u_{I}| |u_{J}|
$$</p>
</blockquote>
<p><strong>Pf.</strong></p>
<p>$$\langle W u_{I}, W u_{J} \rangle = \frac{|W(u_I + u_J)|^2 - |W(u_I - u_J)|^2}{4}$$</p>
<p>$$
\leq \frac{(1 + \epsilon) |u_I + u_J|^2 - (1 - \epsilon) |u_I - u_J|^2}{4}
$$</p>
<p>ç”±äº$I, J$æ˜¯ä¸ç›¸äº¤çš„é›†åˆï¼š</p>
<p>$$
= \frac{(1 + \epsilon) (|u_I|^2 + |u_J|^2) - (1 - \epsilon) (|u_I|^2 + |u_J|^2)}{4}
$$</p>
<p>$$
= \frac{\epsilon}{2} ((|u_I|^2 + |u_J|^2) \leq \epsilon |u_I||u_J|.\blacksquare
$$</p>
<p>åŸå¼ä»£å…¥Lemma,æˆ‘ä»¬æœ‰ï¼š</p>
<p>$$||Wh_{T_{0,1}}||_ 2^2 \leq \epsilon (||h_{T_0}||_ 2 + ||h_{T_{1}}||_ 2) \cdot \sum_{j \geq 2} ||h_{T_j}||_ 2
$$
åˆ©ç”¨$2(a^2 + b^2) \geq (a + b)^2$:
$$||h_{T_0}||_ 2 + ||h_{T_1}||_ 2 \leq \sqrt{2} ||h_ {T_{0,1}}|| _2$$</p>
<p>æ‰€ä»¥</p>
<p>$$
|Wh_{T_{0,1}}|_ 2^2 \leq \sqrt{2} \epsilon |h_{T_{0,1}}| _ 2 \cdot \sum_{j \geq 2} |h_{T_j}|_ 2
$$</p>
<p>$$
\leq \sqrt{2} \epsilon \cdot s^{-1/2} |h_{T_ {0,1}}| _ 2 \cdot |h _{T _{0,1}^C}| _1
$$</p>
<p>å› æ­¤</p>
<p>$$
|h_{T_0,1}|_ 2 \leq \frac{\sqrt{2} \epsilon}{1 - \epsilon} s^{-1/2} |h_{T_0^C}|_1
$$</p>
<p>$$
|h_{T_0,1}|_ 2 \leq \frac{\sqrt{2} \epsilon}{1 - \epsilon} s^{-1/2} (|h_{T_0}|_ 1 + 2|x_{T_0^C}|_1)
$$</p>
<p>$$
\leq \rho ||h_ {T_{0}}|| _{2} + 2 \rho s^{-1/2} ||x _{T _{0}^{C}}|| _{1}
$$</p>
<p>ç”±äº</p>
<p>$$||h_{T_ {1}}|| _ 2 \leq ||h_{T _{0,1}}|| _2$$</p>
<p>å› æ­¤</p>
<p>$$
||h_{T_{0,1}}||_2 \leq \frac{2 \rho}{1 - \rho} s^{-1/2} ||x - x_s||_1\blacksquare
$$</p>
<p>å›åˆ°<em>Thm.3</em>çš„è¯æ˜:</p>
<p>$$
|h|_ 2 \leq |h_{T _{0,1}}| _2 + |h _{T _{0,1}^C}| _2
$$</p>
<p>$$
\leq 2 |h_{T_0,1}|_2 + 2s^{-1/2} |x - x_s|_1
$$</p>
<p>$$
\leq \left( \frac{4 \rho}{1 - \rho} s^{-1/2} + 2s^{-1/2} \right) |x - x_s|_1
$$</p>
<p>$$
= 2 \frac{1 + \rho}{1 - \rho} s^{-1/2} |x - x_s|_1. \blacksquare
$$</p>
<h3 id="thm4">Thm.4<a hidden class="anchor" aria-hidden="true" href="#thm4">#</a></h3>
<p>æœ€åæˆ‘ä»¬å°±å‰©ä¸‹Thm.4äº†,</p>
<blockquote>
<p><strong>Thm.4</strong><br>
è®¾ $U$ ä¸ºä»»æ„å›ºå®šçš„ $d \times d$ æ­£äº¤çŸ©é˜µ,è®¾ $\epsilon, \delta$ ä¸ºåœ¨ $(0, 1)$ ä¹‹é—´çš„æ ‡é‡,è®¾ $s$ æ˜¯ $[d]$ ä¸­çš„ä¸€ä¸ªæ•´æ•°,ä¸”è®¾ $n$ ä¸ºæ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„æ•´æ•°
$$
n \geq 100 \frac{s \ln(40d/(\delta \epsilon))}{\epsilon^2}.$$
è®¾ $W \in \mathbb{R}^{n, d}$ ä¸ºä¸€ä¸ªçŸ©é˜µ,å…¶æ¯ä¸ªå…ƒç´ å‡ä»¥é›¶å‡å€¼å’Œæ–¹å·® $1/n$ æ­£æ€åˆ†å¸ƒã€‚åˆ™,å¯¹äºè‡³å°‘ $1 - \delta$ çš„æ¦‚ç‡è€Œè¨€,çŸ©é˜µ $WU$ æ˜¯ $(\epsilon, s)$-RIPã€‚</p>
</blockquote>
<p>è¿™é‡Œçš„å¸¸æ•°é¡¹å¯èƒ½æœ‰ä¸€äº›é—®é¢˜,è¯æ˜ä¹Ÿæ¯”è¾ƒå¤æ‚,è¿™é‡Œå°±ä¸å±•å¼€äº†ã€‚å¤§ä½“çš„Proof Sketchæ˜¯:</p>
<ul>
<li>
<p>å°†è¿ç»­ç©ºé—´æ˜ å°„åˆ°æœ‰é™ä¸ªç‚¹ä¸Š</p>
</li>
<li>
<p>è€ƒè™‘ä¸€ä¸ªç‰¹å®šçš„å¤§å°ä¸º $s$çš„ç´¢å¼•é›† $I$</p>
</li>
<li>
<p>ä½¿ç”¨è¿™ä¸ªç´¢å¼•é›†è¿›å…¥ç¨€ç–ç©ºé—´</p>
</li>
<li>
<p>å¯¹æ‰€æœ‰å¯èƒ½çš„ $I$ åº”ç”¨union bound
å…·ä½“å¯ä»¥å‚è€ƒ<em>Shai Shalev-Shwartz</em>çš„paper: <a href="https://www.cs.huji.ac.il/~shais/compressedSensing.pdf">Compressed Sensing:
Basic results and self contained proofs*</a>.</p>
</li>
</ul>
<h1 id="4-åè®°">4. åè®°<a hidden class="anchor" aria-hidden="true" href="#4-åè®°">#</a></h1>
<blockquote>
<p>â€œThe people who are crazy enough to think they can change the world, are the ones who do.â€</p>
</blockquote>
<p>æœŸä¸­ä¹‹å‰çš„å†…å®¹å¤§æ¦‚æ˜¯è¿™äº›ã€‚åœ¨å†™ä½œçš„è¿‡ç¨‹ä¸­,æˆ‘å‘ç°æˆ‘å¾€å¾€ä¼šå¿½ç•¥ä¸€äº›æˆ‘ä¸é‚£ä¹ˆæ„Ÿå…´è¶£çš„éƒ¨åˆ†è€Œåªæ˜¯å»å†™è‡ªè®¤ä¸ºæœ‰è¶£çš„éƒ¨åˆ†,è¿™ä¸€ç‚¹äº¦å¦‚æˆ‘çš„å¤ä¹ ,å…¶ä¸­æ¤å…¥äº†å¤ªå¤šçš„ä¸ªäººç†è§£è€Œå¿½è§†æ‰äº†è€å¸ˆæˆ–è€…å­¦ç•Œä¸»æµæƒ³è®©äººå…³æ³¨çš„æ¡†æ¶,å½¢æˆçš„Map of Machine Learning Worldè‡ªç„¶ä¹Ÿä¼šæ˜¯ä¸åŒçš„ã€‚è¿™å¤§æŠµä¹Ÿèƒ½è§£é‡Šè€ƒè¯•ä¸ºä»€ä¹ˆä¼šå¯„çš„ä¸€éƒ¨åˆ†åŸå› å§ã€‚ååŠå­¦æœŸäº‰å–è®©è‡ªå·±å­¦ä¼šçš„ä¸œè¥¿çš„åˆ†å¸ƒå’Œè¯¾ä¸Šçš„åˆ†å¸ƒæ¥è¿‘ä¸€äº›,æˆ–è€…æä¸€ä¸ªgenerative model,ä»è‡ªå·±çš„åˆ†å¸ƒé‡Œé‡‡æ ·,ç»è¿‡ä¸€äº›å˜æ¢èƒ½å¤Ÿæ¥è¿‘ä»–çš„åˆ†å¸ƒå§ã€‚
<img loading="lazy" src="../img/ml1/image5.png#center"></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/blog/tags/machine-learning/">Machine-Learning</a></li>
      <li><a href="http://localhost:1313/blog/tags/computer-science/">Computer-Science</a></li>
      <li><a href="http://localhost:1313/blog/tags/optimization/">Optimization</a></li>
      <li><a href="http://localhost:1313/blog/tags/math/">Math</a></li>
      <li><a href="http://localhost:1313/blog/tags/artificial-intelligence/">Artificial-Intelligence</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blog/posts/nlp2/">
    <span class="title">Â« Prev</span>
    <br>
    <span>Natural Language Processing: Part B. Modern Approaches</span>
  </a>
  <a class="next" href="http://localhost:1313/blog/posts/blogpost/">
    <span class="title">Next Â»</span>
    <br>
    <span>Complex Analysis and its application in Electrostatics</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 1.Optimization, Generalization and Supervised Learning on x"
            href="https://x.com/intent/tweet/?text=Machine%20Learning%20Series%3a%201.Optimization%2c%20Generalization%20and%20Supervised%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f&amp;hashtags=machine-learning%2ccomputer-science%2coptimization%2cmath%2cartificial-intelligence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 1.Optimization, Generalization and Supervised Learning on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f&amp;title=Machine%20Learning%20Series%3a%201.Optimization%2c%20Generalization%20and%20Supervised%20Learning&amp;summary=Machine%20Learning%20Series%3a%201.Optimization%2c%20Generalization%20and%20Supervised%20Learning&amp;source=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 1.Optimization, Generalization and Supervised Learning on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f&title=Machine%20Learning%20Series%3a%201.Optimization%2c%20Generalization%20and%20Supervised%20Learning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 1.Optimization, Generalization and Supervised Learning on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 1.Optimization, Generalization and Supervised Learning on whatsapp"
            href="https://api.whatsapp.com/send?text=Machine%20Learning%20Series%3a%201.Optimization%2c%20Generalization%20and%20Supervised%20Learning%20-%20http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 1.Optimization, Generalization and Supervised Learning on telegram"
            href="https://telegram.me/share/url?text=Machine%20Learning%20Series%3a%201.Optimization%2c%20Generalization%20and%20Supervised%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 1.Optimization, Generalization and Supervised Learning on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Machine%20Learning%20Series%3a%201.Optimization%2c%20Generalization%20and%20Supervised%20Learning&u=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml1%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/blog/">Nemo&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <div class="busuanzi-footer">
        <span id="busuanzi_container_site_pv">
            Total site visits: <span id="busuanzi_value_site_pv"></span> times
        </span>
        
    </div></footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
