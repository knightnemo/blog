<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Machine Learning Series: 3.Unsupervised Learning(II) | Nemo&#39;s Blog</title>
<meta name="keywords" content="machine-learning, computer-science, algebra, math, artificial-intelligence, algorithm">
<meta name="description" content="This is the third article in the Machine Learning Series. It covers the second part of unsupervised learning, including topics like Clustering, Spectral Graph Clustering, SimCLR, SNE and t-SNE.">
<meta name="author" content="Nemo">
<link rel="canonical" href="http://localhost:1313/blog/posts/ml3/">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<meta name="referrer" content="no-referrer-when-downgrade">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.861c79de57c9db7acb194fc40ad15b8fc78b954c4ce73dd6d09ff1b9ac5207f1.css" integrity="sha256-hhx53lfJ23rLGU/ECtFbj8eLlUxM5z3W0J/xuaxSB/E=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/blog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/posts/ml3/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>


<meta property="og:url" content="http://localhost:1313/blog/posts/ml3/">
  <meta property="og:site_name" content="Nemo&#39;s Blog">
  <meta property="og:title" content="Machine Learning Series: 3.Unsupervised Learning(II)">
  <meta property="og:description" content="This is the third article in the Machine Learning Series. It covers the second part of unsupervised learning, including topics like Clustering, Spectral Graph Clustering, SimCLR, SNE and t-SNE.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-03T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-03T00:00:00+00:00">
    <meta property="article:tag" content="Machine-Learning">
    <meta property="article:tag" content="Computer-Science">
    <meta property="article:tag" content="Algebra">
    <meta property="article:tag" content="Math">
    <meta property="article:tag" content="Artificial-Intelligence">
    <meta property="article:tag" content="Algorithm">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Series: 3.Unsupervised Learning(II)">
<meta name="twitter:description" content="This is the third article in the Machine Learning Series. It covers the second part of unsupervised learning, including topics like Clustering, Spectral Graph Clustering, SimCLR, SNE and t-SNE.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning Series: 3.Unsupervised Learning(II)",
      "item": "http://localhost:1313/blog/posts/ml3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Machine Learning Series: 3.Unsupervised Learning(II)",
  "name": "Machine Learning Series: 3.Unsupervised Learning(II)",
  "description": "This is the third article in the Machine Learning Series. It covers the second part of unsupervised learning, including topics like Clustering, Spectral Graph Clustering, SimCLR, SNE and t-SNE.",
  "keywords": [
    "machine-learning", "computer-science", "algebra", "math", "artificial-intelligence", "algorithm"
  ],
  "articleBody": "Clustering 这里是正统的无监督学习了，我们想要把数据点分组，希望在同一组的数据点有一些共同的性质。\nK-Means 形式化一下，我们被给予：\nn个数据点$(x_ 1,…,x_ n), x_ i \\in \\mathbb{R}^d$ 想要把他们partition成：\nk个cluster: ${S_ 1,…,S_ k}$ 我们的目标是最小化在同一个cluster的点距离cluster中心的距离的平方之和。 $$\\arg\\min_ S \\sum_ {i=1}^k \\sum_ {x\\in S_ i} ||x-\\mu_ i||^2$$ 其中$\\mu_ i$是$S_ i$的中心。\n这个问题是NP-Hard的，但是有一些Heuristic的算法，下面介绍Lloyd Algorithm:\nLloyd’s Method:\nDecide $k$ Randomly pick $k$ centers Decide membership of all points by assigning them to the nearest center Re-estimate k centers by average of cluster members Repeat 3\u00264 until convergence 很符合直觉，但是在最坏情况下，这个算法会找到arbitrarily-worse的solution,而且即使是对于seperated gaussian clusters都没有分开的保障。\nLloyd算法是保证终止的，因为每一次都会发生聚类的变化，然后聚类一共只有有限种，所以最终会终止。\nSpectral Graph Clustering 对于有的图，p-范数并不能够做到很好的聚类，比如下面这张:\n这个时候，我们就引入一种思想，对于两个点之间引入一种相似性的衡量，假如说两个点之间的相似性超过一个threshold,那么我们用边把他们连接起来，边权$$w_ {i,j}=\\text{Similarity}(i,j)$$\n理想情况下，我们希望同组之间的边权大，不同组之间的边权小，如果没有边，我们定义$w_ {i,j}=0$.\n举一些例子：\n$\\epsilon$-neighborhood graph (unweighted):\n定义边权重： $$ w_ {i,j} = 1 \\text{ iff } x_ i, x_ j \\text{ are } \\epsilon\\text{-close}. $$ 即如果 $x_ i$ 和 $x_ j$ 的距离小于 $\\epsilon$，则两点之间有一条边。 $k$-nearest neighbor graph:\n如果 $x_ i$ 是 $x_ j$ 的 $k-nn$，或者 $x_ j$ 是 $x_ i$ 的 $k-nn$，则两点之间有一条边。 注意：nearest neighbor 关系不对称。 Fully connected graph:\n定义一个相似性函数（similarity function）来衡量 $x_ i$ 和 $x_ j$ 之间的关系。 下面引入Laplacian Matrix:\nGraph Laplacian\n下面引入$L=D-A$，其中$D$是$diag{deg(v_ i)}$, $A$是邻接矩阵。\n这个Laplacian Matrix有很多好性质，比如：\nTheorem\nGiven: $G$ 是一个无向图，具有非负权重。\n零特征值的数量： $L$的零特征值的数量等于图 $G$ 中的连通支的数量。 记连通分量为 $A_ 1, A_ 2, \\dots, A_ k$。 零特征值的性质： 零特征值对应的特征向量由连通分量的指示向量 $I_ {A_ 1}, I_ {A_ 2}, \\dots, I_ {A_ k}$ 张成。 证明如下:\n$L$是半正定的 对于任意的$v\\in \\mathbb{R}^n$ $$v^T L v=v^T D v-v^T A v$$ $$=\\sum_ {i=1}^n d_ i v_ i^2-\\sum_ {i,j}v_ i v_ j w_ {i,j}$$ $$=\\frac{1}{2}(\\sum_ {i=1}^n d_ i v_ i^2-2\\sum_ {i,j}v_ i v_ j w_ {i,j}+\\sum_ {j=1}^n d_ j v_ j^2)$$ $$=\\frac{1}{2}\\sum_ {ij} w_ {i j} (v_ i-v_ j)^2\\geq 0$$\n记$L$的特征值: $0=\\lambda_ 1\\leq \\lambda_ 2 \\leq …\\leq \\lambda_ n$\n$\\lambda_ 1=0$ 这个原因是$$\\sum_ j w_ {i j}=d_ i$$ 所以我们可以让$v=(1,…,1)^T \\in \\mathbb{R}^n$,就有$v^T L v=0$\n连通图只有一个零特征值 利用反证法，考虑$\\sum_ {ij} w_ {i j} (v_ i-v_ j)^2$，除了$v$为全1之外，假设还有一个$v$对应的特征值也是0。这个特征向量一定存在两个分量:$v_ i\\neq v_ j$, 而且之间有路径连接: $w_ 1,…,w_ k\u003e0$, 这条路径就会对$\\sum_ {ij} w_ {i j} (v_ i-v_ j)^2$贡献非负值，而我们知道这个取0意味着所有的$w_ {i j} (v_ i-v_ j)^2$都取0，所以矛盾。\nk联通支对应k个0特征向量 我们可以对于顶点进行适当的排序，得到 $$ L=\\begin{pmatrix}L_ 1 \u0026 0 \u0026 0 \u0026 0 \\ 0 \u0026 L_ 2 \u0026 0 \u0026 0 \\ 0 \u0026 0 \u0026 \\ddots \u0026 0 \\ 0 \u0026 0 \u0026 0 \u0026 L_ n \\ \\end{pmatrix}$$ 于是我们有$I_ {A_ i}$这一系列的0特征值对应的特征向量，共k个。$\\blacksquare$\n接下来就该讲讲这个Laplacian Matrix怎么在Clustering里用的了:\n使用 Laplacian 找到 $k$ 个聚类的方法： 计算拉普拉斯矩阵 $L$ 的前 $k$ 个特征向量：\n特征向量：$\\mu_ 1, \\mu_ 2, \\dots, \\mu_ k$。 对应的特征值接近 0（不一定等于 0）。 构造矩阵 $U$:\n$U \\in \\mathbb{R}^{n \\times k}$，以 $\\mu_ 1, \\mu_ 2, \\dots, \\mu_ k$ 作为列。 构造特征向量的行向量：\n对于 $i = 1, 2, \\dots, n$，令： $$ y_ i \\in \\mathbb{R}^k \\text{ 是 } U \\text{ 的第 } i \\text{ 行向量}. $$ 构造点集 ${y_ i}_ {i=1, \\dots, n}$。 运行 $k$-means 聚类：\n在点集 ${y_ i}_ {i=1, \\dots, n}$ 上运行 $k$-means 算法，得到 $k$ 个聚类 $C_ 1, C_ 2, \\dots, C_ k$ 输出最终的聚类结果：\n定义每个聚类的集合： $$ A_ i = {j | y_ j \\in C_ i}, \\quad i = 1, 2, \\dots, k. $$ 输出聚类结果 $A_ 1, A_ 2, \\dots, A_ k$。 看一个理想情况的例子:\n这就很漂亮，但是现实情况下一般图是联通的，所以没那么好的事～\n最后解决一个technical problem,我们怎么去找最小的特征值、特征向量对呢？其实很简单，比如说$A$是半正定的，我们取 $$B = A - \\lambda _ {\\max} I $$ 对$B$做power-method，然后加$\\lambda_ {\\max}$就ok了。\nWhy This Makes Sense 考虑RatioCut问题: $$ratiocut(A_ 1,…,A_ k)=\\sum_ {i=1}^k \\frac{W(A_ i, \\overline{A_ i})}{|A_ i|}$$ 其中$W(A_ i, \\overline{A_ i})$代表这$A_ i, \\overline{A_ i}$之间所有边权重之和。\n这个问题是NP-Hard的。\n下面先考虑$k=2$的情况:\n假设 $G$ 是连通图,那么全 1 向量是最小的特征向量。\n对于满足下面这种形式的$v^A\\in \\mathbb{R}^n$： $$ v_ i^A = \\begin{cases} \\sqrt{\\frac{|\\overline{A}|}{|A|}}, \u0026 \\text{if } i \\in A \\ -\\sqrt{\\frac{|A|}{|\\overline{A}|}}, \u0026 \\text{o/w}. \\end{cases} $$\n我们有\n$$ (v^A)^T L v^A =\\frac{1}{2}\\sum_ {i,j} w_ {i,j} (v^A_ i-v^A_ j)^2 $$ $$=\\frac{1}{2}\\sum_ {i\\in A, j\\in \\overline{A}}w_ {i,j}\\left(\\sqrt{\\frac{|\\overline{A}|}{|A|}}-\\sqrt{\\frac{|A|}{|\\overline{A}|}}\\right)^2+\\frac{1}{2}\\sum_ {i\\in \\overline{A}, j\\in A}w_ {i,j}\\left(\\sqrt{\\frac{|\\overline{A}|}{|A|}}-\\sqrt{\\frac{|A|}{|\\overline{A}|}}\\right)^2$$ $$=cut(A,\\overline{A})(\\frac{|A|}{|\\overline{A}|}+\\frac{|\\overline{A}|}{|A|}+2)$$ $$=cut(A,\\overline{A})(\\frac{|A|+|\\overline{A}|}{|\\overline{A}|}+\\frac{|\\overline{A}|+|A|}{|A|})$$ $$=|V|\\cdot ratiocut(A,\\overline{A})$$\n也就是说对于这种形式的$v^A$，我们minimize RatioCut等价于找到一个好的$A \\subset V$: $$\\min_ A ratiocut(A,\\overline{A})=\\min_ {A \\subset V} (v^A)^T L v^A$$ 这个当然还是NP-Hard的，但是我们可以relax一下： $$\\min_ v v^T L v \\quad \\text{s.t. } \\langle v, I_ V \\rangle=0, ||v||=\\sqrt{n}$$ 这不就是求第二小的特征向量吗？\n然后通过这个形式有差异的$v$中还原$A$:\nnaive way: $i \\in A \\text{ iff } v_ i\u003e\\alpha$，其中$\\alpha$是某个threshold。 Run 2-means on ${v_ i}$: 诶，我们仔细一看，这种方式Run 2-mean是和在$(1,v_ i)$上跑2-means是完全一样的，再一想，这不就是我们 Spectral Graph Clustering的算法吗? 至此，豁然开朗。\n但是我们还是没完呢，这个只是$k=2$的情况，对于$k\u003e2$呢？\n定义 $h_ {ij}$： $$ h_ {ij} = \\begin{cases} \\frac{1}{\\sqrt{|A_ j|}}, \u0026 \\text{if } v_ i \\in A_ j \\ 0, \u0026 \\text{o/w}. \\end{cases} $$\n给定 $H \\in \\mathbb{R}^{n \\times k}$，使得 $H^T H = I$。\n我们可以看到(设 $h_ i$ 是 $(h_ {1i}, \\cdots, h_ {ni}) \\in \\mathbb{R}^n$ 的向量)：\n回忆：$$v^T L v = \\frac{1}{2} \\sum_ {ij} w_ {ij}(v_ i - v_ j)^2$$\n$$h_ i^T L h_ i = \\frac{\\text{cut}(A_ i, A_ i^c)}{|A_ i|} = (H^T L H)_ {ii}$$ 所以： $$ \\text{ratiocut}(A_ 1, \\cdots, A_ k) = \\sum_ {i=1}^k h_ i^T L h_ i = \\sum_ {i=1}^k (H^T L H)_ {ii} = \\text{Tr}(H^T L H) $$ 也就是说对于满足上述$h$的条件的矩阵$H$,我们的优化目标是： $$ \\text{min } \\text{Tr}(H^T L H) \\quad \\text{s.t. } H^T H = I, h_ {ij} \\text{ see above} $$\n与$k=2$情况类似，我们relax对于$H$的限制，得到: $$ \\text{min } \\text{Tr}(H^T L H) \\quad \\text{s.t. } H^T H = I $$\n这里有个结论，就是说这个$H$对应的就是$L$最小的$k$个特征向量(受限篇幅与作者的能力（小声），聪明的读者应该不难自证)，然后对$H$的行向量跑k-means就可以还原出聚类${A_ {[1:k]}}$了。所以我们找到Graph Spectral Clustering算法的理论依据了，也就是说我们在解决一个relaxed version的最小化聚类的RatioCut。\nSimCLR 这里讲个Clustering+ Metric Learning的应用，也是袁洋老师ICLR 2024年的崭新工作，证明了SimCLR这种对比学习方法其实就是在similarity graph上折腾了一些操作的spectral clustering。\nWhat is SimCLR 首先讲讲什么是SimCLR。这个是对比学习的一个算法，比如说给一个被查询的样本$q$，同时还有一个正样本$p_ 1$,对应$N-1$个负样本${p_ i}_ {i=2}^N$。\n一个现实的例子是我们对于所有输入的图片生成两个augmented图片，那么以其中一个图片作为查询样本，另一个就是正样本，别的图片augment之后的结果就是负样本。\n这里的$q,p_ 1$可以在pixel space上差距极大，但是我们希望他们在我们学到的semantic space上距离接近。我们的优化目标是InfoNCELoss:\n$$ L(p,q,{p_ i}_ {i=2}^N)=-\\log \\frac{\\exp(-||f(q)-f(p_ 1)||^2)/2\\tau}{\\sum_ {i=1}^N \\exp(-||f(q)-f(p_ i)||^2)/2\\tau} $$\nWhat is the Similarity Graph Here 这里的Similarity Graph的定义对于所有augmented image构成的${X_ i}$集合，$(X_ i,X_ j)$的边权是 $$\\pi_ {i,j}=\\Pr[X_ i,X_ j\\text{ are sampled together}]$$ 有没有一个理想的空间，其中semantic similarity是被自然的捕捉到的呢？答案是有的，这里引入Reproducing Kernel Hilbert Space.\nReproducing Kernel Hilbert Space (RKHS) 给定两个在$Z$空间的物品: $Z_ 1,Z_ 2\\in Z$, 考虑$\\phi: Z\\rightarrow H$, 这里$H$的维度远大于$Z$.\n$$k(Z_ i,Z_ j)=\\langle \\phi(Z_ i),\\phi(Z_ j)\\rangle_ H$$\n这里的$H$就是Hilbert space,而$k$Rreproducing Kernel。\n因为我们关心的只是样本之间的相似性，所以我们不用真的知道或者能算$\\phi(Z_ i)$,我们只需要算样本之间的$k$就ok了。\nMarkov Random Fields (MRF) 原论文里是这么说的\nDue to the large size of $\\pi$ and in practice $\\pi$ is usually formed by using positive samples sampling and hard to explicitly construct, directly comparing $K_ Z$ and $\\pi$ can be difficult, so we treat them as MRFs and compare the induced probability distributions on subgraphs instead.\nmotivation很清晰，那么我们具体怎么采样呢？我们想要sample出的unweighted子图我们设为$W$(也就是说$W_ {ij}\\in{0,1}$)\n$$P(W;\\pi)\\propto \\Omega(W)\\cdot \\prod_ {{i,j}\\in [n]^2}\\pi_ {i,j}^{W_ {i,j}} $$ 怎么解读呢？ $$s(W,\\pi)=\\prod_ {{i,j}\\in [n]^2}\\pi_ {i,j}^{W_ {i,j}}$$ 对于可能的$W$，我们根据他的score function来采样，然后我们看看采样出来的$W$是不是正确的形状，比如\n$$\\Omega(w)=\\prod_ i \\mathbb{1}[\\sum_ jW_ {ij}=1]$$ 这个被称为Unitary out-degree filter，也就是说每一个顶点要求有且仅有一条出边。\n接下来考虑$P(W;K_ z)$: $$P(W;K_ z)\\propto \\Omega(W)\\cdot \\prod_ {{i,j}\\in [n]^2}k(Z_ i,Z_ j)^{W_ {i,j}} $$\nInfoNCE and Spectral Clustering 好了，概念搭的差不多了，落地我们通过比较$W_ X,W_ Z$的差异来反应我们理想中$\\pi,K_ Z$的差值。这一差值可以通过Cross Entropy衡量： $$H_ \\pi^k(Z)=-\\mathbb{E}_ {W_ X\\sim P[\\cdot;\\pi]}\\log P[W_ Z=W_ X;K_ z]$$ 接下来如果能证明\nInfoNCE和$H_ \\pi^k(Z)$等价 $H_ \\pi^k(Z)$和Spectral Clustering等价 我们的任务就完成了。\nInfoNCE和$H_ \\pi^k(Z)$等价 先从直觉上理解下这件事情: $$H_ \\pi^k(Z)=-\\mathbb{E}_ {W_ X\\sim P[\\cdot;\\pi]}\\log P[W_ Z=W_ X;K_ z]$$\n$W_ X\\sim P[\\cdot;\\pi]$: 我们从$\\pi$上取样，代表着Data Augmentation Step 接下来用一个原论文中的重要结论: 对于Unitary out-deg $\\Omega(w)$, $$W_ i\\sim M(1, \\frac{\\pi_ i}{\\sum_ j \\pi_ {i,j}})$$ 也就是我们按照$\\frac{\\pi_ i}{\\sum_ j \\pi_ {i,j}}$取样一个one-hot vector。 因为每一行是独立的，所以我们有: $$H_ \\pi^k(Z)=-\\sum_ i\\mathbb{E}_ {W_ {X,i}}\\log P[W_ {Z,i}=W_ {X,i};K_ z]$$ 这里$W_ {X,i}$代表$W_ X$的第$i$行(one-hot 向量)。\n接下来想一下InfoNCE在说什么事情：\n$$InfoNCE=-\\sum_ {i=1}^n\\log \\frac{\\exp(-||f(X_ i)-f(X_ i’)||^2)/2\\tau}{\\sum_ {j=1}^N \\exp(-||f(X_ i)-f(X_ j))||^2)/2\\tau}$$\n如果我们定义 $$Q_ i=\\frac{K_ {Z,i}}{||K_ {Z,i}||_ 1} $$ 作为$P(\\cdot;K_ Z)$的分布\n那么 $$\\text{InfoNCE}=-\\sum_ {i=1}^n \\sum_ {i’=1}^n \\Pr[W_ {X,i,i’}=1] \\log Q_ {i,i’}=-\\sum_ i\\mathbb{E}_ {W_ {X,i}}\\log P[W_ {Z,i}=W_ {X,i};K_ z]=H_ \\pi^k(Z)$$\n$H_ \\pi^k(Z)$和Spectral Clustering等价 $$H_ \\pi^k(Z)=-\\mathbb{E}_ {W_ X\\sim P[\\cdot;\\pi]}\\log P[W_ Z=W_ X;K_ z]$$ 其中 $$P[W_ Z=W_ X;K_ z] \\propto \\Omega(W_ X) \\prod_ {{i,j}\\in [n]^2} K_ {Z_ {i,j}}^{W_ {X_ {i,j}}}$$\n然后我们就可以显式的表达概率 $$R(Z)=\\sum_ {W}\\Omega(W)\\cdot \\prod_ {{i,j}\\in [n]^2}K_ {Z_ {i,j}}^{W_ {i,j}}$$\n所以说 $$\\log P[W_ Z=W_ X;K_ z]=\\sum_ {i,j}W_ {X_ {i,j}}\\log K_ {Z_ {i,j}}+\\log \\Omega(W_ X)-\\log R(Z)$$\n这里对于fixed $W_ X$，$\\log \\Omega(W_ X)$是常数，所以: $$ \\arg\\min_ Z H_ \\pi^k(Z) = \\arg\\min_ Z -\\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\Bigg[\\sum_ {(i,j) \\in [n]^2} W_ {X,ij} \\log k(Z_ i, Z_ j) - \\log R(Z) \\Bigg] $$ $$ = \\arg\\min_ Z -\\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\sum_ {(i,j) \\in [n]^2} W_ {X,ij} \\log k(Z_ i, Z_ j) + \\log R(Z) $$ 回忆下，$k$ 是 Gaussian 分布来的：\n$$\\log k(Z_ i, Z_ j) = -\\frac{||Z_ i - Z_ j||^2}{2\\tau} $$\n所以原式 $$ = \\arg\\min_ Z \\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\frac{1}{2\\tau} \\sum_ {(i,j) \\in [n]^2} W_ {X,ij} |Z_ i - Z_ j|^2 + \\log R(Z) $$\n$$ = \\arg\\min_ Z \\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\frac{1}{\\tau} \\operatorname{tr}(Z^T L(W_ X) Z) + \\log R(Z) $$\n$$ = \\arg\\min_ Z \\frac{1}{\\tau} \\operatorname{tr}(Z^T L^* Z) + \\log R(Z)\\quad\\blacksquare $$ 真长啊。\nt-SNE 最后讲一个Data Visualization/ Dimension Reduction的算法，首先回忆一下NCA:\n原空间上两个点$(x_ i,x_ j)$的similarity是这么定义的: $$ p_ {j|i} = \\frac{\\exp\\left(-\\frac{|x_ i - x_ j|_ 2^2}{2\\sigma_ i^2}\\right)}{\\sum_ {k \\neq i} \\exp\\left(-\\frac{|x_ i - x_ k|_ 2^2}{2\\sigma_ i^2}\\right)} $$\n我们做映射后$ x_ i \\to y_ i = f(x_ i) $: $$ q_ {j|i} = \\frac{\\exp\\left(-|y_ i - y_ j|_ 2^2\\right)}{\\sum_ {k \\neq i} \\exp\\left(-|y_ i - y_ k|_ 2^2\\right)} $$\n我们希望$p,q$能对应的上。\n在训练中，用KL-Divergence做损失函数： $$ L = \\sum_ i \\text{KL}(P_ i | Q_ i) = \\sum_ i \\sum_ j p_ {j|i} \\log \\frac{p_ {j|i}}{q_ {j|i}} $$\n细心的读者发现和无监督学习(I)里讲的NCA的优化目标好像略有区别(差个log)，但无伤大雅，我们不去管他。怎么去选$\\sigma_ i$呢?\nSNE 用户选择一个Perplexity，这个直觉的理解是对于有效邻居数量的 smooth measure。 $$ \\text{Perp}(P_ i) = 2^{H(P_ i)} $$ $$ H(P_ i) = -\\sum_ j p_ {j|i} \\log p_ {j|i} $$ 接下来我们对$\\sigma_ i$做二分查找找到合适的$\\sigma_ i$值，这就叫SNE algorithm。\nt-SNE SNE很经典，但是有如下两个缺点的\n要优化很多个loss，你看loss其实是对i和j求了两遍和，相当于每个pair贡献了一个loss，或者说有$n$个分布要去学\n拥挤问题(crowding problem)\n比如说考虑一个${0,1}^d$的grid,对于$r=10$的情况有$2^{10}$个可以放的位置，但是投影到$r=2$上就只有$2^2$个了，全挤到一块分不开了。\n为了解决第一个问题，t-SNE的做法是，把所有的距离放一起做运算，捏成一个概率分布，优化一个 single概率分布的loss: $$p_ {i,j}=\\frac{p_ {j|i}+p_ {i|j}}{2n}$$ 这样设置是为了 $$\\sum_ {i j} p_ {i j}=1$$\n为什么要变成一个distribution呢，因为计算梯度更容易、更快。\n为了解决第二个问题，t-SNE的想法是换一个更heavy-tail的distribution，这样保持相对距离，绝对距离改变，就还能分得开。\n也就是:\n高维中近距离的点，在低维中距离要变得更小 高维中远距离的点，在低维中距离要变得更大 这里用student t-distribution就很合适,因为更heavy-tail:\n距离由$\\frac{1}{1+||y_ i-y_ j||^2}$刻画 $$p_ {i j}=\\frac{(1+||y_ i-y_ j||^2)^{-1}}{\\sum_ {k\\neq l}(1+||y_ l-y_ k||^2)^{-1}}$$\n没了，感觉这一部分大量参考了这篇文献。\nA side note: If you are interested in clustering, you can also check out this website, which contains a survey of clustering algorithms(the pdf file link is in the website). It’s a project done by me and my friend Yiming Liu.\n",
  "wordCount" : "1383",
  "inLanguage": "en",
  "datePublished": "2024-12-03T00:00:00Z",
  "dateModified": "2024-12-03T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Nemo"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blog/posts/ml3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nemo's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/blog/" accesskey="h" title="Nemo&#39;s Blog (Alt + H)">Nemo&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/blog/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://knightnemo.github.io" title="About Me">
                    <span>About Me</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/blog/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blog/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Machine Learning Series: 3.Unsupervised Learning(II)
    </h1>
    <div class="post-meta"><span title='2024-12-03 00:00:00 +0000 UTC'>December 3, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Nemo


      <div  class="meta-item">&nbsp·&nbsp
        <span id="busuanzi_container_page_pv"> Reads: <span id="busuanzi_value_page_pv"></span> times</span>
      </div>
    </div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#clustering" aria-label="Clustering">Clustering</a><ul>
                        
                <li>
                    <a href="#k-means" aria-label="K-Means">K-Means</a></li>
                <li>
                    <a href="#spectral-graph-clustering" aria-label="Spectral Graph Clustering">Spectral Graph Clustering</a><ul>
                        
                <li>
                    <a href="#%e4%bd%bf%e7%94%a8-laplacian-%e6%89%be%e5%88%b0-k-%e4%b8%aa%e8%81%9a%e7%b1%bb%e7%9a%84%e6%96%b9%e6%b3%95" aria-label="使用 Laplacian 找到 $k$ 个聚类的方法：">使用 Laplacian 找到 $k$ 个聚类的方法：</a></li>
                <li>
                    <a href="#why-this-makes-sense" aria-label="Why This Makes Sense">Why This Makes Sense</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#simclr" aria-label="SimCLR">SimCLR</a><ul>
                        
                <li>
                    <a href="#what-is-simclr" aria-label="What is SimCLR">What is SimCLR</a></li>
                <li>
                    <a href="#what-is-the-similarity-graph-here" aria-label="What is the Similarity Graph Here">What is the Similarity Graph Here</a></li>
                <li>
                    <a href="#reproducing-kernel-hilbert-space-rkhs" aria-label="Reproducing Kernel Hilbert Space (RKHS)">Reproducing Kernel Hilbert Space (RKHS)</a></li>
                <li>
                    <a href="#markov-random-fields-mrf" aria-label="Markov Random Fields (MRF)">Markov Random Fields (MRF)</a></li>
                <li>
                    <a href="#infonce-and-spectral-clustering" aria-label="InfoNCE and Spectral Clustering">InfoNCE and Spectral Clustering</a><ul>
                        
                <li>
                    <a href="#infonce%e5%92%8ch_-pikz%e7%ad%89%e4%bb%b7" aria-label="InfoNCE和$H_ \pi^k(Z)$等价">InfoNCE和$H_ \pi^k(Z)$等价</a></li>
                <li>
                    <a href="#h_-pikz%e5%92%8cspectral-clustering%e7%ad%89%e4%bb%b7" aria-label="$H_ \pi^k(Z)$和Spectral Clustering等价">$H_ \pi^k(Z)$和Spectral Clustering等价</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#t-sne" aria-label="t-SNE">t-SNE</a><ul>
                        
                <li>
                    <a href="#sne" aria-label="SNE">SNE</a></li>
                <li>
                    <a href="#t-sne-1" aria-label="t-SNE">t-SNE</a></li>
                <li>
                    <a href="#a-side-note" aria-label="A side note:">A side note:</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="clustering">Clustering<a hidden class="anchor" aria-hidden="true" href="#clustering">#</a></h2>
<p>这里是正统的无监督学习了，我们想要把数据点分组，希望在同一组的数据点有一些共同的性质。</p>
<h3 id="k-means">K-Means<a hidden class="anchor" aria-hidden="true" href="#k-means">#</a></h3>
<p>形式化一下，我们被给予：</p>
<ul>
<li>n个数据点$(x_ 1,&hellip;,x_ n), x_ i \in \mathbb{R}^d$</li>
</ul>
<p>想要把他们partition成：</p>
<ul>
<li>k个cluster: ${S_ 1,&hellip;,S_ k}$</li>
</ul>
<p>我们的目标是最小化在同一个cluster的点距离cluster中心的距离的平方之和。
$$\arg\min_ S \sum_ {i=1}^k \sum_ {x\in S_ i} ||x-\mu_ i||^2$$
其中$\mu_ i$是$S_ i$的中心。</p>
<p>这个问题是NP-Hard的，但是有一些Heuristic的算法，下面介绍Lloyd Algorithm:</p>
<p><img loading="lazy" src="../img/ml3/image.png#center"></p>
<p><img loading="lazy" src="../img/ml3/image2.png#center"></p>
<p><img loading="lazy" src="../img/ml3/image3.png#center"></p>
<p><img loading="lazy" src="../img/ml3/image4.png#center"></p>
<p><img loading="lazy" src="../img/ml3/image5.png#center"></p>
<blockquote>
<p><strong>Lloyd&rsquo;s Method:</strong></p>
<ol>
<li>Decide $k$</li>
<li>Randomly pick $k$ centers</li>
<li>Decide membership of all points by assigning them to the nearest center</li>
<li>Re-estimate k centers by average of cluster members</li>
<li>Repeat 3&amp;4 until convergence</li>
</ol>
</blockquote>
<p>很符合直觉，但是在最坏情况下，这个算法会找到arbitrarily-worse的solution,而且即使是对于seperated gaussian clusters都没有分开的保障。</p>
<p>Lloyd算法是保证终止的，因为每一次都会发生聚类的变化，然后聚类一共只有有限种，所以最终会终止。</p>
<h3 id="spectral-graph-clustering">Spectral Graph Clustering<a hidden class="anchor" aria-hidden="true" href="#spectral-graph-clustering">#</a></h3>
<p>对于有的图，p-范数并不能够做到很好的聚类，比如下面这张:</p>
<p><img loading="lazy" src="../img/ml3/image7.png#center"></p>
<p>这个时候，我们就引入一种思想，对于两个点之间引入一种<strong>相似性</strong>的衡量，假如说两个点之间的相似性超过一个threshold,那么我们用边把他们连接起来，边权$$w_ {i,j}=\text{Similarity}(i,j)$$</p>
<p>理想情况下，我们希望同组之间的边权大，不同组之间的边权小，如果没有边，我们定义$w_ {i,j}=0$.</p>
<p>举一些例子：</p>
<ol>
<li>
<p><strong>$\epsilon$-neighborhood graph (unweighted)</strong>:</p>
<ul>
<li>定义边权重：
$$
w_ {i,j} = 1 \text{ iff } x_ i, x_ j \text{ are } \epsilon\text{-close}.
$$</li>
<li>即如果 $x_ i$ 和 $x_ j$ 的距离小于 $\epsilon$，则两点之间有一条边。</li>
</ul>
</li>
<li>
<p><strong>$k$-nearest neighbor graph</strong>:</p>
<ul>
<li>如果 $x_ i$ 是 $x_ j$ 的 $k-nn$，或者 $x_ j$ 是 $x_ i$ 的 $k-nn$，则两点之间有一条边。</li>
<li>注意：nearest neighbor 关系不对称。</li>
</ul>
</li>
<li>
<p><strong>Fully connected graph</strong>:</p>
<ul>
<li>定义一个相似性函数（similarity function）来衡量 $x_ i$ 和 $x_ j$ 之间的关系。</li>
</ul>
</li>
</ol>
<p>下面引入Laplacian Matrix:</p>
<blockquote>
<p><strong>Graph Laplacian</strong><br>
下面引入$L=D-A$，其中$D$是$diag{deg(v_ i)}$, $A$是邻接矩阵。</p>
</blockquote>
<p>这个Laplacian Matrix有很多好性质，比如：</p>
<blockquote>
<p><strong>Theorem</strong><br>
Given: $G$ 是一个无向图，具有非负权重。</p>
<ol>
<li><strong>零特征值的数量</strong>：</li>
</ol>
<ul>
<li>$L$的零特征值的数量等于图 $G$ 中的连通支的数量。</li>
<li>记连通分量为 $A_ 1, A_ 2, \dots, A_ k$。</li>
</ul>
<ol start="2">
<li><strong>零特征值的性质</strong>：</li>
</ol>
<ul>
<li>零特征值对应的特征向量由连通分量的指示向量 $I_ {A_ 1}, I_ {A_ 2}, \dots, I_ {A_ k}$ 张成。</li>
</ul>
</blockquote>
<p>证明如下:</p>
<ol>
<li>$L$是半正定的</li>
</ol>
<p>对于任意的$v\in \mathbb{R}^n$
$$v^T L v=v^T D v-v^T A v$$
$$=\sum_ {i=1}^n d_ i v_ i^2-\sum_ {i,j}v_ i v_ j w_ {i,j}$$
$$=\frac{1}{2}(\sum_ {i=1}^n d_ i v_ i^2-2\sum_ {i,j}v_ i v_ j w_ {i,j}+\sum_ {j=1}^n d_ j v_ j^2)$$
$$=\frac{1}{2}\sum_ {ij} w_ {i j} (v_ i-v_ j)^2\geq 0$$</p>
<p>记$L$的特征值: $0=\lambda_ 1\leq \lambda_ 2 \leq &hellip;\leq \lambda_ n$</p>
<ol start="2">
<li>$\lambda_ 1=0$</li>
</ol>
<p>这个原因是$$\sum_ j w_ {i j}=d_ i$$
所以我们可以让$v=(1,&hellip;,1)^T \in \mathbb{R}^n$,就有$v^T L v=0$</p>
<ol start="3">
<li>连通图只有一个零特征值</li>
</ol>
<p>利用反证法，考虑$\sum_ {ij} w_ {i j} (v_ i-v_ j)^2$，除了$v$为全1之外，假设还有一个$v$对应的特征值也是0。这个特征向量一定存在两个分量:$v_ i\neq v_ j$, 而且之间有路径连接: $w_ 1,&hellip;,w_ k&gt;0$, 这条路径就会对$\sum_ {ij} w_ {i j} (v_ i-v_ j)^2$贡献非负值，而我们知道这个取0意味着所有的$w_ {i j} (v_ i-v_ j)^2$都取0，所以矛盾。</p>
<ol start="4">
<li>k联通支对应k个0特征向量</li>
</ol>
<p>我们可以对于顶点进行适当的排序，得到
$$
L=\begin{pmatrix}L_ 1 &amp; 0 &amp; 0 &amp; 0 \
0 &amp; L_ 2 &amp; 0 &amp; 0 \
0 &amp; 0 &amp; \ddots &amp; 0 \
0 &amp; 0 &amp; 0 &amp; L_ n \
\end{pmatrix}$$
于是我们有$I_ {A_ i}$这一系列的0特征值对应的特征向量，共k个。$\blacksquare$</p>
<p>接下来就该讲讲这个Laplacian Matrix怎么在Clustering里用的了:</p>
<h4 id="使用-laplacian-找到-k-个聚类的方法">使用 Laplacian 找到 $k$ 个聚类的方法：<a hidden class="anchor" aria-hidden="true" href="#使用-laplacian-找到-k-个聚类的方法">#</a></h4>
<ol>
<li>
<p><strong>计算拉普拉斯矩阵 $L$ 的前 $k$ 个特征向量</strong>：</p>
<ul>
<li>特征向量：$\mu_ 1, \mu_ 2, \dots, \mu_ k$。</li>
<li>对应的特征值接近 0（不一定等于 0）。</li>
</ul>
</li>
<li>
<p><strong>构造矩阵 $U$:</strong></p>
<ul>
<li>$U \in \mathbb{R}^{n \times k}$，以 $\mu_ 1, \mu_ 2, \dots, \mu_ k$ 作为列。</li>
</ul>
</li>
<li>
<p><strong>构造特征向量的行向量</strong>：</p>
<ul>
<li>对于 $i = 1, 2, \dots, n$，令：
$$
y_ i \in \mathbb{R}^k \text{ 是 } U \text{ 的第 } i \text{ 行向量}.
$$</li>
<li>构造点集 ${y_ i}_ {i=1, \dots, n}$。</li>
</ul>
</li>
<li>
<p><strong>运行 $k$-means 聚类</strong>：</p>
<ul>
<li>在点集 ${y_ i}_ {i=1, \dots, n}$ 上运行 $k$-means 算法，得到 $k$ 个聚类 $C_ 1, C_ 2, \dots, C_ k$</li>
</ul>
</li>
<li>
<p><strong>输出最终的聚类结果</strong>：</p>
<ul>
<li>定义每个聚类的集合：
$$
A_ i = {j | y_ j \in C_ i}, \quad i = 1, 2, \dots, k.
$$</li>
<li>输出聚类结果 $A_ 1, A_ 2, \dots, A_ k$。</li>
</ul>
</li>
</ol>
<p>看一个理想情况的例子:</p>
<p><img loading="lazy" src="../img/ml3/image6.png#center">
这就很漂亮，但是现实情况下一般图是联通的，所以没那么好的事～</p>
<p>最后解决一个technical problem,我们怎么去找最小的特征值、特征向量对呢？其实很简单，比如说$A$是半正定的，我们取
$$B = A - \lambda _ {\max} I $$
对$B$做power-method，然后加$\lambda_ {\max}$就ok了。</p>
<h4 id="why-this-makes-sense">Why This Makes Sense<a hidden class="anchor" aria-hidden="true" href="#why-this-makes-sense">#</a></h4>
<p>考虑RatioCut问题:
$$ratiocut(A_ 1,&hellip;,A_ k)=\sum_ {i=1}^k \frac{W(A_ i, \overline{A_ i})}{|A_ i|}$$
其中$W(A_ i, \overline{A_ i})$代表这$A_ i, \overline{A_ i}$之间所有边权重之和。</p>
<p>这个问题是NP-Hard的。</p>
<p><strong>下面先考虑$k=2$的情况:</strong></p>
<p>假设 $G$ 是连通图,那么全 1 向量是最小的特征向量。</p>
<p>对于满足下面这种形式的$v^A\in \mathbb{R}^n$：
$$
v_ i^A =
\begin{cases}
\sqrt{\frac{|\overline{A}|}{|A|}}, &amp; \text{if } i \in A \
-\sqrt{\frac{|A|}{|\overline{A}|}}, &amp; \text{o/w}.
\end{cases}
$$</p>
<p>我们有</p>
<p>$$
(v^A)^T L v^A =\frac{1}{2}\sum_ {i,j} w_ {i,j} (v^A_ i-v^A_ j)^2
$$
$$=\frac{1}{2}\sum_ {i\in A, j\in \overline{A}}w_ {i,j}\left(\sqrt{\frac{|\overline{A}|}{|A|}}-\sqrt{\frac{|A|}{|\overline{A}|}}\right)^2+\frac{1}{2}\sum_ {i\in \overline{A}, j\in A}w_ {i,j}\left(\sqrt{\frac{|\overline{A}|}{|A|}}-\sqrt{\frac{|A|}{|\overline{A}|}}\right)^2$$
$$=cut(A,\overline{A})(\frac{|A|}{|\overline{A}|}+\frac{|\overline{A}|}{|A|}+2)$$
$$=cut(A,\overline{A})(\frac{|A|+|\overline{A}|}{|\overline{A}|}+\frac{|\overline{A}|+|A|}{|A|})$$
$$=|V|\cdot ratiocut(A,\overline{A})$$</p>
<p>也就是说对于这种形式的$v^A$，我们minimize RatioCut等价于找到一个好的$A \subset V$:
$$\min_ A ratiocut(A,\overline{A})=\min_ {A \subset V} (v^A)^T L v^A$$
这个当然还是NP-Hard的，但是我们可以relax一下：
$$\min_ v v^T L v \quad \text{s.t. } \langle v, I_ V \rangle=0, ||v||=\sqrt{n}$$
这不就是求第二小的特征向量吗？</p>
<p>然后通过这个形式有差异的$v$中还原$A$:</p>
<ul>
<li>naive way: $i \in A \text{ iff } v_ i&gt;\alpha$，其中$\alpha$是某个threshold。</li>
<li>Run 2-means on ${v_ i}$:</li>
</ul>
<p>诶，我们仔细一看，<strong>这种方式Run 2-mean是和在$(1,v_ i)$上跑2-means是完全一样的，再一想，这不就是我们 Spectral Graph Clustering的算法吗?</strong> 至此，豁然开朗。</p>
<p>但是我们还是没完呢，这个只是$k=2$的情况，<strong>对于$k&gt;2$呢？</strong></p>
<p>定义 $h_ {ij}$：
$$
h_ {ij} =
\begin{cases}
\frac{1}{\sqrt{|A_ j|}}, &amp; \text{if } v_ i \in A_ j \
0, &amp; \text{o/w}.
\end{cases}
$$</p>
<p>给定 $H \in \mathbb{R}^{n \times k}$，使得 $H^T H = I$。</p>
<p>我们可以看到(设 $h_ i$ 是 $(h_ {1i}, \cdots, h_ {ni}) \in \mathbb{R}^n$ 的向量)：</p>
<p>回忆：$$v^T L v = \frac{1}{2} \sum_ {ij} w_ {ij}(v_ i - v_ j)^2$$</p>
<ul>
<li>$$h_ i^T L h_ i = \frac{\text{cut}(A_ i, A_ i^c)}{|A_ i|} = (H^T L H)_ {ii}$$
所以：
$$
\text{ratiocut}(A_ 1, \cdots, A_ k) = \sum_ {i=1}^k h_ i^T L h_ i = \sum_ {i=1}^k (H^T L H)_ {ii} = \text{Tr}(H^T L H)
$$</li>
</ul>
<p>也就是说对于满足上述$h$的条件的矩阵$H$,我们的<strong>优化目标</strong>是：
$$
\text{min } \text{Tr}(H^T L H) \quad \text{s.t. } H^T H = I, h_ {ij} \text{ see above}
$$</p>
<p>与$k=2$情况类似，我们relax对于$H$的限制，得到:
$$
\text{min } \text{Tr}(H^T L H) \quad \text{s.t. } H^T H = I
$$</p>
<p>这里有个结论，就是说这个$H$对应的就是$L$最小的$k$个特征向量(受限篇幅与作者的能力（小声），聪明的读者应该不难自证)，然后对$H$的行向量跑k-means就可以还原出聚类${A_ {[1:k]}}$了。所以我们找到Graph Spectral Clustering算法的理论依据了，也就是说我们在解决一个relaxed version的最小化聚类的RatioCut。</p>
<h2 id="simclr">SimCLR<a hidden class="anchor" aria-hidden="true" href="#simclr">#</a></h2>
<p>这里讲个Clustering+ Metric Learning的应用，也是袁洋老师ICLR 2024年的崭新工作，证明了SimCLR这种对比学习方法其实就是在similarity graph上折腾了一些操作的spectral clustering。</p>
<h3 id="what-is-simclr">What is SimCLR<a hidden class="anchor" aria-hidden="true" href="#what-is-simclr">#</a></h3>
<p>首先讲讲什么是SimCLR。这个是对比学习的一个算法，比如说给一个被查询的样本$q$，同时还有一个正样本$p_ 1$,对应$N-1$个负样本${p_ i}_ {i=2}^N$。</p>
<p><img loading="lazy" src="../img/ml3/image8.png#center">
一个现实的例子是我们对于所有输入的图片生成两个augmented图片，那么以其中一个图片作为查询样本，另一个就是正样本，别的图片augment之后的结果就是负样本。</p>
<p><img loading="lazy" src="../img/ml3/image9.png#center"></p>
<p>这里的$q,p_ 1$可以在pixel space上差距极大，但是我们希望他们在我们学到的semantic space上距离接近。我们的优化目标是<strong>InfoNCELoss</strong>:</p>
<p>$$
L(p,q,{p_ i}_ {i=2}^N)=-\log \frac{\exp(-||f(q)-f(p_ 1)||^2)/2\tau}{\sum_ {i=1}^N \exp(-||f(q)-f(p_ i)||^2)/2\tau}
$$</p>
<h3 id="what-is-the-similarity-graph-here">What is the Similarity Graph Here<a hidden class="anchor" aria-hidden="true" href="#what-is-the-similarity-graph-here">#</a></h3>
<p>这里的Similarity Graph的定义对于所有augmented image构成的${X_ i}$集合，$(X_ i,X_ j)$的边权是
$$\pi_ {i,j}=\Pr[X_ i,X_ j\text{ are sampled together}]$$
有没有一个理想的空间，其中semantic similarity是被自然的捕捉到的呢？答案是有的，这里引入<strong>Reproducing Kernel Hilbert Space</strong>.</p>
<h3 id="reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)<a hidden class="anchor" aria-hidden="true" href="#reproducing-kernel-hilbert-space-rkhs">#</a></h3>
<p>给定两个在$Z$空间的物品: $Z_ 1,Z_ 2\in Z$, 考虑$\phi: Z\rightarrow H$, 这里$H$的维度远大于$Z$.</p>
<p>$$k(Z_ i,Z_ j)=\langle \phi(Z_ i),\phi(Z_ j)\rangle_ H$$</p>
<p>这里的$H$就是Hilbert space,而$k$Rreproducing Kernel。</p>
<p><img loading="lazy" src="../img/ml3/image10.png#center"></p>
<p>因为我们关心的只是样本之间的相似性，所以我们不用真的知道或者能算$\phi(Z_ i)$,我们只需要算样本之间的$k$就ok了。</p>
<h3 id="markov-random-fields-mrf">Markov Random Fields (MRF)<a hidden class="anchor" aria-hidden="true" href="#markov-random-fields-mrf">#</a></h3>
<p>原论文里是这么说的</p>
<blockquote>
<p>Due to the large size of $\pi$ and in practice $\pi$ is usually formed by using positive samples sampling and hard to explicitly construct, directly comparing $K_ Z$ and $\pi$ can be difficult, so we treat them as MRFs and compare the induced probability distributions on subgraphs instead.</p>
</blockquote>
<p>motivation很清晰，那么我们具体怎么采样呢？我们想要sample出的unweighted子图我们设为$W$(也就是说$W_ {ij}\in{0,1}$)</p>
<p>$$P(W;\pi)\propto \Omega(W)\cdot \prod_ {{i,j}\in [n]^2}\pi_ {i,j}^{W_ {i,j}}  $$
怎么解读呢？
$$s(W,\pi)=\prod_ {{i,j}\in [n]^2}\pi_ {i,j}^{W_ {i,j}}$$
对于可能的$W$，我们根据他的score function来采样，然后我们看看采样出来的$W$是不是正确的形状，比如</p>
<p>$$\Omega(w)=\prod_ i \mathbb{1}[\sum_ jW_ {ij}=1]$$
这个被称为Unitary out-degree filter，也就是说每一个顶点要求有且仅有一条出边。</p>
<p>接下来考虑$P(W;K_ z)$:
$$P(W;K_ z)\propto \Omega(W)\cdot \prod_ {{i,j}\in [n]^2}k(Z_ i,Z_ j)^{W_ {i,j}}  $$</p>
<p><img loading="lazy" src="../img/ml3/image11.png#center"></p>
<h3 id="infonce-and-spectral-clustering">InfoNCE and Spectral Clustering<a hidden class="anchor" aria-hidden="true" href="#infonce-and-spectral-clustering">#</a></h3>
<p>好了，概念搭的差不多了，落地我们通过比较$W_ X,W_ Z$的差异来反应我们理想中$\pi,K_ Z$的差值。这一差值可以通过Cross Entropy衡量：
$$H_ \pi^k(Z)=-\mathbb{E}_ {W_ X\sim P[\cdot;\pi]}\log P[W_ Z=W_ X;K_ z]$$
接下来如果能证明</p>
<ol>
<li>InfoNCE和$H_ \pi^k(Z)$等价</li>
<li>$H_ \pi^k(Z)$和Spectral Clustering等价</li>
</ol>
<p>我们的任务就完成了。</p>
<h4 id="infonce和h_-pikz等价">InfoNCE和$H_ \pi^k(Z)$等价<a hidden class="anchor" aria-hidden="true" href="#infonce和h_-pikz等价">#</a></h4>
<p>先从直觉上理解下这件事情:
$$H_ \pi^k(Z)=-\mathbb{E}_ {W_ X\sim P[\cdot;\pi]}\log P[W_ Z=W_ X;K_ z]$$</p>
<ul>
<li>$W_ X\sim P[\cdot;\pi]$: 我们从$\pi$上取样，代表着Data Augmentation Step</li>
</ul>
<p>接下来用一个原论文中的重要结论: 对于Unitary out-deg $\Omega(w)$,
$$W_ i\sim M(1, \frac{\pi_ i}{\sum_ j \pi_ {i,j}})$$
也就是我们按照$\frac{\pi_ i}{\sum_ j \pi_ {i,j}}$取样一个one-hot vector。
<img loading="lazy" src="../img/ml3/image12.png#center">
因为每一行是独立的，所以我们有:
$$H_ \pi^k(Z)=-\sum_ i\mathbb{E}_ {W_ {X,i}}\log P[W_ {Z,i}=W_ {X,i};K_ z]$$
这里$W_ {X,i}$代表$W_ X$的第$i$行(one-hot 向量)。</p>
<p>接下来想一下InfoNCE在说什么事情：</p>
<p>$$InfoNCE=-\sum_ {i=1}^n\log \frac{\exp(-||f(X_ i)-f(X_ i&rsquo;)||^2)/2\tau}{\sum_ {j=1}^N \exp(-||f(X_ i)-f(X_ j))||^2)/2\tau}$$</p>
<p>如果我们定义
$$Q_ i=\frac{K_ {Z,i}}{||K_ {Z,i}||_ 1} $$
作为$P(\cdot;K_ Z)$的分布</p>
<p>那么
$$\text{InfoNCE}=-\sum_ {i=1}^n \sum_ {i&rsquo;=1}^n \Pr[W_ {X,i,i&rsquo;}=1] \log Q_ {i,i&rsquo;}=-\sum_ i\mathbb{E}_ {W_ {X,i}}\log P[W_ {Z,i}=W_ {X,i};K_ z]=H_ \pi^k(Z)$$</p>
<h4 id="h_-pikz和spectral-clustering等价">$H_ \pi^k(Z)$和Spectral Clustering等价<a hidden class="anchor" aria-hidden="true" href="#h_-pikz和spectral-clustering等价">#</a></h4>
<p>$$H_ \pi^k(Z)=-\mathbb{E}_ {W_ X\sim P[\cdot;\pi]}\log P[W_ Z=W_ X;K_ z]$$
其中
$$P[W_ Z=W_ X;K_ z] \propto \Omega(W_ X) \prod_ {{i,j}\in [n]^2} K_ {Z_ {i,j}}^{W_ {X_ {i,j}}}$$</p>
<p>然后我们就可以显式的表达概率
$$R(Z)=\sum_ {W}\Omega(W)\cdot \prod_ {{i,j}\in [n]^2}K_ {Z_ {i,j}}^{W_ {i,j}}$$</p>
<p>所以说
$$\log P[W_ Z=W_ X;K_ z]=\sum_ {i,j}W_ {X_ {i,j}}\log K_ {Z_ {i,j}}+\log \Omega(W_ X)-\log R(Z)$$</p>
<p>这里对于fixed $W_ X$，$\log \Omega(W_ X)$是常数，所以:
$$
\arg\min_ Z H_ \pi^k(Z) = \arg\min_ Z -\mathbb{E}_ {W_ X \sim P(\cdot; \pi)} \Bigg[\sum_ {(i,j) \in [n]^2} W_ {X,ij} \log k(Z_ i, Z_ j) - \log R(Z) \Bigg]
$$
$$
= \arg\min_ Z -\mathbb{E}_ {W_ X \sim P(\cdot; \pi)} \sum_ {(i,j) \in [n]^2} W_ {X,ij} \log k(Z_ i, Z_ j) + \log R(Z)
$$
回忆下，$k$ 是 Gaussian 分布来的：</p>
<p>$$\log k(Z_ i, Z_ j) = -\frac{||Z_ i - Z_ j||^2}{2\tau}
$$</p>
<p>所以原式
$$
= \arg\min_ Z \mathbb{E}_ {W_ X \sim P(\cdot; \pi)} \frac{1}{2\tau} \sum_ {(i,j) \in [n]^2} W_ {X,ij} |Z_ i - Z_ j|^2 + \log R(Z)
$$</p>
<p>$$
= \arg\min_ Z \mathbb{E}_ {W_ X \sim P(\cdot; \pi)} \frac{1}{\tau} \operatorname{tr}(Z^T L(W_ X) Z) + \log R(Z)
$$</p>
<p>$$
= \arg\min_ Z \frac{1}{\tau} \operatorname{tr}(Z^T L^* Z) + \log R(Z)\quad\blacksquare
$$
真长啊。</p>
<h2 id="t-sne">t-SNE<a hidden class="anchor" aria-hidden="true" href="#t-sne">#</a></h2>
<p>最后讲一个Data Visualization/ Dimension Reduction的算法，首先回忆一下NCA:</p>
<p>原空间上两个点$(x_ i,x_ j)$的similarity是这么定义的:
$$
p_ {j|i} = \frac{\exp\left(-\frac{|x_ i - x_ j|_ 2^2}{2\sigma_ i^2}\right)}{\sum_ {k \neq i} \exp\left(-\frac{|x_ i - x_ k|_ 2^2}{2\sigma_ i^2}\right)}
$$</p>
<p>我们做映射后$ x_ i \to y_ i = f(x_ i) $:
$$
q_ {j|i} = \frac{\exp\left(-|y_ i - y_ j|_ 2^2\right)}{\sum_ {k \neq i} \exp\left(-|y_ i - y_ k|_ 2^2\right)}
$$</p>
<p>我们希望$p,q$能对应的上。</p>
<p>在训练中，用KL-Divergence做损失函数：
$$
L = \sum_ i \text{KL}(P_ i | Q_ i) = \sum_ i \sum_ j p_ {j|i} \log \frac{p_ {j|i}}{q_ {j|i}}
$$</p>
<p>细心的读者发现和无监督学习(I)里讲的NCA的优化目标好像略有区别(差个log)，但无伤大雅，我们不去管他。<strong>怎么去选$\sigma_ i$呢?</strong></p>
<h3 id="sne">SNE<a hidden class="anchor" aria-hidden="true" href="#sne">#</a></h3>
<p>用户选择一个<strong>Perplexity</strong>，这个直觉的理解是对于有效邻居数量的
smooth measure。
$$
\text{Perp}(P_ i) = 2^{H(P_ i)}
$$
$$
H(P_ i) = -\sum_ j p_ {j|i} \log p_ {j|i}
$$
接下来我们对$\sigma_ i$做二分查找找到合适的$\sigma_ i$值，这就叫<strong>SNE algorithm</strong>。</p>
<h3 id="t-sne-1">t-SNE<a hidden class="anchor" aria-hidden="true" href="#t-sne-1">#</a></h3>
<p>SNE很经典，但是有如下两个缺点的</p>
<ol>
<li>
<p>要优化很多个loss，你看loss其实是对i和j求了两遍和，相当于每个pair贡献了一个loss，或者说有$n$个分布要去学</p>
</li>
<li>
<p><strong>拥挤问题(crowding problem)</strong></p>
</li>
</ol>
<p><img loading="lazy" src="../img/ml3/image13.png#center">
比如说考虑一个${0,1}^d$的grid,对于$r=10$的情况有$2^{10}$个可以放的位置，但是投影到$r=2$上就只有$2^2$个了，全挤到一块分不开了。</p>
<p><strong>为了解决第一个问题</strong>，t-SNE的做法是，把所有的距离放一起做运算，捏成一个概率分布，优化一个
single概率分布的loss:
$$p_ {i,j}=\frac{p_ {j|i}+p_ {i|j}}{2n}$$
这样设置是为了
$$\sum_ {i j} p_ {i j}=1$$</p>
<p>为什么要变成一个distribution呢，因为计算梯度更容易、更快。</p>
<p><strong>为了解决第二个问题</strong>，t-SNE的想法是换一个更heavy-tail的distribution，这样保持相对距离，绝对距离改变，就还能分得开。</p>
<p>也就是:</p>
<ul>
<li>高维中近距离的点，在低维中距离要变得更小</li>
<li>高维中远距离的点，在低维中距离要变得更大</li>
</ul>
<p>这里用student t-distribution就很合适,因为更heavy-tail:</p>
<p>距离由$\frac{1}{1+||y_ i-y_ j||^2}$刻画
$$p_ {i j}=\frac{(1+||y_ i-y_ j||^2)^{-1}}{\sum_ {k\neq l}(1+||y_ l-y_ k||^2)^{-1}}$$</p>
<p>没了，感觉这一部分大量参考了<a href="https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">这篇文献</a>。</p>
<h3 id="a-side-note">A side note:<a hidden class="anchor" aria-hidden="true" href="#a-side-note">#</a></h3>
<p>If you are interested in clustering, you can also check out <a href="https://leo1oel.github.io/clustering/">this website</a>, which contains a survey of clustering algorithms(the pdf file link is in the website). It&rsquo;s a project done by me and my friend <a href="https://leo1oel.github.io/">Yiming Liu</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/blog/tags/machine-learning/">Machine-Learning</a></li>
      <li><a href="http://localhost:1313/blog/tags/computer-science/">Computer-Science</a></li>
      <li><a href="http://localhost:1313/blog/tags/algebra/">Algebra</a></li>
      <li><a href="http://localhost:1313/blog/tags/math/">Math</a></li>
      <li><a href="http://localhost:1313/blog/tags/artificial-intelligence/">Artificial-Intelligence</a></li>
      <li><a href="http://localhost:1313/blog/tags/algorithm/">Algorithm</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blog/posts/ml4/">
    <span class="title">« Prev</span>
    <br>
    <span>Machine Learning Series: 4.Robust Machine Learning</span>
  </a>
  <a class="next" href="http://localhost:1313/blog/posts/ml2/">
    <span class="title">Next »</span>
    <br>
    <span>Machine Learning Series: 2.Unsupervised Learning(I)</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 3.Unsupervised Learning(II) on x"
            href="https://x.com/intent/tweet/?text=Machine%20Learning%20Series%3a%203.Unsupervised%20Learning%28II%29&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f&amp;hashtags=machine-learning%2ccomputer-science%2calgebra%2cmath%2cartificial-intelligence%2calgorithm">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 3.Unsupervised Learning(II) on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f&amp;title=Machine%20Learning%20Series%3a%203.Unsupervised%20Learning%28II%29&amp;summary=Machine%20Learning%20Series%3a%203.Unsupervised%20Learning%28II%29&amp;source=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 3.Unsupervised Learning(II) on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f&title=Machine%20Learning%20Series%3a%203.Unsupervised%20Learning%28II%29">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 3.Unsupervised Learning(II) on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 3.Unsupervised Learning(II) on whatsapp"
            href="https://api.whatsapp.com/send?text=Machine%20Learning%20Series%3a%203.Unsupervised%20Learning%28II%29%20-%20http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 3.Unsupervised Learning(II) on telegram"
            href="https://telegram.me/share/url?text=Machine%20Learning%20Series%3a%203.Unsupervised%20Learning%28II%29&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning Series: 3.Unsupervised Learning(II) on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Machine%20Learning%20Series%3a%203.Unsupervised%20Learning%28II%29&u=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fml3%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/blog/">Nemo&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
    <div class="busuanzi-footer">
        <span id="busuanzi_container_site_pv">
            Total site visits: <span id="busuanzi_value_site_pv"></span> times
        </span>
        
    </div></footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
