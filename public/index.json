[{"content":"Motivation 调超参当然是一个痛苦的事情了，那么有没有什么办法来让我们少调一调呢？答案是有的，下面将介绍一些古人的智慧。\n但在开始之前，还是让我们用数学的语言表达一下超参选择是个什么样的问题吧：\n我们想要找到$f(x_ 1,⋯, x_ d )$的最小值点，其中$x_ i$代表超参可能是连续或者离散的：$x_ i\\in[a,b]$或者$x_ i \\in {0,1,2,\u0026hellip;}$，我们只能通过查询单点处的函数值，没法获得1阶和2阶的梯度信息。\n而且，这个$f$很可能是一个没有太多好的性质的函数，比如不一定有convex的性质。我们希望找最小值的过程尽可能的sample-efficient,因为每一次实验都可能需要好几小时甚至天。\nBayesian Optimization 这个算法的high-level idea是这样的：\nStep 1: Assume a prior distribution for the loss function $f$. Step 2: Select new sample(s) that balances exploration and exploitation. Either the new sample(s) gives better result.Or gives more information about $f$. Step 3: Update prior with the new sample(s) using Bayes\u0026rsquo; rule. Go to Step 2. 在这里，我们需要两个模型：\n代理模型（surrogate model）: 用于对目标函数进行建模。 采集函数（acquisition function）：用来平衡探索和利用，指导选择下一个采样点。 Gaussian Process 这里一般对于代理模型的建模，我们将高斯函数拓展到无穷维空间上去，其中每一个输入$x$都对应一个维度。通常假设每个维度的均值为 0（即$\\mathbb{E}[f(x_ i)]=0$），这样能简化模型。如果有先验知识，也可以设为非零的均值函数。两个输入点 $x_ i$和$x_ j$的相关性由核函数$\\mathbb{E}[f(x_ i)f(x_ j)]=K(x_ i,x_ j)$定义。这个核函数表达了我们对函数光滑性、相似性等性质的假设。\n经过一些数学，我们有如下结论：\n当给定 $m$ 个已有的观测点 $(x_ 1, y_ 1), \\cdots, (x_ m, y_ m)$ 时，我们有： $$ f(x) \\mid ((x_ 1, y_ 1), \\cdots, (x_ m, y_ m)) \\sim \\mathcal{N}\\left(k_ *^T \\Sigma^{-1} y, K(x, x) - k_ *^T \\Sigma^{-1} k_ *\\right)$$\n其中：\n$k_ * = [K(x_ 1, x), \\cdots, K(x_ m, x)]$ $y = (y_ 1, \\cdots, y_ m)$ $\\Sigma$ 是 $m \\times m$ 的协方差矩阵，由 $K(x_ i, x_ j)$ 组成 由此我们可以获得我们当前模型在特定输入下的输出的均值和方差，之后我们的采集函数可以通过对于计算例如期望改进（Expected Improvement, EI）在当前最优值的基础上寻找改进期望值。\n这个具体的过程鼠鼠我也不是很懂，放几个链接大家有兴趣看看去吧：\n链接1 链接2 看点visualization:\nGradient Optimization 对于连续的超参，我们可以通过梯度递降的方法来优化：\n我们考虑一个简单的例子，在线性回归中寻找学习率的最佳值。我们知道这个损失函数是：$$L(w)=\\frac{1}{2}\\sum_ {i=1}^n(w^T x-y)^2$$ 对于其参数$w$求梯度： $$\\nabla_ w L(w)=\\sum_ {i=1}^n(w^T x-y)x$$ 然后梯度递降： $$w_ 1=w_ 0-\\eta \\nabla_ w L(w_ 0)$$ $$w_ 2=w_ 1-\\eta \\nabla_ w L(w_ 1)$$ 那么我们想要求$\\nabla_ \\eta f(w_ 0,\\eta):=L(w_ 2)$，其实只用使用下链式法则： $$\\nabla_ \\eta f(w_ 0,\\eta)=\\nabla_ w L(w_ 2)\\cdot \\nabla_ \\eta w_ 2=\\sum_ {i=1}^n(w_ 2^T x-y)x \\cdot \\nabla_ \\eta w_ 2$$ 对于$\\nabla_ \\eta w_ 2$,我们继续求导： $$\\nabla_ \\eta w_ 2=\\nabla_ \\eta w_ 1-\\nabla_ w L(w_ 1)-\\eta \\nabla_ \\eta(\\nabla_ w L(w_ 1))$$ 然后接着顺着往下求。\nMemory Problem 刚才是naive的反向传播梯度的方法，但是这个会带来一个很显著的问题，就是对于计算$\\eta \\nabla_ \\eta \\nabla_ w L(w_ i),i=1,2,\u0026hellip;,T$的梯度，我们假如将$w_ 1,\u0026hellip;,w_ T$全部存入内存的话，内存是会爆炸的，因为太大了。那么，有什么办法解决吗？让我们对SGD with momentum的优化器进行分析：\n$v_ t$如何理解呢？$v_ t$可以理解为一个历史梯度状态的压缩(等比平均?)，因为当前的梯度的方差可能太大，所以有这样一种soft的更新方法有利于让优化过程更鲁棒的。而且收敛率也更快，$O(\\frac{1}{T^2})$快于SGD的$O(\\frac{1}{\\sqrt{T}})$.\n这里的核心出装在于： $$ v_ {t+1} = \\gamma v_ t - (1 - \\gamma) \\nabla_ w L(w_ t) $$\n$$ w_ {t+1} = w_ t + \\eta v_ {t+1} $$ 也就是说因为我只需要$w_ t$和$v_ t$,我们就可以左脚踩右脚，算出之前的$w_ i$和$v_ i$了，所以我们只需要存一对当前时刻的$w_ t$和$v_ t$即可了。\n听起来挺好的，但是因为这是计算机科学不是数学，我们存的数是会有精度损失的，也就是说因为$v_ t$的精度有限，所以其实还是会丢失一部分历史信息。而且这个问题不能忽略，因为误差累计是指数上涨的。那么怎么解决呢？\n我们可以用整数表达一切，在除什么的时候，将余数放入一个Buffer中，然后再乘回来的时候把这个余数加回来。\nComments: 这种方法只适用于连续的超参优化，而且优化过程也比较容易卡在local minima。\nRandom Search 顾名思义，就是对于可选的参数区间随机的取样。在实际中效果很好，比Grid Search（枚举所有可能）要样本利用率高很多。\nMulti-Arm Bandits Best Arm Identification 这里的多臂老虎机的目标和强化学习中比如UCB算法是不同的，对于UCB类的算法，他的目标是获得最高的累积回报，而在这里的setup是去找到最佳的老虎机。\n有$n$个臂，每次拉动一个臂时都会得到一个奖励，该奖励是一个具有期望值$v_ i$的有界随机变量。 每次选择一个臂并拉动时，会得到其奖励的一个独立样本。 在固定预算的情况下，我们如何找到期望值$v_ i$最大的臂？\nSuccessive Halving(SH) Algorithm 也就是说每一轮我们把预算平均分配给还存活的机器，然后计算获得的回报的均值，然后去掉回报小的那一半机器，再进入下一轮。下图为一示例：\nWLOG, 我们假设$v_ 1\u0026gt;v_ 2\\geq\u0026hellip;\\geq v_ n$,定义$\\Delta_ i=v_ 1-v_ i$.\nThm. With Probability $1-\\delta$, the algorithm finds the best arm with $$B = \\Theta\\left(H_ 2 \\log n \\log\\left(\\frac{\\log n}{\\delta}\\right)\\right)$$ arm pulls. $H_ 2=max_ {i\u0026gt;1}\\frac{i}{\\Delta_ i^2}$.\n证明如下：\n如果第一个arm在第$r$轮之前没有被淘汰，那么对于任意不是arm 1的$i \\in S_ r$, 对于每一个arm有$\\frac{B}{|S_ r|log(n)}$的采样率，所以由Hoeffding Inequality:\n$$Pr[\\hat{v}_ 1^r\u0026lt;\\hat{v}_ i^r]\\leq \\exp(-\\frac{1}{2}\\frac{B\\Delta^2_ i}{|S_ r|\\log(n)})$$\n令$n_ r=\\frac{n}{2^{r+2}}$,也就是说我们在round r把这些还存活的arm进行4等分。接下来我们把这个arm对应的真实值小的后3/4记为$S_ r\u0026rsquo;$,那么如果我们用$N_ r$记录$S_ r\u0026rsquo;$中在这一轮中的平均值大于arm1的arm的数量，有： $$\\mathbb{E}[N_ r]\\leq\\sum_ {i \\in S_ r\u0026rsquo;} \\exp(-\\frac{1}{2}\\frac{B\\Delta^2_ i}{|S_ r|\\log(n)})\\leq |S_ r\u0026rsquo;|\\exp(-\\frac{1}{8}\\frac{B\\Delta^2_ {n_ r}}{n_ r \\log(n)}) $$ 接着用Markov Inequality: $$Pr[N_ r\u0026gt;\\frac{1}{3}|S_ r\u0026rsquo;|]\\leq 3 \\exp(-\\frac{1}{8}\\frac{B\\Delta^2_ {n_ r}}{n_ r \\log(n)})$$ 也就是说，有很高概率并没有那么多不那么好的机器的empirical mean比最好的机器的empirical mean大。\n最后，因为只有在后3/4中有至少1/3比arm 1大的时候，arm 1才有可能被淘汰，所以说arm 1在任意一轮被淘汰的概率最多是： $$3 \\sum_ {r=1}^{\\log n} \\exp(-\\frac{1}{8}\\frac{B\\Delta^2_ {n_ r}}{n_ r \\log(n)})\\leq 3 \\log(n) \\exp(-\\frac{B}{8 H_ 2 \\log(n)})$$ 这等价于 $$B = \\Omega\\left(H_ 2 \\log n \\log\\left(\\frac{\\log n}{\\delta}\\right)\\right)$$\nApplication to HyperParameter Tuning 在超参选择上，每一个超参的set都是一个arm，在初始阶段，我们随机选择许多配置。\n在setting上不太一样的点是：\n假设：可以观察到中间结果，能够在训练中途终止一些配置。\n操作：在训练过程中移除较不具前景的超参对应的实验。\n另一不一样的点是，我们并不是直接从随机变量中抽取样本，而是可以通过付出一定的代价来获得更加准确的观测值，这个代价就是更久的观察时间。最后观测到的值作为返回值。\n也就是说对于所有 $i \\in [n], k \\geq 1$，令 $\\ell_ {i,k} \\in \\mathbb{R}$ 为臂 $i$ 的一个序列，假设： $$ v_ i = \\lim_ {\\tau \\to \\infty} \\ell_ {i,\\tau} \\quad \\text{存在} $$ 那么对应的投入更多的budget就是对于运行更多的epoch数。\n一个实际运行的例子： 那么在这样的setting下有没有理论的保证呢？\n我们首先引入一些记号：\n$\\gamma_ i (t)$: 关于$t$单调不增，它给出了每个 $t$ 对应的最小值，使得： $$|\\ell_ {i,t} - v_ i| \\leq \\gamma_ i(t)$$ 也就是说它是曲线的“包络线”，表示当前观测值距离极限$v_ i$的接近程度。 $\\gamma_ i^{-1}(\\alpha) = \\min{t \\in \\mathbb{N}: \\gamma_ i(t) \\leq \\alpha}$ 表示首次进入与 $v_ i$ 的 $\\alpha$-邻域的时间点,值得注意的是，这里我们假设一旦我们进入，我们就再也不会出去了。 如果 $ k_ i \\geq \\gamma_ i^{-1}\\left(\\frac{v_ i - v_ 1}{2}\\right) $ 且 $ k_ 1 \\geq \\gamma_ 1^{-1}\\left(\\frac{v_ i - v_ 1}{2}\\right) $，则臂 $ i $ 和臂 $ 1 $ 可以被分开（即区分出优劣)。\nTheorem：\n令 $\\bar{\\gamma}(t) = \\max_ i \\gamma_ i(t)$，则有：$$B \\geq 2 \\log_ 2(n) \\left( n + \\sum_ {i=2,\\dots,n} \\bar{\\gamma}^{-1}\\left(\\frac{v_ i - v_ 1}{2}\\right) \\right)$$ 在以上条件下，SH算法能够返回最佳臂。\n证明如下：\n注意到： $$ B\u0026rsquo; = 2 \\left( n + \\sum_ {i=2,\\dots,n} \\bar{\\gamma}^{-1} \\left( \\frac{v_ i - v_ 1}{2} \\right) \\right) $$\n每个臂被拉的次数为：$\\frac{B\u0026rsquo;}{|S_ r|}$,其中： $$ \\frac{B\u0026rsquo;}{|S_ r|} \u0026gt; \\bar{\\gamma}^{-1} \\left( \\frac{v_ {\\lfloor\\frac{|S_ r|}{2}\\rfloor+1} - v_ 1}{2} \\right) $$ 这个结论是初等数学结论，读者不难自证。\n如果： $$ k_ i \\geq \\gamma_ i^{-1} \\left( \\frac{v_ i - v_ 1}{2} \\right), \\quad k_ 1 \\geq \\gamma_ 1^{-1} \\left( \\frac{v_ i - v_ 1}{2} \\right) $$ 那么臂 $i$ 和臂 $1$ 可以被区分开。\n因此，在第 $k$ 轮中，我们知道臂 $\\lfloor |S_ r| / 2 \\rfloor + 1$ 和臂 $1$ 已经被区分开。\n所以我们在$S_ {\\log_ 2(n)}$轮中就能够辨认最佳臂1了。\nNeural Architecture Search 这里的任务是Given a specific task. Find the best network structure for this task.\n一些成功的工作包括：\n强化学习 随机搜索 传统NAS算法需要大量的GPU计算资源。 通常只能用于一些代理任务（小规模/辅助任务）： 在小型数据集上训练。 使用少量的神经网络模块（blocks），仅训练几个epoch。 计算代价较低，但扩展到大规模任务时效果有限。 ProxyLess NAS 我们希望找到一个算法，使其能够适用于更大的任务。因此引入ProxylessNAS。\n方法：\n对于每一层（或边），考虑所有可能的结构 定义 $N$ 个组件 $o_ i$（例如不同的卷积滤波器大小、Identity层、池化层等） 联合训练结构（layer被选择的概率）与权重（内部的权重和偏置） 对于模型输出的类型，有如下三种方法：\nOne-shot方法（Bender et al., 2018）：\n输出为所有组件的加权和： $$ \\sum_ {i=1}^N o_ i(x) $$ 性能不足。 DARTS方法（Liu et al., 2018）：\n使用权重 $ \\alpha_ i $ 定义输出： $$ \\sum_ {i=1}^N p_ i o_ i(x), \\quad p_ i = \\frac{e^{\\alpha_ i}}{\\sum_ {j=1}^N e^{\\alpha_ j}} $$ 缺点：内存效率低，因为需要存储所有$N$条路径。最终模型只包含一条路径。 ProxylessNAS方法：\n二值化路径，定义布尔变量 $g$（一个one-hot向量）： $$ g = \\begin{cases} [1, 0, \\dots, 0], \u0026amp; \\text{概率为 } p_ 1 \\ \\vdots \\ [0, 0, \\dots, 1], \u0026amp; \\text{概率为 } p_ N \\end{cases} $$ 输出依赖于单一路径： $$ \\sum_ {i=1}^N g_ i o_ i(x) = \\begin{cases} o_ 1(x), \u0026amp; \\text{概率为 } p_ 1 \\ \\vdots \\ o_ N(x), \u0026amp; \\text{概率为 } p_ N \\end{cases} $$ 注：这里的$p_ {[1:N]}$是根据$\\alpha_ {[1:n]}$通过softmax采样得到的。 优势：大幅节省内存。只需存储单一路径，而不存储所有 (N) 条路径。 最后看一下训练过程：\n交替训练网络结构和权重：\n在训练权重时，冻结 $ \\alpha_ i $，并采样结构。 在训练 $ \\alpha_ i $ 时，冻结权重。 如何学习 $\\alpha_ i$：\n链式法则近似计算 $ \\frac{\\partial L}{\\partial \\alpha_ i} $： $$ \\frac{\\partial L}{\\partial \\alpha_ i} = \\sum_ {j=1}^N \\frac{\\partial L}{\\partial g_ j} \\frac{\\partial g_ j}{\\partial \\alpha_ i} \\approx \\sum_ {j=1}^N \\frac{\\partial L}{\\partial g_ j} \\frac{\\partial p_ j}{\\partial \\alpha_ i} $$ 其中： $$ \\frac{\\partial p_ j}{\\partial \\alpha_ i} = \\sum_ {j=1}^N \\delta_ {ij} p_ j (1 - p_ i) - p_ i p_ j $$ $ \\delta_ {ij} = 1 $ 如果 $ i = j $，否则为0。 更多的细节可以看一下原论文。 Misc 2025年了，祝大家新年快乐！\n至于认识我的朋友，解释下为什么今年没有发朋友圈，因为实在是没有太多值得说的东西，有很多under-construction的事情，所以，2025对我、也希望对大家，会是很让人兴奋的一年。 调超参是一个听起来很有趣但实际上大家都在做Graduate Student Search的领域，也希望在做AI相关领域科研的朋友能够在2025年有“金手指”，调参手到擒来！\n","permalink":"http://localhost:1313/blog/posts/ml5/","summary":"This is the fifth article in the Machine Learning Series. It covers classic approaches to Hyperparameter Selection, including Bayesian Optimization, Gradient Optimization, Random Search, Multi-Arm Bandits and Neural Architecture Search.","title":"Machine Learning Series: 5.Hyperparameter Selection"},{"content":"动机 鲁棒的机器学习的动机来自于对于模型而言，在输入中加入一点点微小的修改，就可以让模型的输出大不相同，这样的现象可以被别有用心者利用，比如让摄像头识别不出来人，或者引诱自动驾驶汽车认为前面有障碍物，从而干扰后续的decision making. Adversial Attacks 所以为了获得一个对于扰动鲁棒的模型，我们讲原模型的loss function重新定义： $$\\mathbb{E}_ {x,y}[Loss_ \\theta (f_ \\theta (x),y)]\\Rightarrow \\mathbb{E}_ {x,y}[\\max_ {\\delta \\in \\Delta}Loss_ \\theta (f_ \\theta (x+\\delta),y)] $$ 面对这样的优化目标，我们可以采取以下两种优化算法来获得最佳的$\\delta$：\nProjected gradient descent 这个算法本质上就是做正常的梯度递降，然后投影到$\\Delta$上 $$\\delta_ {t+1}=P_ \\Delta[\\delta_ t+\\eta\\nabla_ x Loss(f(x+\\delta_ t),y)]$$\n其中 $$ \\Delta={\\delta: ||\\delta||_ \\infty \u0026lt; \\epsilon }$$ $$P_ \\delta=Clip(\\delta,[-\\epsilon,\\epsilon])$$\nFast Gradient Sign Method (FGSM) 考虑$\\eta\\rightarrow \\infty$的情况，对于一个convex set, 我们一定会走到convex set的vertex上，所以我们只选取梯度的每个方向上的符号，就足够了。\n所以 $$\\delta=\\eta \\cdot sign(\\nabla_ x Loss(f(x),y)).$$\n所以，也可以把PGD理解成多次进行FGSM，这样会更慢但是找到的local optimal更好。\nAdversial Training 鲁棒机器学习带来的一个问题就是说我们知道怎么优化$\\delta$了，那怎么优化$\\theta$呢？ 这个本质上是一个min-max的优化问题： $$\\min_ \\theta \\sum_ {(x,y)\\in S}\\max_ {\\delta \\in \\Delta}Loss_ \\theta (f_ \\theta (x+\\delta),y)$$ 这个max函数是没有gradient的，那么怎么解决呢？\n这里我们要引入一个数学定理:\nDanskin\u0026rsquo;s Theorem: $$\\frac{\\partial}{\\partial \\theta}\\max_ {\\delta \\in \\Delta}Loss_ \\theta (f_ \\theta (x+\\delta),y)=\\frac{\\partial}{\\partial \\theta}Loss_ \\theta (f_ \\theta (x+\\delta^*),y)$$ 其中$\\delta^*=\\arg\\max_ {\\delta \\in \\Delta}Loss_ \\theta (f_ \\theta (x+\\delta),y)$\n这个看似显然的定理的证明其实不是很容易。 于是乎我们有如下算法：\nRepeat the following:\nSelect minibatch B For each $(x, y) \\in B$, compute adversarial example $\\delta^*(x)$ Update parameters:$\\theta_ {t+1} = \\theta_ t - \\frac{\\eta}{|B|} \\sum_ {(x, y) \\in B} \\frac{\\partial}{\\partial \\theta_ t} \\text{Loss}(f_ {\\theta_ t}(x + \\delta^*(x)), y)$ 在测试模型的时候，可以随机选取数据点，进行PGD的adversial attack，来测试模型预测的准确性。\n值得注意的是，鲁棒的模型并不具备通用的鲁棒性，也就是对于L2-norm鲁棒的模型不一定对于L1或者L$\\infty$的鲁棒性。\n这样的鲁棒模型也可能带来意想不到的好处，比如这样的adversial training模式可能可以迫使模型学习到一些在语义上有价值的信息：\nRobust Features 下面介绍什么是鲁棒的特征，什么是非鲁棒(Non-Robust)的特征。\n鲁棒的特征就是在扰动下不变的特征，而非鲁棒的特征是在扰动下改变的特征。\n非鲁棒特征足以进行分类 这是一个有趣的研究。对于$(x,y)$，我们进行PGD攻击得到的样本为$(x\u0026rsquo;,y\u0026rsquo;)$,这样的$x\u0026rsquo;$中蕴含和$x$相同的鲁棒特征，但不蕴含相同的非鲁棒特征。然后我们完全从$(x\u0026rsquo;,y\u0026rsquo;)$构成的数据集中训练模型，发现得到的模型在正常的数据点上的效果也不错。\n如何生成鲁棒的数据集 对于一个鲁棒的模型$M$，将他的特征提取函数记为$g$,对于每一个训练的输入$x$，我们通过随机初始化$x_ r$并用梯度递降实现$g(x)=g(x_ r)$, 这样产生的数据集的鲁棒特征是一致的，但是非鲁棒特征是完全不同的。对于这样的${x_ r}$构建的数据集上正常训练的模型的效果也很好。\n也就是说,(Default) Dataset+Robust Machine Learning与Robust Feature Dataset + (Default) Machine Learning，都是可行的获得鲁棒模型的方案。\n但是你仔细想一下不对劲，你在造Robust Feature Dataset的时候已经有一个鲁棒模型$M$了，所以这个研究的真正意义在于在小数据集上通过这种方式获得一个鲁棒的数据集，然后用正常的训练，得到一个鲁棒的模型。也就是小数据集上，plain machine learning在${x}$上不能学到一个鲁棒的模型但在${x_ r}$上可以。直觉上这个很对，因为${x_ r}$上没有任何不是噪音的非鲁棒信息，所以这迫使模型学习到鲁棒特征。\nObfuscated Gradients 前面的PGD，需要知道导数信息，那自然而然想到的防守方法，就是把导数信息给藏起来。\nShattered Gradient Non-differentiable Numeric instability Gradient nonexistent/incorrect 我们的模型可以不可导，或者求出来是非法值,这样PGD就没法攻击了。\nExploding and vanishing gradients Multiple iterations of neural networks Very deep network, long chain rule, so gradients explode/vanish 导数太难求了，这样也没法PGD攻击了。\nStochastic Gradient 随机化的分类器PGD也没法攻击。 但是呢，对应这样的防守策略，也会有对应的进攻策略来crack这些防守。\nBackward pass differentiable approximation： 比如说守方用$f(g(x))$来保护模型，其中$g(x)$比如JPEG压缩是一个不可导的操作，进攻方可以通过:\n$$\\nabla_ x f(g(x))|_ {x=x_ 0}=\\nabla_ x f(x)|_ {x=g(x_ 0)}$$\n来计算梯度。\nAttack randomized classifier 进攻方可以通过多次采样，然后对于梯度求期望来获得估计值。\nProvable Robust Certificates 我们从上面的argument能够发现，这样的工作变成了一种“矛盾游戏”，攻守双方不断的crack对方的方法。没有对于模型鲁棒性的理论保障。\n首先，我们认为在高维空间中很容易找到一个微小扰动就能产生分类器结果改变的点(adversial point)。所以为了fix这样的非鲁棒性，我们引入一个核函数(smooth kernel)和一个点周围的单位球，用这样得到的分布在单位球上不同类的占比来决定输出(取占比最大的)。\n这样的话，微小的扰动可以反应在直方图(histogram)的概率变化上，就没有那么容易的发生改变了。\nGreedy Filling Algorithm 那么为了分析最坏情况下一个微扰($x\\rightarrow x+\\delta$)能够发生的直方图的变化，我们先把之前的定义数学的写出来：\n$f$: base的分类器的函数，是不鲁棒的\n$g$: smoothed的分类器的函数，我们认为它更鲁棒\n$$g(x)=\\int_ {v \\in B_ r(0)}f(v)\\Pr(v)dv$$ 其中$\\Pr(v)$来自于核函数(smooth kernel)的分布。我们想找到一个最大的$||\\delta||$使得$g(x)$与$g(x+\\delta)$一定是相同色的。\n二分类的情况下，也就是$g(x)$对应的输出类在$g(x+\\delta)\u0026gt;1/2$。\n那么这个Greedy Filling究竟是怎么染色的呢：\n首先定义likelihood: $$LL(y)=\\frac{\\Pr(y-x)}{\\Pr(y-x-\\delta)}$$ 然后按照$LL(y)$从大到小依次排序，比如说我们知道$x$点的直方图是$\\Pr(blue)=0.6,\\Pr(red)=0.4$,我们就先把$LL(y)$最大的区域填蓝色，直到蓝色的部分填够了，剩下的部分就填红色。最后我们check一下是不是$g(x+\\delta)\u0026gt;1/2$了。\n值得注意的是，对于Gaussian核函数，等likelihood线是一个直线。 证明也很简单，如下：\n假设高斯分布为：\n$$ p(z; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{|z - \\mu|^2}{2\\sigma^2}\\right) $$\n因此：\n$$ \\frac{p(z; x, \\sigma)}{p(z; x + \\delta, \\sigma)} = \\exp\\left(-\\frac{|z - x|^2}{2\\sigma^2} + \\frac{|z - x - \\delta|^2}{2\\sigma^2}\\right) $$\n对于具有相同likelihood的两个 $z$ ，我们有：\n$$ |z - x - \\delta|^2 - |z - x|^2 = \\langle \\delta, 2z - 2x - \\delta \\rangle = C $$\n因此，对于 $z_ 1, z_ 2$，我们有：\n$$ \\langle \\delta, z_ 1 - z_ 2 \\rangle = 0 $$\n因此，所有具有相同likelihood的点都落在同一条线上。 ■\n","permalink":"http://localhost:1313/blog/posts/ml4/","summary":"This is the fourth article in the Machine Learning Series. It covers classic approaches to Robust Machine Learning, including Adversial Attacks, Adversial Training, Robust Features, Obfuscated Gradients and Provable Robust Certificates.","title":"Machine Learning Series: 4.Robust Machine Learning"},{"content":"Clustering 这里是正统的无监督学习了，我们想要把数据点分组，希望在同一组的数据点有一些共同的性质。\nK-Means 形式化一下，我们被给予：\nn个数据点$(x_ 1,\u0026hellip;,x_ n), x_ i \\in \\mathbb{R}^d$ 想要把他们partition成：\nk个cluster: ${S_ 1,\u0026hellip;,S_ k}$ 我们的目标是最小化在同一个cluster的点距离cluster中心的距离的平方之和。 $$\\arg\\min_ S \\sum_ {i=1}^k \\sum_ {x\\in S_ i} ||x-\\mu_ i||^2$$ 其中$\\mu_ i$是$S_ i$的中心。\n这个问题是NP-Hard的，但是有一些Heuristic的算法，下面介绍Lloyd Algorithm:\nLloyd\u0026rsquo;s Method:\nDecide $k$ Randomly pick $k$ centers Decide membership of all points by assigning them to the nearest center Re-estimate k centers by average of cluster members Repeat 3\u0026amp;4 until convergence 很符合直觉，但是在最坏情况下，这个算法会找到arbitrarily-worse的solution,而且即使是对于seperated gaussian clusters都没有分开的保障。\nLloyd算法是保证终止的，因为每一次都会发生聚类的变化，然后聚类一共只有有限种，所以最终会终止。\nSpectral Graph Clustering 对于有的图，p-范数并不能够做到很好的聚类，比如下面这张:\n这个时候，我们就引入一种思想，对于两个点之间引入一种相似性的衡量，假如说两个点之间的相似性超过一个threshold,那么我们用边把他们连接起来，边权$$w_ {i,j}=\\text{Similarity}(i,j)$$\n理想情况下，我们希望同组之间的边权大，不同组之间的边权小，如果没有边，我们定义$w_ {i,j}=0$.\n举一些例子：\n$\\epsilon$-neighborhood graph (unweighted):\n定义边权重： $$ w_ {i,j} = 1 \\text{ iff } x_ i, x_ j \\text{ are } \\epsilon\\text{-close}. $$ 即如果 $x_ i$ 和 $x_ j$ 的距离小于 $\\epsilon$，则两点之间有一条边。 $k$-nearest neighbor graph:\n如果 $x_ i$ 是 $x_ j$ 的 $k-nn$，或者 $x_ j$ 是 $x_ i$ 的 $k-nn$，则两点之间有一条边。 注意：nearest neighbor 关系不对称。 Fully connected graph:\n定义一个相似性函数（similarity function）来衡量 $x_ i$ 和 $x_ j$ 之间的关系。 下面引入Laplacian Matrix:\nGraph Laplacian\n下面引入$L=D-A$，其中$D$是$diag{deg(v_ i)}$, $A$是邻接矩阵。\n这个Laplacian Matrix有很多好性质，比如：\nTheorem\nGiven: $G$ 是一个无向图，具有非负权重。\n零特征值的数量： $L$的零特征值的数量等于图 $G$ 中的连通支的数量。 记连通分量为 $A_ 1, A_ 2, \\dots, A_ k$。 零特征值的性质： 零特征值对应的特征向量由连通分量的指示向量 $I_ {A_ 1}, I_ {A_ 2}, \\dots, I_ {A_ k}$ 张成。 证明如下:\n$L$是半正定的 对于任意的$v\\in \\mathbb{R}^n$ $$v^T L v=v^T D v-v^T A v$$ $$=\\sum_ {i=1}^n d_ i v_ i^2-\\sum_ {i,j}v_ i v_ j w_ {i,j}$$ $$=\\frac{1}{2}(\\sum_ {i=1}^n d_ i v_ i^2-2\\sum_ {i,j}v_ i v_ j w_ {i,j}+\\sum_ {j=1}^n d_ j v_ j^2)$$ $$=\\frac{1}{2}\\sum_ {ij} w_ {i j} (v_ i-v_ j)^2\\geq 0$$\n记$L$的特征值: $0=\\lambda_ 1\\leq \\lambda_ 2 \\leq \u0026hellip;\\leq \\lambda_ n$\n$\\lambda_ 1=0$ 这个原因是$$\\sum_ j w_ {i j}=d_ i$$ 所以我们可以让$v=(1,\u0026hellip;,1)^T \\in \\mathbb{R}^n$,就有$v^T L v=0$\n连通图只有一个零特征值 利用反证法，考虑$\\sum_ {ij} w_ {i j} (v_ i-v_ j)^2$，除了$v$为全1之外，假设还有一个$v$对应的特征值也是0。这个特征向量一定存在两个分量:$v_ i\\neq v_ j$, 而且之间有路径连接: $w_ 1,\u0026hellip;,w_ k\u0026gt;0$, 这条路径就会对$\\sum_ {ij} w_ {i j} (v_ i-v_ j)^2$贡献非负值，而我们知道这个取0意味着所有的$w_ {i j} (v_ i-v_ j)^2$都取0，所以矛盾。\nk联通支对应k个0特征向量 我们可以对于顶点进行适当的排序，得到 $$ L=\\begin{pmatrix}L_ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; L_ 2 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; L_ n \\ \\end{pmatrix}$$ 于是我们有$I_ {A_ i}$这一系列的0特征值对应的特征向量，共k个。$\\blacksquare$\n接下来就该讲讲这个Laplacian Matrix怎么在Clustering里用的了:\n使用 Laplacian 找到 $k$ 个聚类的方法： 计算拉普拉斯矩阵 $L$ 的前 $k$ 个特征向量：\n特征向量：$\\mu_ 1, \\mu_ 2, \\dots, \\mu_ k$。 对应的特征值接近 0（不一定等于 0）。 构造矩阵 $U$:\n$U \\in \\mathbb{R}^{n \\times k}$，以 $\\mu_ 1, \\mu_ 2, \\dots, \\mu_ k$ 作为列。 构造特征向量的行向量：\n对于 $i = 1, 2, \\dots, n$，令： $$ y_ i \\in \\mathbb{R}^k \\text{ 是 } U \\text{ 的第 } i \\text{ 行向量}. $$ 构造点集 ${y_ i}_ {i=1, \\dots, n}$。 运行 $k$-means 聚类：\n在点集 ${y_ i}_ {i=1, \\dots, n}$ 上运行 $k$-means 算法，得到 $k$ 个聚类 $C_ 1, C_ 2, \\dots, C_ k$ 输出最终的聚类结果：\n定义每个聚类的集合： $$ A_ i = {j | y_ j \\in C_ i}, \\quad i = 1, 2, \\dots, k. $$ 输出聚类结果 $A_ 1, A_ 2, \\dots, A_ k$。 看一个理想情况的例子:\n这就很漂亮，但是现实情况下一般图是联通的，所以没那么好的事～\n最后解决一个technical problem,我们怎么去找最小的特征值、特征向量对呢？其实很简单，比如说$A$是半正定的，我们取 $$B = A - \\lambda _ {\\max} I $$ 对$B$做power-method，然后加$\\lambda_ {\\max}$就ok了。\nWhy This Makes Sense 考虑RatioCut问题: $$ratiocut(A_ 1,\u0026hellip;,A_ k)=\\sum_ {i=1}^k \\frac{W(A_ i, \\overline{A_ i})}{|A_ i|}$$ 其中$W(A_ i, \\overline{A_ i})$代表这$A_ i, \\overline{A_ i}$之间所有边权重之和。\n这个问题是NP-Hard的。\n下面先考虑$k=2$的情况:\n假设 $G$ 是连通图,那么全 1 向量是最小的特征向量。\n对于满足下面这种形式的$v^A\\in \\mathbb{R}^n$： $$ v_ i^A = \\begin{cases} \\sqrt{\\frac{|\\overline{A}|}{|A|}}, \u0026amp; \\text{if } i \\in A \\ -\\sqrt{\\frac{|A|}{|\\overline{A}|}}, \u0026amp; \\text{o/w}. \\end{cases} $$\n我们有\n$$ (v^A)^T L v^A =\\frac{1}{2}\\sum_ {i,j} w_ {i,j} (v^A_ i-v^A_ j)^2 $$ $$=\\frac{1}{2}\\sum_ {i\\in A, j\\in \\overline{A}}w_ {i,j}\\left(\\sqrt{\\frac{|\\overline{A}|}{|A|}}-\\sqrt{\\frac{|A|}{|\\overline{A}|}}\\right)^2+\\frac{1}{2}\\sum_ {i\\in \\overline{A}, j\\in A}w_ {i,j}\\left(\\sqrt{\\frac{|\\overline{A}|}{|A|}}-\\sqrt{\\frac{|A|}{|\\overline{A}|}}\\right)^2$$ $$=cut(A,\\overline{A})(\\frac{|A|}{|\\overline{A}|}+\\frac{|\\overline{A}|}{|A|}+2)$$ $$=cut(A,\\overline{A})(\\frac{|A|+|\\overline{A}|}{|\\overline{A}|}+\\frac{|\\overline{A}|+|A|}{|A|})$$ $$=|V|\\cdot ratiocut(A,\\overline{A})$$\n也就是说对于这种形式的$v^A$，我们minimize RatioCut等价于找到一个好的$A \\subset V$: $$\\min_ A ratiocut(A,\\overline{A})=\\min_ {A \\subset V} (v^A)^T L v^A$$ 这个当然还是NP-Hard的，但是我们可以relax一下： $$\\min_ v v^T L v \\quad \\text{s.t. } \\langle v, I_ V \\rangle=0, ||v||=\\sqrt{n}$$ 这不就是求第二小的特征向量吗？\n然后通过这个形式有差异的$v$中还原$A$:\nnaive way: $i \\in A \\text{ iff } v_ i\u0026gt;\\alpha$，其中$\\alpha$是某个threshold。 Run 2-means on ${v_ i}$: 诶，我们仔细一看，**这种方式Run 2-mean是和在$(1,v_ i)$上跑2-means是完全一样的，再一想，这不就是我们 Spectral Graph Clustering的算法吗？**至此，豁然开朗。\n但是我们还是没完呢，这个只是$k=2$的情况，对于$k\u0026gt;2$呢？\n定义 $h_ {ij}$： $$ h_ {ij} = \\begin{cases} \\frac{1}{\\sqrt{|A_ j|}}, \u0026amp; \\text{if } v_ i \\in A_ j \\ 0, \u0026amp; \\text{o/w}. \\end{cases} $$\n给定 $H \\in \\mathbb{R}^{n \\times k}$，使得 $H^T H = I$。\n我们可以看到(设 $h_ i$ 是 $(h_ {1i}, \\cdots, h_ {ni}) \\in \\mathbb{R}^n$ 的向量)：\n回忆：$$v^T L v = \\frac{1}{2} \\sum_ {ij} w_ {ij}(v_ i - v_ j)^2$$\n$$h_ i^T L h_ i = \\frac{\\text{cut}(A_ i, A_ i^c)}{|A_ i|} = (H^T L H)_ {ii}$$ 所以： $$ \\text{ratiocut}(A_ 1, \\cdots, A_ k) = \\sum_ {i=1}^k h_ i^T L h_ i = \\sum_ {i=1}^k (H^T L H)_ {ii} = \\text{Tr}(H^T L H) $$ 也就是说对于满足上述$h$的条件的矩阵$H$,我们的优化目标是： $$ \\text{min } \\text{Tr}(H^T L H) \\quad \\text{s.t. } H^T H = I, h_ {ij} \\text{ see above} $$\n与$k=2$情况类似，我们relax对于$H$的限制，得到: $$ \\text{min } \\text{Tr}(H^T L H) \\quad \\text{s.t. } H^T H = I $$\n这里有个结论，就是说这个$H$对应的就是$L$最小的$k$个特征向量(受限篇幅与作者的能力（小声），聪明的读者应该不难自证)，然后对$H$的行向量跑k-means就可以还原出聚类${A_ {[1:k]}}$了。所以我们找到Graph Spectral Clustering算法的理论依据了，也就是说我们在解决一个relaxed version的最小化聚类的RatioCut。\nSimCLR 这里讲个Clustering+ Metric Learning的应用，也是袁洋老师ICLR 2024年的崭新工作，证明了SimCLR这种对比学习方法其实就是在similarity graph上折腾了一些操作的spectral clustering。\nWhat is SimCLR 首先讲讲什么是SimCLR。这个是对比学习的一个算法，比如说给一个被查询的样本$q$，同时还有一个正样本$p_ 1$,对应$N-1$个负样本${p_ i}_ {i=2}^N$。\n一个现实的例子是我们对于所有输入的图片生成两个augmented图片，那么以其中一个图片作为查询样本，另一个就是正样本，别的图片augment之后的结果就是负样本。\n这里的$q,p_ 1$可以在pixel space上差距极大，但是我们希望他们在我们学到的semantic space上距离接近。我们的优化目标是InfoNCELoss:\n$$ L(p,q,{p_ i}_ {i=2}^N)=-\\log \\frac{\\exp(-||f(q)-f(p_ 1)||^2)/2\\tau}{\\sum_ {i=1}^N \\exp(-||f(q)-f(p_ i)||^2)/2\\tau} $$\nWhat is the Similarity Graph Here 这里的Similarity Graph的定义对于所有augmented image构成的${X_ i}$集合，$(X_ i,X_ j)$的边权是 $$\\pi_ {i,j}=\\Pr[X_ i,X_ j\\text{ are sampled together}]$$ 有没有一个理想的空间，其中semantic similarity是被自然的捕捉到的呢？答案是有的，这里引入Reproducing Kernel Hilbert Space.\nReproducing Kernel Hilbert Space (RKHS) 给定两个在$Z$空间的物品: $Z_ 1,Z_ 2\\in Z$, 考虑$\\phi: Z\\rightarrow H$, 这里$H$的维度远大于$Z$.\n$$k(Z_ i,Z_ j)=\\langle \\phi(Z_ i),\\phi(Z_ j)\\rangle_ H$$\n这里的$H$就是Hilbert space,而$k$Rreproducing Kernel。\n因为我们关心的只是样本之间的相似性，所以我们不用真的知道或者能算$\\phi(Z_ i)$,我们只需要算样本之间的$k$就ok了。\nMarkov Random Fields (MRF) 原论文里是这么说的\nDue to the large size of $\\pi$ and in practice $\\pi$ is usually formed by using positive samples sampling and hard to explicitly construct, directly comparing $K_ Z$ and $\\pi$ can be difficult, so we treat them as MRFs and compare the induced probability distributions on subgraphs instead.\nmotivation很清晰，那么我们具体怎么采样呢？我们想要sample出的unweighted子图我们设为$W$(也就是说$W_ {ij}\\in{0,1}$)\n$$P(W;\\pi)\\propto \\Omega(W)\\cdot \\prod_ {{i,j}\\in [n]^2}\\pi_ {i,j}^{W_ {i,j}} $$ 怎么解读呢？ $$s(W,\\pi)=\\prod_ {{i,j}\\in [n]^2}\\pi_ {i,j}^{W_ {i,j}}$$ 对于可能的$W$，我们根据他的score function来采样，然后我们看看采样出来的$W$是不是正确的形状，比如\n$$\\Omega(w)=\\prod_ i \\mathbb{1}[\\sum_ jW_ {ij}=1]$$ 这个被称为Unitary out-degree filter，也就是说每一个顶点要求有且仅有一条出边。\n接下来考虑$P(W;K_ z)$: $$P(W;K_ z)\\propto \\Omega(W)\\cdot \\prod_ {{i,j}\\in [n]^2}k(Z_ i,Z_ j)^{W_ {i,j}} $$\nInfoNCE and Spectral Clustering 好了，概念搭的差不多了，落地我们通过比较$W_ X,W_ Z$的差异来反应我们理想中$\\pi,K_ Z$的差值。这一差值可以通过Cross Entropy衡量： $$H_ \\pi^k(Z)=-\\mathbb{E}_ {W_ X\\sim P[\\cdot;\\pi]}\\log P[W_ Z=W_ X;K_ z]$$ 接下来如果能证明\nInfoNCE和$H_ \\pi^k(Z)$等价 $H_ \\pi^k(Z)$和Spectral Clustering等价 我们的任务就完成了。\nInfoNCE和$H_ \\pi^k(Z)$等价 先从直觉上理解下这件事情: $$H_ \\pi^k(Z)=-\\mathbb{E}_ {W_ X\\sim P[\\cdot;\\pi]}\\log P[W_ Z=W_ X;K_ z]$$\n$W_ X\\sim P[\\cdot;\\pi]$: 我们从$\\pi$上取样，代表着Data Augmentation Step 接下来用一个原论文中的重要结论: 对于Unitary out-deg $\\Omega(w)$, $$W_ i\\sim M(1, \\frac{\\pi_ i}{\\sum_ j \\pi_ {i,j}})$$ 也就是我们按照$\\frac{\\pi_ i}{\\sum_ j \\pi_ {i,j}}$取样一个one-hot vector。 因为每一行是独立的，所以我们有: $$H_ \\pi^k(Z)=-\\sum_ i\\mathbb{E}_ {W_ {X,i}}\\log P[W_ {Z,i}=W_ {X,i};K_ z]$$ 这里$W_ {X,i}$代表$W_ X$的第$i$行(one-hot 向量)。\n接下来想一下InfoNCE在说什么事情：\n$$InfoNCE=-\\sum_ {i=1}^n\\log \\frac{\\exp(-||f(X_ i)-f(X_ i\u0026rsquo;)||^2)/2\\tau}{\\sum_ {j=1}^N \\exp(-||f(X_ i)-f(X_ j))||^2)/2\\tau}$$\n如果我们定义 $$Q_ i=\\frac{K_ {Z,i}}{||K_ {Z,i}||_ 1} $$ 作为$P(\\cdot;K_ Z)$的分布\n那么 $$\\text{InfoNCE}=-\\sum_ {i=1}^n \\sum_ {i\u0026rsquo;=1}^n \\Pr[W_ {X,i,i\u0026rsquo;}=1] \\log Q_ {i,i\u0026rsquo;}=-\\sum_ i\\mathbb{E}_ {W_ {X,i}}\\log P[W_ {Z,i}=W_ {X,i};K_ z]=H_ \\pi^k(Z)$$\n$H_ \\pi^k(Z)$和Spectral Clustering等价 $$H_ \\pi^k(Z)=-\\mathbb{E}_ {W_ X\\sim P[\\cdot;\\pi]}\\log P[W_ Z=W_ X;K_ z]$$ 其中 $$P[W_ Z=W_ X;K_ z] \\propto \\Omega(W_ X) \\prod_ {{i,j}\\in [n]^2} K_ {Z_ {i,j}}^{W_ {X_ {i,j}}}$$\n然后我们就可以显式的表达概率 $$R(Z)=\\sum_ {W}\\Omega(W)\\cdot \\prod_ {{i,j}\\in [n]^2}K_ {Z_ {i,j}}^{W_ {i,j}}$$\n所以说 $$\\log P[W_ Z=W_ X;K_ z]=\\sum_ {i,j}W_ {X_ {i,j}}\\log K_ {Z_ {i,j}}+\\log \\Omega(W_ X)-\\log R(Z)$$\n这里对于fixed $W_ X$，$\\log \\Omega(W_ X)$是常数，所以: $$ \\arg\\min_ Z H_ \\pi^k(Z) = \\arg\\min_ Z -\\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\Bigg[\\sum_ {(i,j) \\in [n]^2} W_ {X,ij} \\log k(Z_ i, Z_ j) - \\log R(Z) \\Bigg] $$ $$ = \\arg\\min_ Z -\\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\sum_ {(i,j) \\in [n]^2} W_ {X,ij} \\log k(Z_ i, Z_ j) + \\log R(Z) $$ 回忆下，$k$ 是 Gaussian 分布来的：\n$$\\log k(Z_ i, Z_ j) = -\\frac{||Z_ i - Z_ j||^2}{2\\tau} $$\n所以原式 $$ = \\arg\\min_ Z \\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\frac{1}{2\\tau} \\sum_ {(i,j) \\in [n]^2} W_ {X,ij} |Z_ i - Z_ j|^2 + \\log R(Z) $$\n$$ = \\arg\\min_ Z \\mathbb{E}_ {W_ X \\sim P(\\cdot; \\pi)} \\frac{1}{\\tau} \\operatorname{tr}(Z^T L(W_ X) Z) + \\log R(Z) $$\n$$ = \\arg\\min_ Z \\frac{1}{\\tau} \\operatorname{tr}(Z^T L^* Z) + \\log R(Z)\\quad\\blacksquare $$ 真长啊。\nt-SNE 最后讲一个Data Visualization/ Dimension Reduction的算法，首先回忆一下NCA:\n原空间上两个点$(x_ i,x_ j)$的similarity是这么定义的: $$ p_ {j|i} = \\frac{\\exp\\left(-\\frac{|x_ i - x_ j|_ 2^2}{2\\sigma_ i^2}\\right)}{\\sum_ {k \\neq i} \\exp\\left(-\\frac{|x_ i - x_ k|_ 2^2}{2\\sigma_ i^2}\\right)} $$\n我们做映射后$ x_ i \\to y_ i = f(x_ i) $: $$ q_ {j|i} = \\frac{\\exp\\left(-|y_ i - y_ j|_ 2^2\\right)}{\\sum_ {k \\neq i} \\exp\\left(-|y_ i - y_ k|_ 2^2\\right)} $$\n我们希望$p,q$能对应的上。\n在训练中，用KL-Divergence做损失函数： $$ L = \\sum_ i \\text{KL}(P_ i | Q_ i) = \\sum_ i \\sum_ j p_ {j|i} \\log \\frac{p_ {j|i}}{q_ {j|i}} $$\n细心的读者发现和无监督学习(I)里讲的NCA的优化目标好像略有区别(差个log)，但无伤大雅，我们不去管他。怎么去选$\\sigma_ i$呢?\nSNE 用户选择一个Perplexity，这个直觉的理解是对于有效邻居数量的 smooth measure。 $$ \\text{Perp}(P_ i) = 2^{H(P_ i)} $$ $$ H(P_ i) = -\\sum_ j p_ {j|i} \\log p_ {j|i} $$ 接下来我们对$\\sigma_ i$做二分查找找到合适的$\\sigma_ i$值，这就叫SNE algorithm。\nt-SNE SNE很经典，但是有如下两个缺点的\n要优化很多个loss，你看loss其实是对i和j求了两遍和，相当于每个pair贡献了一个loss，或者说有$n$个分布要去学\n拥挤问题(crowding problem)\n比如说考虑一个${0,1}^d$的grid,对于$r=10$的情况有$2^{10}$个可以放的位置，但是投影到$r=2$上就只有$2^2$个了，全挤到一块分不开了。\n为了解决第一个问题，t-SNE的做法是，把所有的距离放一起做运算，捏成一个概率分布，优化一个 single概率分布的loss: $$p_ {i,j}=\\frac{p_ {j|i}+p_ {i|j}}{2n}$$ 这样设置是为了 $$\\sum_ {i j} p_ {i j}=1$$\n为什么要变成一个distribution呢，因为计算梯度更容易、更快。\n为了解决第二个问题，t-SNE的想法是换一个更heavy-tail的distribution，这样保持相对距离，绝对距离改变，就还能分得开。\n也就是:\n高维中近距离的点，在低维中距离要变得更小 高维中远距离的点，在低维中距离要变得更大 这里用student t-distribution就很合适,因为更heavy-tail:\n距离由$\\frac{1}{1+||y_ i-y_ j||^2}$刻画 $$p_ {i j}=\\frac{(1+||y_ i-y_ j||^2)^{-1}}{\\sum_ {k\\neq l}(1+||y_ l-y_ k||^2)^{-1}}$$\n没了，感觉这一部分大量参考了这篇文献。\nA side note: If you are interested in clustering, you can also check out this website, which contains a survey of clustering algorithms(the pdf file link is in the website). It\u0026rsquo;s a project done by me and my friend Yiming Liu.\n","permalink":"http://localhost:1313/blog/posts/ml3/","summary":"This is the third article in the Machine Learning Series. It covers the second part of unsupervised learning, including topics like Clustering, Spectral Graph Clustering, SimCLR, SNE and t-SNE.","title":"Machine Learning Series: 3.Unsupervised Learning(II)"},{"content":"Dimension Reduction 对于输入的一系列点: $x_ 1,\u0026hellip;,x_ n\\in \\mathbb{R}^d$,我们想要在一个更低维度的空间研究他们，一种常见的方法就是Random Projection。\n这里我们应用JL-Lemma的结论，也就是说如果我们关心的是Pairwise Distance的话，我们可以通过一个随机矩阵$A\\in \\mathbb{R}^{d \\times m}\\sim \\frac{1}{\\sqrt{d}}\\mathcal{N}(0,1)$来构造一个映射：$f(x)=Ax$，$f: \\mathbb{R}^d\\rightarrow \\mathbb{R}^m$，对于$$m=\\Omega(\\frac{\\log n}{\\epsilon^2})$$ 我们可以保障pairwise distance的误差在$\\epsilon$之内，即 $$ (1 - \\epsilon) |v^i - v^j|_ 2 \\leq |f(v^i) - f(v^j)|_ 2 \\leq (1 + \\epsilon) |v^i - v^j|_ 2. $$\nPCA 但有的时候我们不关心pairwise distance, 这里我们将另外一种非常流行的数据降维方法: 主成分分析。\nKeep Most Variance 主成分分析的第一种解读方式是我们想要找到一个方向$v$，使得它能够保留和数据点之间最大的方差。首先得定义下方差： $$\\mathbb{E}_ {x_ i}[\\langle x_ i, v\\rangle^2]$$\n那么对于n个数据点$x\\in \\mathbb{R}^d$构成的矩阵$X\\in \\mathbb{R}^{n \\times d}$(每行对应一个数据点): $$\\frac{1}{n}\\sum_ {i=1}^n(v^T x_ i)^2=\\frac{1}{n}v^T X X^T v$$\n这里的$X^T v$获得更多维度$X^T V\\in \\mathbb{R}^{n \\times k}$就是数据降维后的结果。\nMinimize Reconstruction Error 由于勾股定理，这里还有另一种理解方法：\n就是原来的$||x||$长度不变，你在最大化投影距离: $\\max||v^T x \\cdot v||$，那么由于勾股定理，你同时就在最小化$||x-(v^T x)\\cdot v||$\n也就是说当我们找到了$v_ 1,\u0026hellip;,v_ k$之后，我们将原来的数据点投影到这个线性生成的子空间中，没有损失太多的信息。\n所以我们想要的就是: $$\\max_ v v^T X X^T v\\quad s.t. ||v||=1.$$ 学过线性代数的我们一看就知道我们要找的是最大的特征向量啊，那么怎么去算呢？\nPower Method 取$b_ 0$为$\\mathbb{R}^d$空间上的一个随机向量，那么因为实对称矩阵能够特征值分解，所以$$b_ 0=\\sum_ i \\alpha_ i v_ i$$ 其中$v_ i$代表$X X^T$的特征向量。\n然后我们用这种方法更新$b$:\n$$b_ {t+1}=\\frac{(X X^T)b_ t}{||X X^T b_ t||}$$ 这个时候呢，因为特征值的定义，就会发生： $$b_ t=\\sum_ i \\lambda_ i^t \\alpha_ i v_ i$$ 不同分量之间的magnitude以指数的形式被拉开，在$t$足够大的时候，我们就可以认为: $b_ t=v_ 1$了。\n当我们知道了$v_ 1$之后，我们可以通过$b_ 0=b_ 0-\\langle v_ 1, b_ 0 \\rangle \\cdot v_ 1$来获得剔除了$v_ 1$的向量，然后就可以算出$v_ 2$，依次类推。\n学过线性代数的我们知道$X=U \\Sigma V^T$，所以其实我们找的$X X^T$的特征值就是$U$的前k列。\nNearest Neighbor 接下来继续讲一个非常有趣的问题，这个问题是Supervised Learning的问题，但是和后面Clustering之类的关系挺密切，所以也放在这一章讲了。\nk-NN Algorithm 这里我们考虑一个分类问题，我们的假设是我的label由我的neighbor的label决定，这是一个non-parametric的方法，没有模型参数，取决于数据集。\n一个empirical的观察是随着k越来越大，模型的分界线也变得越来越平滑。\n现在一个核心问题是：我怎么找到一个query point附近的邻居呢？\nLSH Algorithm 准确的neighbor search是困难的，而且我们也没那么关心，我们想要的是一个Approximate的算法。\n这里我们考虑刚才这个问题的等价问题：\nNearest Neighbor Problem(原问题):\nFind closest $p\\in P$ (L2 norm)\nR-Near Neighbor Problem(转化的问题):\nFind $p\\in P$ s.t. $||p-q||\u0026lt;R $\n这两个问题的等价性并不难证明:\n$\\Rightarrow:$ 找到的nearest neighbor,算一下距离是否$\u0026lt;R $即可。\n$\\Leftarrow:$ 从$R=1,2,4,\u0026hellip; $上试，只需要$\\log$次查询也能找到最近邻。\n接下来，我们先引入一个定义:\nRandomized c-approximate R-near neighbor, or $(c, R)$-NN:\n给定一组点$P$ 在 $\\mathbb{R}^d$ 中，以及参数$R \u0026gt; 0 ,\\delta \u0026gt; 0$ 构造一个数据结构，使得对于任意查询点 $q$，如果 $P$ 中存在一个 $R$-近邻，则以概率 $ 1 - \\delta $ 返回 $ q $ 在 $ P $ 中的某个 $cR$-近邻。 注意到这个$\\delta$可以通过重复跑$t$次这个算法降低到$\\delta^t$。\n好的，接下来进入正题，我们想要构造这样的数据结构，我们需要借助于local sensitive-hashing(LSH) family:\nLocality-sensitive hashing (LSH)\n一个Hash Family $\\mathcal{H}$ 被称为 $(R, cR, P_ 1, P_ 2)$-sensitive，如果对于任意两个点 $p, q \\in \\mathbb{R}^d$ 满足以下条件： 如果 $|p - q| \\leq R$，则：$$\\Pr_ {h \\sim \\mathcal{H}}[h(q) = h(p)] \\geq P_ 1$$ 如果 $|p - q| \\geq cR$，则：$$\\Pr_ {h \\sim \\mathcal{H}}[h(q) = h(p)] \\leq P_ 2$$ 为了使 LSH 有用，必须满足 $P_ 1 \u0026gt; P_ 2$。\n举个LSH的例子：\n考虑所有数据点都$\\in {0,1}^d$,distance由Hamming Distance决定。\n我们构造这样一组hash function: $$h_ i(p)=p_ i$$ 也就是返回查询点的第$i$个坐标的值，这个Hash Family一共有$d$个hash function。\n接下来，我们验证下他是一个LSH:\n因为我们是随机去$h\\sim \\mathcal{H}$，所以说\n$$Pr[h(p)=h(q)]=\\frac{ ||p-q||_ 1 }{d} $$\n所以$P_ 1=1-\\frac{R}{d}\u0026gt;1-\\frac{cR}{d}=P_ 2$.\n直觉上理解，一个最理想的LSH想干的事情是把相邻的点都映射到同一个铜里，而距离远的点都映射到相异的桶里，理想情况$P_ 1=1,P_ 2=0$，但无法达到。\n接下来讲一下具体的算法:\nTable Construction 构造一个 $L \\times k$ 的矩阵，其中每个元素是一个哈希函数 $h_ {i,j}$，该函数从 LSH Family中随机选择。 构造 $L$ 个哈希表，第$i$个哈希表对应矩阵的第 $i$ 行包含 $k$ 个元素： $$ g_ i = (h_ {i,1}, h_ {i,2}, \\dots, h_ {i,k}) $$ Construct the Database 对于每一个表$i \\in [L]$：\n对于每一个点 $p_ j, j\\in [n]$:\n计算 $g_ i(p_ j)\\in {0,1}^k$（这里不妨假设每一个hash function的输出都是0或1），放入对应桶中\n这里需要的内存是$O(n \\cdot L)$.\nQuery Algorithm for Point $q$ 过程： 对于 $j = 1, 2, \\dots, L$： 计算 $g_ j(q)$ 在第 $j$ 个哈希表中的值，并找到对应桶中的所有点。 对于桶中的每个点，计算它到查询点 $q$ 的距离。 如果它是 $cR$-近邻，则报告该点。 当我们已经找到超过 $L\u0026rsquo;=2L+1$ 个点时，停止搜索。 这个算法为什么对？因为有如下定理：\nTheorem:\n如果存在 $p^* \\in B(q, R)$，则我们以至少 $ \\frac{1}{2} - \\frac{1}{e} $ 的概率找到一个 $q$的$cR$-近邻点。\n有了这个定理，我们可以通过 $\\mathcal{O}\\left(\\frac{1}{\\delta}\\right)$ 次重复算法（使用不同的哈希表），将成功概率提升到 $1 - \\delta$。那就ok了，接下来我们证明这个定理：\n定义： $$ \\rho = \\frac{\\log \\frac{1}{P_ 1}}{\\log \\frac{1}{P_ 2}} $$ 设置： $$ k = \\log_ {1/P_ 2}(n), \\quad L = n^\\rho $$\n定义： $$ P\u0026rsquo; = P - B(q, cR) $$\n如果存在 $p^* \\in P$ 是 $R$-近邻，则对于 $p\u0026rsquo; \\in P\u0026rsquo;$，其满足 $g_ i(p\u0026rsquo;) = g_ i(q)$ 的概率最多为： $$ P_ 2^k = \\frac{1}{n} $$ 这对每个 $g_ i$ 都成立。 所以说 $$\\mathbb{E}[ \\text{number of } g_ i \\text{ wrong points in }P\u0026rsquo;]\\leq 1$$ 所以$$\\mathbb{E}[\\text{total wrong points}]\\leq L$$ 接下来用Markov不等式，有: $$\\Pr[\\text{total wrong points}\\geq 2L]\\leq \\frac{1}{2}$$\n接下来，我们再去考虑这个$p*$:\n$$\\Pr[g_ i(p*) = g_ i(q)] \\geq P_ 1^k = P_ 1^{\\log_ {1/P_ 2}(n)} = n^{-\\frac{\\log(1/P_ 1)}{\\log(1/P_ 2)}} = n^{-\\rho}$$\n因为 $L = n^\\rho$，我们有： $$ \\Pr[g_ i(p^*) \\neq g_ i(q), \\forall i] \\leq (1 - n^{-\\rho})^{n^\\rho} \\leq \\frac{1}{e} $$\n所以： $$ \\Pr[\\exists i, g_ i(p^*) = g_ i(q)] \\geq 1 - \\frac{1}{e} $$\n因此，这两个事件同时成立的概率： $$ \\geq\\frac{1}{2} - \\frac{1}{e}. $$\nLSH Example 在L2中的LSH Family示例： $$h_ {r,b}=\\lfloor \\frac{\\langle r,x \\rangle+b}{w}\\rfloor$$\n$w$ 是单位长度（unit length），为一个超参数。 $r \\in \\mathbb{R}^d$，从高斯分布中采样。 $b \\sim \\text{unif}[0, w)$，从均匀分布中采样。 给定两点 $p, q$:\n$\\langle r, p \\rangle$，$\\langle r, q \\rangle$，以及 $\\langle r, p - q \\rangle$ 都服从高斯分布。 一种直观的理解方式是：\n将所有向量投影到一条直线上，并将直线分成不同的桶。 如果 $\\langle r, p \\rangle$ 和 $\\langle r, q \\rangle$ 落在相同的桶中，则两点会得到相同的哈希值。 Simple Case: $w = 1$ 当 $|p - q|_ 2 = 1$ 时，在同一个桶中的概率由以下公式给出： $$ P(c) = \\Pr[h_ {r,b}(p) = h_ {r,b}(q)] = 2 \\int_ 0^1 f_ p(t)(1 - t) , dt $$ 解释：\n当 $\\langle r, p - q \\rangle = t$ 时，$p, q$ 在一个桶中的概率为 $1 - t$。\n$t$ 的取值在 $[0, 1]$ 上，且 $[-1, 0]$ 和 $[0, 1]$ 对称，因此积分结果乘以 $2$。\n$\\langle r, p - q \\rangle$ 服从高斯分布，所以： $$ \\Pr[\\langle r, p - q \\rangle = t] = f_ p(t) $$\nScaled Case: General $w$ and $|p - q|_ 2$ 定义 $c = |p - q|_ 2$，在同一桶中的概率为： $$ P(c) = \\Pr[h_ {r,b}(p) = h_ {r,b}(q)] = \\int_ 0^w \\frac{2}{c} f_ p\\left(\\frac{t}{c}\\right) \\left(1 - \\frac{t}{w}\\right) , dt $$ 解释：\n关于 $f_ p$ 的缩放：\n$f_ p$ 是高斯密度分布，$\\frac{1}{c} f_ p\\left(\\frac{t}{c}\\right)$ 来源于缩放。 高斯分布归一化： $$ \\int_ {-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\left(\\frac{x}{c}\\right)^2}{2}\\right) , dx = \\int_ {-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) c , du = c $$\n值得注意的是，$P(c)$ 随 $c$ 递减（给定 $w$ 的情况下）。\nMetric Learning 好了，我们现在有获得k-近邻的算法了，但是有些时候在原来的空间上选邻居并不是最佳选择。我们可以通过神经网络找到一个更好的feature space，在这个空间上做k近邻。\n也就是说我们想要找到一个原空间到feature space的映射：$f: \\mathbb{R}^d\\rightarrow \\mathbb{R}^k$。那么，什么是一个好的映射呢？我们希望在这个feature space中相同标签的点有相近的距离，反之亦然。\n但是原来的kNN是没有gradient信息的，那么又到了这个重要的思想: Make Hard things Soft,下面介绍NCA算法。\n$$p_ {i,j}=\\frac{\\exp(-||f(x_ i)-f(x_ j)||^2)}{\\sum_ {k\\neq i}\\exp(-||f(x_ i)-f(x_ k)||^2)}$$ $$p_ {i,i}=0$$\n定义 $C_ i = {j ,|, c_ i = c_ j}$其中 $c_ i$表示数据点 $i$ 的类别。\n定义 $P_ i$ 为类别内的概率和： $$ P_ i = \\sum_ {j \\in C_ i} p_ {ij} $$\n定义目标函数 $L(A)$ 为所有类别内概率和的总和： $$ L(A) = \\sum_ i \\sum_ {j \\in C_ i} p_ {ij} = \\sum_ i P_ i $$\nLMNN: triplet loss 和NCA并列的还有另一种算法，也是为了学习一个好的feature space。\nRanking Loss定义： $$ L_ {\\text{rank}} = \\max(0, |f(x) - f(x^+)|_ 2 - |f(x) - f(x^-)|_ 2 + r) $$\n$r$：Margin（边界）。 比较正样本 $x^+$ 和负样本 $x^-$ 的距离。 如何选择 $x^+$ 和 $x^-$？选择接近最坏的样本（almost-worst cases), 为什么需要这样选择？Loss 可能很快趋于 $0$，导致训练无效。\n","permalink":"http://localhost:1313/blog/posts/ml2/","summary":"This is the second article in the Machine Learning Series. It covers the first part of unsupervised learning, including topics like Dimension Reduction, PCA, k-NN, LSH and Metric Learning.","title":"Machine Learning Series: 2.Unsupervised Learning(I)"},{"content":"Part B: Neural Networks \u0026amp; LLMs Info. This is the continuation from Part A.\nCredits Again, huge thanks to Prof. Tianxing He for his amazing lectures and brilliant teaching.\nThe PDF Version You can also download the pdf version of the notes here.\n","permalink":"http://localhost:1313/blog/posts/nlp2/","summary":"This is the second part of the Natural Language Processing Series. It covers modern approaches in natural language processing, including RNNs, VAE-LMs, Transformer, BERT, GPT, GAN-LMs, In-Context Learning, CoT, RLHF, DPO, etc.","title":"Natural Language Processing: Part B. Modern Approaches"},{"content":"0.饭后甜品，你不能指望跟正餐一起 Everything should be made as simple as possible, but not simpler. Albert Einstein.\n记得高三的时候写过一篇作文，文章的立意大概是 “整顿旗鼓再出发” 。是啊，多少次，我们奋力狂奔，迎接着狂风骤雨的敲打，却不愿意放慢脚步，从对未来不确定性的焦虑之中跳脱出来，看看自己的来时路，看看昨日之我、今日之我。在忙忙叨叨之中时光便流逝掉了，有时不妨做点 reflection,整理一下杂乱的思绪和没想明白的问题。\n另一个落在实处的动机是我发现我学东西有个特点，就是忘东西很快。如果不留下点东西呢，会忘，然后忘了没有笔记又很难捡起来。 所以我想，为什么不在自己对这个领域的内容认识最深刻的时候留下点记忆，寄希望于未来的自己或者或许对机器学习有兴趣的读者能够通过今日的一篇文章了解一些今日之我所思所想的一些内容呢，于是就诞生了这篇文章。\n但这件事怎么看都还是很呆，都考完了，然后在写的过程中肯定又能学到点东西。一位朋友跟我说 “饭后甜品，你不能指望跟正餐一起” ，于是本着一个品味甜品的食客的心态，我决定将这篇文章尽量写的轻量化一点、故事性强一点，穿起一个思考的主线。\n1. Optimization 优化问题自然而然地出现在许多应用领域中。无论人们做什么，在某些时候，他们都会产生一种想要以最佳方式组织事物的渴望。这种意图，当被转换成数学形式时，就会变成某种类型的优化问题。下面介绍几种优化算法，包括：Gradient Descent, Stochastic Gradient Descent, SVRG, Mirror Desent, Linear Coupling.\n1.1 L-Smooth \u0026amp; Convex 在优化函数的时候，我们往往需要一些有关函数性质的保障，才能够确保他有好的收敛率。\nL-smooth 以下三条等价：\n$f(x) \\leq f(x_0) + \\langle \\nabla f(x_0), x-x_0 \\rangle + \\frac{L}{2}||x-x_0||^2$\n$|\\lambda_{\\nabla^2 f(x)}| \\leq L$\n$||\\nabla f(x) - \\nabla f(y)|| \\leq L||x-y||$\n注意到L-smooth其实告诉我们的是梯度变化不会太快，另外一个有趣的看法是：\nUpper Bound: $f(x) \\leq f(x_0) + \\langle \\nabla f(x_0), x-x_0 \\rangle + \\frac{L}{2}||x-x_0||^2$ Lower Bound: $f(x) \\geq f(x_0) + \\langle \\nabla f(x_0), x-x_0 \\rangle - \\frac{L}{2}||x-x_0||^2$ 也就是说给定一个点$f(x_0)$的零阶和一阶信息，我们就可以获得别的点的函数值的一个二次型的上下界。 Convex 以下四条等价：\n$ f(x) \\geq f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle $ $ f(x) \\leq f(x_0) + \\langle \\nabla f(x), x - x_0 \\rangle $ $ \\lambda_{\\min}(\\nabla^2 f(x)) \\geq 0 $ $ \\frac{1}{T} \\sum_{i=1}^{T} f(x_i) \\geq f(\\bar{x}), \\quad \\bar{x} = \\frac{1}{T} \\sum_{i=1}^{T} x_i $ $\\mu$-strongly Convex 以下三条等价：\n$ f(x) \\geq f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{\\mu}{2} |x - x_0|^2 $ $ \\lambda_{\\min}(\\nabla^2 f(x)) \\geq \\mu $ $ |\\nabla f(x) - \\nabla f(y)| \\geq \\mu |x - y| $ Convex \u0026amp; L-Smooth: 在一个函数又convex又L-Smooth的情况下，我们会有一些更好的性质：\nThm.1 $$ f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle \\geq \\frac{1}{2L} |\\nabla f(x) - \\nabla f(y)|^2 $$\n证明如下:\n令 $h(y) = f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle$\n注意到 $$ \\nabla h(y) = \\nabla f(y) - \\nabla f(x) $$ $$ \\nabla^2 h(y) = \\nabla^2 f(y) $$ 所以说$h(y)$也是convex且L-smooth的，而且最小值点在$y=x$处取的。 所以,\n$$ h(x) \\leq h(y - \\frac{1}{L} \\nabla h(y))\\ $$ $$\\leq h(y) - \\frac{1}{L} |\\nabla h(y)|^2 + \\frac{1}{2L} |\\nabla h(y)|^2 $$ $$ =h(y) - \\frac{1}{2L} |\\nabla h(y)|^2 $$\n因此,\n$$ f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle \\geq \\frac{1}{2L} |\\nabla f(y)-\\nabla f(x)|^2 $$\nThm.2 $$ \\langle \\nabla f(x) - \\nabla f(y), x - y \\rangle \\geq \\frac{1}{L} |\\nabla f(x) - \\nabla f(y)|^2$$\n这个的证明可以由Thm.1交换$x,y$次序之后相加得到。\n1.2 Gradient Descent GD的update rule如下: $$x_{t+1}=x_{t}-\\eta \\nabla f(x_t)$$ 在以下三种情况下，分别有不同的收敛率：\nConvex, L-Smooth $$ x_{t+1} = x_t - \\eta \\nabla f(x_t) $$\n$$ f(x_{t+1}) \\leq f(x_t) + \\langle \\nabla f(x_t), x_{t+1} - x_t \\rangle + \\frac{L}{2} |x_{t+1} - x_t|^2 $$\n$$ = f(x_t) - \\eta |\\nabla f(x_t)|^2 - \\frac{L \\eta^2}{2} |\\nabla f(x_t)|^2 $$\n取$\\eta \\leq \\frac{1}{L}$:\n$$ f(x_{t+1}) \\leq f(x_t) - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 $$\n由convexity:\n$$ f(x_{t+1}) \\leq f(x^*) + \\langle \\nabla f(x_t), x_t - x^* \\rangle - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 $$\n$$ = f(x^*) - \\frac{1}{\\eta} \\langle x_{t+1} - x_t, x_t - x^* \\rangle - \\frac{1}{2\\eta} |x_{t+1} - x_t|^2 $$\n$$ = f(x^*) - \\frac{1}{2\\eta} |x_{t+1} - x^*|^2 + \\frac{1}{2\\eta} |x_t - x^*|^2 $$\n接下来我们做telescope:\n$$ \\sum_{t=0}^{T-1} (f(x_{t+1}) - f(x^*)) \\leq \\frac{1}{2\\eta} (|x_0 - x^*|^2 - |x_T - x^*|^2) $$\n因为$f(x_t)$是单调递减的(convex保证)\n$$ f(x_T) - f(x^*) \\leq \\frac{1}{2\\eta T} |x_0 - x^*|^2 = \\epsilon $$ 所以说 $$ T = \\frac{|x_0 - x^*|^2}{2\\eta \\epsilon} = O\\left(\\frac{L}{\\epsilon}\\right) $$ 在这种情况下需要迭代$O(\\frac{1}{\\epsilon})$次，收敛率为$O(\\frac{1}{T})$.\n$\\mu$-strongly Convex \u0026amp; L-smooth 这里起手式我们卡$||x-x^*||$: $$ |x_{t+1} - x^*|^2 = |x_t - \\eta \\nabla f(x_t) - x^*|^2 $$\n$$ = |x_t - x^*|^2 - 2\\eta \\langle \\nabla f(x_t), x_t - x^* \\rangle + \\eta^2 |\\nabla f(x_t)|^2 $$ 因为强凸性： $$ f(y) \\geq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{\\mu}{2} |y - x|^2 $$\n代入 $x = x_t$, $y = x^*$:\n$$ f(x^*) \\geq f(x_t) + \\langle \\nabla f(x_t), x^* - x_t \\rangle + \\frac{\\mu}{2} |x_t - x^*|^2 $$\n$$ \\langle \\nabla f(x_t), x_t - x^* \\rangle \\geq f(x_t) - f(x^*) + \\frac{\\mu}{2} |x_t - x^*|^2 $$ 所以 $$ |x_{t+1} - x^*|^2 \\leq |x_t - x^*|^2 - 2\\eta (f(x_t) - f(x^*) + \\frac{\\mu}{2} |x_t - x^*|^2） + \\eta^2 |\\nabla f(x_t)|^2 $$ 根据之前的Thm.1: $$ \\frac{1}{2L} |\\nabla f(x_t)|^2 \\leq f(x_t) - f(x^*) $$ 所以 $$ |x_{t+1} - x^*|^2 \\leq (1 - \\eta \\mu) |x_t - x^*|^2 + (2\\eta^2 L - 2\\eta )(f(x_t) - f(x^*)) $$\n取 $\\eta = \\frac{1}{L}$:\n$$ |x_{t+1} - x^*|^2 \\leq (1 - \\frac{\\mu}{L}) |x_t - x^*|^2 $$ 所以说Linear Convergence, 反映在$f(x)$上: $$f(x_T)\\leq f(x^*)+\\frac{L}{2}||x_T-x^*||^2$$ $$\\leq f(x^*)+\\frac{L}{2}(1 - \\frac{\\mu}{L})^T||x_0-x^*||^2$$ 也就是说需要迭代次数$O(log(\\frac{1}{\\epsilon}))$, 收敛率为Linear Convergence.\nRemark: 对于$\\mu$-strongly Convex \u0026amp; L-smooth的函数有如下性质：$ \\frac{\\mu}{2} | \\mathbf{x}^* - \\mathbf{x} |^2 \\leq f(\\mathbf{x}) - f^* \\leq \\frac{L}{2} | \\mathbf{x}^* - \\mathbf{x} |^2 $ $ \\frac{1}{2L} | \\nabla f(\\mathbf{x}) |^2 \\leq f(\\mathbf{x}) - f^* \\leq \\frac{1}{2\\mu} | \\nabla f(\\mathbf{x}) |^2 $根据这些性质有一个更为简洁的证明。\nL-Smooth 根据第一种情况下的分析： $$ f(x_{t+1}) - f(x_t) \\leq -\\frac{\\eta}{2} |\\nabla f(x_t)|^2 $$\n然后做Telescope:\n$$ \\min_{k \\in [T]} |\\nabla f(x_t)|^2 \\leq \\frac{2L(f(x_0) - f(x^*))}{T} = \\epsilon^2 $$ 所以说当我们想获得$|\\nabla f(x_t)|^2\u0026lt;\\epsilon$，我们需要 $ T = O\\left(\\frac{1}{\\epsilon^2}\\right) $的迭代次数，收敛率为 $ O\\left(\\frac{1}{\\sqrt{T}}\\right) $。\nRecap: 总结起来大概是: 1.3 Stochastic Gradient Descent Why SGD GD看起来不错，但是有两个问题:\n计算一次full gradient很贵 GD会在local maximum和saddle point（鞍点）卡住 于是我们就会去想，能不能少算几个数据点对应的loss function，同时又能有一些convergence guarantee呢，SGD便是这样的一种算法。\nAlgorithm SGD的update rule如下所示: $$ x_{t+1} = x_t - \\eta G_t, $$ 其中$G_t$满足: $$ \\mathbb{E}[G_t] = \\nabla f(x_t), \\quad \\text{Var}(G_t) \\leq \\sigma^2 $$\nConvergence 下面我们证明SGD在L-Smooth, Convex, $\\text{Var}(G_t) \\leq \\sigma^2$的条件下的收敛率:\n因为L-smooth: $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) + \\mathbb{E}[\\langle \\nabla f(x_t), x_{t+1} - x_t \\rangle] + \\frac{L}{2} \\mathbb{E}[|x_{t+1} - x_t|^2] $$\n$$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) - \\eta |\\nabla f(x_t)|^2 + \\frac{L \\eta^2}{2} \\mathbb{E}[|G_t|^2] $$\n根据方差的定义： $$\\mathbb{E}[ ||G_t||^2 ] = \\text{Var}(G_t) + ||\\mathbb{E}[G_t]||^2 \\leq \\sigma^2 + |\\nabla f(x_t)|^2$$ 所以有 $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) + \\left(\\frac{L \\eta^2}{2} - \\eta\\right) |\\nabla f(x_t)|^2 + \\frac{L \\eta^2}{2} \\sigma^2 $$\n取 $\\eta = \\frac{1}{L}$:\n$$ \\mathbb{E}[f(x_{t+1})] \\leq f(x_t) - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 + \\frac{\\eta}{2} \\sigma^2 $$\n根据convexity:\n$$ f(x_t) \\leq f(x^*) + \\langle \\nabla f(x_t), x_t - x^* \\rangle $$\n$$ \\mathbb{E}[f(x_{t+1})] \\leq f(x^*) + \\mathbb{E}[\\langle G_t, x_t - x^* \\rangle] - \\frac{\\eta}{2} |\\nabla f(x_t)|^2 + \\frac{\\eta}{2} \\sigma^2 $$ 又因为 $$ |\\nabla f(x_t)|^2 = \\mathbb{E}[|G_t|^2] - \\text{Var}(G_t) \\geq \\mathbb{E}[|G_t|^2] - \\sigma^2 $$\n所以 $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x^*) + \\mathbb{E}[\\langle G_t, x_t - x^* \\rangle - \\frac{\\eta}{2} |G_t|^2] + \\eta \\sigma^2 $$ 注意到 $$ \\langle G_t, x_t - x^* \\rangle - \\frac{\\eta}{2} |G_t|^2 $$\n$$ = -\\frac{1}{2\\eta} |(x_{t+1} - x_t) - (x^* - x_t)|^2 + \\frac{1}{2\\eta} |x_t - x^*|^2 $$\n$$ = \\frac{1}{2\\eta} (|x_t - x^*|^2 - |x_{t+1} - x^*|^2) $$ 也就是说 $$ \\mathbb{E}[f(x_{t+1})] \\leq f(x^*) + \\frac{\\eta}{2} \\mathbb{E}[|x_t - x^*|^2 - |x_{t+1} - x^*|^2] + \\eta \\sigma^2 $$\n从 $t = 0$ 到 $T-1$求和(telescope):\n$$ \\frac{1}{T}\\sum_{t=0}^{T-1} (\\mathbb{E}[f(x_t)] - f(x^*)) \\leq \\frac{1}{2\\eta T} |x_0 - x^*|^2 + \\eta \\sigma^2 $$\n取 $\\eta = \\frac{\\epsilon}{2\\sigma^2} \\leq \\frac{1}{L}$, 则有:\n$$ T = \\frac{2 \\sigma^2 |x_0 - x^*|^2}{\\epsilon^2} $$\nStochastic Gradient Descent (SGD) 的收敛率是 $ O\\left(\\frac{1}{\\sqrt{T}}\\right) $。\n1.4 SVRG 我们看到了通过Stochastic Gradient可以减少computation cost,但是随之而来的问题是因为 $G_t$拥有的variance,导致原来$O(\\frac{1}{T})$的convergence rate变成了$O(\\frac{1}{\\sqrt{T}})$,于是我们去想，有没有什么办法能够在保持computation cost比较小的情况下同时把variance降下来，SVRG是其中的一种算法，在strongly-convex和l-smooth的情况下最后能够获得和GD一样的convergence rate。\nAlgorithm Procedure SVRG Parameters: update frequency $m$ and learning rate $\\eta$\nInitialize $\\tilde{w}_0$\nIterate: for $s = 1, 2, \\ldots$\n$\\tilde{w} = \\tilde{w}_{s-1}$ $\\tilde{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla l_i(\\tilde{w})$ $w_0 = \\tilde{w}$\nIterate: for $t = 1, 2, \\ldots, m$\ni. Randomly pick $i_t \\in {1, \\ldots, n}$ and update weight\n$ w_t = w_{t-1} - \\eta \\left( \\nabla l_{i_t}(w_{t-1}) - \\nabla l_{i_t}(\\tilde{w}) + \\tilde{\\mu} \\right) $ end\nOption I: set $\\tilde{w}_s = w_m$\nOption II: set $\\tilde{w}_s = w_t$ for randomly chosen $t \\in {0, \\ldots, m - 1}$\nend Convergence Rate 前提假设：\nL-smooth, $l_i$: convex, $f$: strong-convex\nBound $\\mathbb{E}[||v_t||^2]$:\n令 $v_t = \\nabla l_i(w_{t-1}) - \\nabla l_i(\\tilde{w}) + \\tilde{u}$\n$\\mathbb{E}[||v_t||^{2}] = \\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(\\tilde{w}) + \\tilde{u})^2]$\n因为$ (a+b)^2 \\leq 2a^2 + 2b^2 $：\n$\\leq 2\\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(w^*))^2] + 2\\mathbb{E}[(\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}) + \\tilde{u})^2]$\n$= 2\\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(w^*))^2] $\n$+ 2\\mathbb{E}[\\left((\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}))-\\mathbb{E}[(\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}))]\\right)^2]$\n又因为$$\\mathbb{E}[(x - \\mathbb{E}[x])^2] = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2 \\leq \\mathbb{E}[x^2]:$$\n所以$\\mathbb{E}[||v_t||^{2}]$\n$\\leq 2\\mathbb{E}[(\\nabla l_i(w_{t-1}) - \\nabla l_i(w^*))^2] + 2\\mathbb{E}[(\\nabla l_i(w^*) - \\nabla l_i(\\tilde{w}))^2]$\n根据Thm.1:\n$\\leq 4L(f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*))$\nBound $||w_t-w^*||$ $$ \\mathbb{E}[|w_{t} - w^*|^2] = \\mathbb{E}[|w_t - w_{t-1} + w_{t-1} - w^*|^2] $$ $$ = \\mathbb{E}[|w_{t} - w^*|^2] + 2 \\mathbb{E}[\\langle w_t - w_{t-1}, w_{t-1} - w^* \\rangle] + \\mathbb{E}[|w_t - w_{t-1}|^2] $$\n$$ = |w_{t-1} - w^*|^2 - 2\\eta \\mathbb{E}[\\langle v_t, w_{t-1} - w^* \\rangle] + \\eta^2 \\mathbb{E}[v_t^2] $$\n$$ \\leq |w_{t-1} - w^*|^2 - 2\\eta \\mathbb{E}[\\langle v_t, w_{t-1} - w^* \\rangle] + 4\\eta^2 L(f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*)) $$\n$$ = |w_{t} - w^*|^2 - 2\\eta \\langle \\nabla f(w_{t-1}), w_{t+1} - w^* \\rangle + 4L\\eta^2 (f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*)) $$ 又因为convexity： $$ f(w_{t-1}) - f(w^*) \\geq \\langle \\nabla f(w_{t-1}), w_{t-1} - w^* \\rangle $$\n$$ \\Rightarrow \\mathbb{E}[|w_{t} - w^*|^2] \\leq |w_{t-1} - w^*|^2 - 2\\eta (f(w_{t-1}) - f(w^*)) + 4L\\eta^2 (f(w_{t-1}) - f(w^*) + f(\\tilde{w}) - f(w^*)) $$\n$$ = |w_{t+1} - w^*|^2 + 4L\\eta^2 (f(\\tilde{w}) - f(w^*)) + 2\\eta (2L\\eta - 1)(f(w_{t-1}) - f(w^*)) $$\nTelescope 从$\\sum_{t=1}^{m}$，用option 2: $$ \\mathbb{E}[|w_m - w^*|^2] \\leq \\mathbb{E}[|\\tilde{w} - w^*|^2] + 4mL\\eta^2 (f(\\tilde{w}) - f(w^*)) + 2m\\eta (2L\\eta - 1) \\mathbb{E}[f(\\tilde{w}_s) - f(w^*)] $$\n重新整理成:\n$$ \\mathbb{E}[|w_m - w^*|^2] + 2m\\eta (1 - 2L\\eta) \\mathbb{E}[f(\\tilde{w}_s) - f(w^*)] $$\n$$ \\leq \\mathbb{E}[|\\tilde{w} - w^*|^2] + 4mL\\eta^2 (f(\\tilde{w}) - f(w^*)) $$\n$$ \\leq \\left(\\frac{2}{u} + 4mL\\eta^2\\right)(f(\\tilde{w}) - f(w^*)) $$\n所以\n$$ \\mathbb{E}[f(\\tilde{w}_s) - f(w^*)] \\leq (\\frac{1}{u\\eta (1 - 2L\\eta)m} + \\frac{2L\\eta}{1-2L\\eta}) $$\n$$\\cdot \\mathbb{E} [f(\\tilde{w}_{s - 1})-f(w^*)]$$\n所以收敛率是Linear Convergence, $\\frac{L}{u}$大时比GD快。\n1.5 Mirror Descent Algorithm 对于一个1-strongly convex的Distance Generating Function$w(x)$,我们定义Bergman Divergence:$$V_x(y)=w(y)-w(x)-\\langle \\nabla w(x),y-x \\rangle$$ 然后我们定义: $$\\text{Mirror}_ {x}(\\zeta) = \\arg \\min_ {y} { V_ {x}(y) + \\langle \\zeta, y - x \\rangle } $$\n一个Mirror Descent的定义是 $$ x_{t+1} = \\text{Mirror}_ {x_t} (\\alpha \\nabla f(x_t)) $$\n$$ = \\arg \\min_{y} \\left( w(y) - w(x_t) - \\langle \\nabla w(x_t), y - x_t \\rangle + \\alpha \\langle \\nabla f(x_t), y - x_t \\rangle \\right) $$\nIntuition 第二种视角称为镜像空间 (Mirror space) 视角，一个 Mirror step 可以被视作将偶空间上的梯度下降，即朝另一个新的极值点进行搜索。过程形如：\n将 $x$ 通过 Mirror map 映射到对偶空间上的 $\\theta_k$。 $\\theta_ {k+1} = \\theta_ k - \\alpha \\nabla f(x_k)$。 将 $\\theta_ {k+1}$ 映射回原空间上的 $\\overline{x} _{k+1}$。 将 $\\overline{x}_ {k+1}$ 投影到约束集，投影使用 Bregman divergence 作为其距离，即 $x_ {k+1} = \\arg \\min_ {y} V_ {x_{k+1}}(y)$。 按照 Mirror step 的式子，可以看出 Mirror map 就是 $\\nabla w(\\cdot)$。因此实际过程为：\n$\\theta_k = \\nabla w(x)$。 $\\theta_{k+1} = \\theta_k - \\alpha \\nabla f(x_k)$。 $\\overline{x}_{k+1} = (\\nabla w)^{-1}(\\theta{k+1})$。 $x_{k+1} = \\arg \\min_{y} V_{\\overline{x}_{k+1}}(y)$。 这个视角提出了一点假设，$(\\nabla w)^{-1}(\\overline{x}_{k+1})$ 始终存在，即 ${\\nabla w(x)} = \\mathbb{R}^n$。\nRelationship between GD \u0026amp; MD 这个问题曾很长一段时间让笔者感到困惑。笔者对于这一块并非很懂，笔者现在的理解是:\n我们知道一个Primal Space和Dual Space的范数之间满足$\\frac{1}{p}+\\frac{1}{q}=1$\nGD是MD在 $\\alpha=\\frac{1}{L}$，primal space取$||·||_2$范数，Distance Generating Function取 $w(x)=\\frac{1}{2} x^2$下的特殊情况。在这种情况下，因为L2-norm的Dual就是L2-norm，所以这个对偶空间就是原空间。\n但是另一种理解方式是，MD是先通过梯度映射到Dual Space之后在这个空间下做GD再逆映射后project回原来的空间中。\nConvergence: 前提条件: $f(x)$ convex, $w(x)$ 1-strongly convex, $\\nabla f(x)\\leq \\rho$\nBound $f(x_t)-f(x^*)$: 因为convexity： $$ \\alpha (f(x_{t+1}) - f(u)) \\leq \\langle \\alpha \\nabla f(x_t), x_t - u \\rangle $$ 又因为MD的更新规则： $$ x_{t+1} = \\arg \\min_{y} \\left( V_{x_t}(y) + \\langle \\alpha \\nabla f(x_t), y - x_t \\rangle \\right) $$ 所以说由最小值点梯度等于0: $$ \\alpha \\nabla f(x_t) = - \\nabla V_{x_t}(x_{t+1}) $$ 因此 $$ \\alpha (f(x_t) - f(u)) \\leq \\langle \\alpha \\nabla f(x_t), x_t - x_{k+1} \\rangle + \\langle - \\nabla V_{x_t}(x_{k+1}), x_{k+1} - u \\rangle $$ 接下来我们证明一个重要的triangle inequality: $$ \\langle - \\nabla V_{x_t}(y), y - u \\rangle = \\langle \\nabla w(x) - \\nabla w(y), y - u \\rangle $$\n$$ = (w(u) - w(x)) - \\langle \\nabla w(x), u - x \\rangle - (w(y) - w(x) - \\langle \\nabla w(x), y - x \\rangle) $$\n$$ = V_x(u) - V_x(y) - V_y(u) $$ 带回原式: $$ \\alpha (f(x_t) - f(u)) \\leq \\langle \\alpha \\nabla f(x_t), x_t - x_{k+1} \\rangle + V_{x_k}(u) - V_{x_k}(x_{k+1}) - V_{x_{k+1}}(u) $$\n由于DGF的1-strongly convex:\n$$ \\leq \\langle \\alpha \\nabla f(x_t), x_t - x_{k+1} \\rangle- \\frac{1}{2} |x_{k+1} - x_t|^2 + V_{x_k}(u) - V_{x_{k+1}}(u) $$ 这步是前两项做个配方法: $$ \\leq \\frac{\\alpha^2}{2} |\\nabla f(x_t)|^2 + V_{x_k}(u) - V_{x_k}(x_{k+1}) $$\nTelescoping: $$ \\alpha T (f(\\overline{x}) - f(x_t)) \\leq \\sum \\text{LHS} \\leq \\sum \\text{RHS} $$ $$ \\leq \\frac{\\alpha^2 T}{2} \\cdot \\rho^2 + V_{x_0}(x^*) - V_{x_T}(x^*) $$ 所以说 $$ f(\\overline{x}) - f(x^*) \\leq \\frac{\\alpha}{2} \\rho^2 + \\frac{\\Theta}{\\alpha T} $$ 令$\\alpha = \\sqrt{\\frac{2\\Theta}{T \\rho^2}}$.\n有$f(x_T) - f(x^*) \\leq \\sqrt{\\frac{2\\Theta}{T }}\\rho= \\epsilon$ 于是我们得到了我们的收敛率 $$ T = \\Omega \\left( \\frac{\\rho^2}{\\epsilon^2} \\right) $$\n1.6 Linear Coupling Wishful Thinking 我们通过1.5的分析已经知道Mirror Descent有 $ T = O\\left(\\frac{\\rho^2}{\\epsilon^2}\\right) $的收敛率\n然后我们知道在GD中 $$ f(x_{t+1}) - f(x_t) \\leq -\\frac{1}{2L} |\\nabla f(x_t)|^2 $$ 所以说在gradient比较大的时候: $$ |\\nabla f(x_t)| \u0026gt; \\rho : \\Omega\\left(\\frac{L \\epsilon}{\\rho^2}\\right) \\text{ steps} $$\n在gradient比较小的时候MD:\n$$ |\\nabla f(x_t)| \u0026lt; \\rho : \\Omega\\left(\\frac{\\rho^2}{\\epsilon^2}\\right) \\text{ steps} $$ 所以我们想能不能在梯度大的时候跑GD，在梯度小的时候跑MD，这样会获得一个更好的收敛率\nCoupling:\n$$\\Omega ( \\max { \\frac{L \\epsilon}{\\rho^2}, \\frac{\\rho^2}{\\epsilon^2} })$$ 取$\\rho = (L \\epsilon^{3})^\\frac{1}{4}$: $$ \\Omega\\left(\\sqrt{\\frac{L}{\\epsilon}}\\right) \\text{ steps} $$\nAlgorithm 初始化 $$x_0 = y_0 = z_0$$ 每一步更新,更新$x$: $$ x_{k+1} = \\tau z_k + (1 - \\tau) y_k $$ 更新$y$: $$ y_{k+1} = \\arg \\min_{y \\in \\mathcal{Q}} { \\frac{L}{2} |y - x_{k+1}|^2 + \\langle \\nabla f(x_{k+1}), y - x_{k+1} \\rangle } $$\n$$ = x_{k+1} - \\frac{1}{L} \\nabla f(x_{k+1}) \\quad \\text{(GD step)} $$\n更新$z$: $$ z_{k+1} = Mirror_{z_k} (\\alpha \\nabla f(x_{k+1})) $$ Convergence 根据MD的分析: $$ \\alpha \\langle \\nabla f(x_{k+1}), z_k - u \\rangle \\leq \\frac{\\alpha^2}{2} |\\nabla f(x_{k+1})|^2 + V_{z_k}(u) - V_{z_{k+1}}(u) $$ 由于 $$ f(x_{k+1}) - f(y_{k+1}) \\geq \\frac{1}{2L} |\\nabla f(x_{k+1})|^2$$ 所以原式 $$ \\leq \\alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u) $$ 又因为convexity: $$ \\alpha (f(x_{k+1}) - f(u)) \\leq \\alpha \\langle \\nabla f(x_{k+1}), x_{k+1} - u \\rangle $$\n$$ = \\alpha \\langle \\nabla f(x_{k+1}), z_k - u \\rangle + \\alpha \\langle \\nabla f(x_{k+1}), x_{k+1} - z_k \\rangle $$ 前面一项我们已经MD做掉了，后面一项 $$ \\alpha \\langle \\nabla f(x_{k+1}), x_{k+1} - z_k \\rangle $$\n$$ = \\frac{(1 - \\tau) \\alpha}{\\tau} \\langle \\nabla f(x_{k+1}), y_k - x_{k+1} \\rangle $$\n$$ \\leq \\frac{(1 - \\tau) \\alpha}{\\tau} (f(y_k) - f(x_{k+1})) $$ 所以说 $$ \\alpha (f(x_{k+1}) - f(u)) \\leq \\alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + \\frac{(1 - \\tau) \\alpha}{\\tau} (f(y_k) - f(x_{k+1})) $$\n$$+ V_{z_k}(u) - V_{z_{k+1}}(u) $$ 令 $ \\frac{(1 - \\tau) \\alpha}{\\tau} = \\alpha^2 L $， 有 $$ f(x_{k+1}) - f(u) \\leq \\alpha^2 L (f(y_k) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u) $$\nTelescope:\n$$ \\alpha T (f(\\overline{x}) - f(x^*)) \\leq \\alpha^2 L (f(y_0) - f(y_T)) + V_{x_0}(x^*) - V_{z_T}(x^*) $$\n假设 $f(y_0) - f(x^*) = d$, $V_{x_0}(x^*) = \\Theta$ 有 $$ f(x_i) - f(x^*) \\leq \\frac{\\alpha dL}{T} + \\frac{\\Theta}{\\alpha T} $$ 令$ \\alpha = \\sqrt{\\frac{\\Theta}{dL}}$, 有 $$ f(\\overline{x}) - f(x^*) \\leq \\frac{2 \\sqrt{\\Theta Ld}}{T}$$\n取 $ T = 4 \\sqrt{\\frac{L\\Theta}{d}}$, 有$$f(\\overline{x})-f(x^*)\\leq \\frac{d}{2}$$ 所以说我们每 $2\\epsilon\\rightarrow \\epsilon$过程重新调整一次$\\tau,\\alpha$，最后得到的迭代次数是: $$O(\\sqrt{\\frac{L \\Theta}{\\epsilon}})+O(\\sqrt{\\frac{L \\Theta}{2\\epsilon}})+O(\\sqrt{\\frac{L \\Theta}{4\\epsilon}})+\u0026hellip;=O(\\sqrt{\\frac{L \\Theta}{\\epsilon}})$$ Nesterov告诉我们$O(\\frac{1}{T^2})$(aka.$O(\\sqrt{\\frac{L}{\\epsilon}})$)就是我们对于convex且L-smooth函数能得到的最好结果了，所以Linear Coupling确实很牛。\n1.7 Non-Convex Optimization Matrix Completion $A \\in \\mathbb{R}^{m \\times n}$满足以下假设:\n1° $A$ is low rank\n2° Known entries are uniformly distributed\n3° Incoherence: $$ A = U \\Sigma V^T \\quad \\text{for } i \\in [n], j \\in [m]$$ $$\\exists \\mu: 1 \\leq \\mu \\leq \\frac{min(m,n)}{r}$$$$ |e_i^T U| \\leq \\sqrt{\\frac{\\mu r}{n}}, \\quad |e_j^T V| \\leq \\sqrt{\\frac{\\mu r}{m}}$$\n那么我们的目标($P_\\Omega$代表不知道的元素都mask掉): $$ \\min |P_\\Omega(UV^T) - P_\\Omega(A)|_F^2 $$ 可以有以下算法:\nAlgorithm:\nFor $t = 0, 1, 2, \\ldots, T$\n$V^{t+1} \\leftarrow \\arg \\min_V ||P_{\\Omega}(U^t V) - P_{\\Omega}(A)||_F^2$ $U^{t+1} \\leftarrow \\arg \\min_U ||P_{\\Omega}(U V^{t}) - P_{\\Omega}(A)||_F^2$ Escaping Saddle Points SGD在非凸优化中有一些GD之类算法没有的好处，这就是噪声所带来的随机性所展现的优势:\nThm.If 𝐿 is smooth, bounded and strict saddle (actually more general version, applies to points with small gradients, rather than zero gradients), and Hessian is smooth. If SGD noise has non-negligible variance in every direction with constant probability, SGD will escape all saddle points and local maxima, converge to a local minimum after polynomial number of steps.\n其中Strict Saddle Point是指一个点$\\nabla f(x)=0$， $\\nabla^2 f(x)$又有正特征值又有负特征值。Flat Saddle Point是指一个点$\\nabla f(x)=0$， $\\nabla^2 f(x)$的所有特征值都大于等于0，且有一个等于0的特征值。\n2.Generalization 2.1 No Free Lunch Thm. Thm. 设 $A$ 为在定义域 $\\mathcal{X}$ 上相对于 0-1 损失的二元分类任务的任意学习算法。设 $m$ 为小于 $|\\mathcal{X}|/2$ 的任意数，表示训练集大小。则存在一个在 $\\mathcal{X} \\times {0, 1}$ 上的分布 $\\mathcal{D}$ 使得：\n存在一个函数 $f : \\mathcal{X} \\to {0, 1}$，使得 $L_\\mathcal{D}(f) = 0$。 以至少 $1/7$ 的概率，对于从 $\\mathcal{D}^m$ 中选取的 $S$，有 $L_\\mathcal{D}(A(S)) \\geq 1/8$。 这个的直觉在于由Markov不等式，$\\mathbb{E}_{S \\sim D^m }[L_D(A(S))]\\geq \\frac{1}{4}$,也就是说对于一个完全靠背诵的算法: 假如见过$(X,y)$,输出$y$，假如没见过就随机输出0或1。这样对于一个$|C|=2m$的$X$的子集，这样“背诵+瞎蒙”的loss function是$\\frac{1}{4}$。也就是说，没有什么办法能够从期望上比“背诵+瞎蒙”效果更好，也就是说学习算法失败了。\n证明:\n为了简洁性，不妨设$|C| = 2m$.\n记$T = 2^{2m}$。从$C$到${0, 1}$的函数一共有$f_1, \\ldots, f_T$，共$T$个\n记 $$ D_i({x, y}) = \\frac{1}{|C|} \\quad \\text{if } y = f_i(x) $$ $$ D_i({x, y}) = 0 \\quad \\text{otherwise.} $$\n显然，$L_{D_i}(f_i) = 0$.\n我们接下来证明:\n$$\\max_{i \\in [T]} E_{S \\sim D_{i}^{m}} [ L_{D_i}(A(S)) ] \\geq \\frac{1}{4}$$\n记一共有$k$个可能的从$C$中取样出的$m$个数据点$x_i$序列: 有$k = (2m)^m$，记 $S_j = (x_1, \\ldots, x_m)$ ，记 $S_j^i = \\left( (x_1, f_i(x_1)), \\ldots, (x_m, f_i(x_m)) \\right)$。\n我们只需要取出一个$i \\in [T]$能够让$E_{S \\sim D_i^m} \\left[ L_{D_i}(A(S)) \\right]\\geq \\frac{1}{4}$,那么对应的$D_i$便是我们在NFL中所希望找到的$D$。 $$ \\max_{i \\in [T]} E_{S \\sim D_i^m} \\left[ L_{D_i}(A(S)) \\right] $$\n$$ = \\max_{i \\in [T]} \\frac{1}{k} \\sum_{j=1}^k L_{D_i}(A(S_j^i)) $$\n$$ \\geq \\frac{1}{T} \\sum_{i=1}^T \\frac{1}{k} \\sum_{j=1}^k L_{D_i}(A(S_j^i)) $$\n$$ = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{T} \\sum_{i=1}^T L_{D_i}(A(S_j^i)) $$\n$$ \\geq \\min_{j \\in [k]} \\frac{1}{T} \\sum_{i=1}^T L_{D_i}(A(S_j^i)) $$ 对于给定的 $j$:\n令$v_1, \\ldots, v_p$ 为$S_j$中没有出现的$x\\in C$, 注意到$p \\geq m$。\n$$ L_{D_i}(A(S_j^i)) = \\frac{1}{2m} \\sum_{x \\in C} \\mathbf{1}[h(x) \\neq f_i(x)] $$\n$$ \\geq \\frac{1}{2m} \\sum_{r=1}^p \\mathbf{1}[h(v_r) \\neq f_i(v_r)] $$\n$$ \\geq \\frac{1}{2p} \\sum_{r=1}^p \\mathbf{1}[h(v_r) \\neq f_i(v_r)] $$\n所以说\n$$ \\frac{1}{T} \\sum_{i=1}^T L_{D_i}(A(S_j^i)) $$\n$$ \\geq \\frac{1}{T} \\sum_{i=1}^T \\frac{1}{2p} \\sum_{r=1}^p \\mathbf{1}[h(v_r) \\neq f_i(v_r)] $$\n我们可以将 $f_1, \\ldots, f_T$ 中的所有函数划分成 $T/2$ 对不相交的函数对，其中对于每一对 $(f_i, f_{i\u0026rsquo;})$，对于任意 $c \\in C$，都有 $f_i(c) \\neq f_{i\u0026rsquo;}(c)$。\n于是有 $$ \\frac{1}{2p} \\sum_{r=1}^p \\frac{1}{T} \\sum_{i=1}^T \\mathbf{1}[h(v_r) \\neq f_i(v_r)] = \\frac{1}{4} $$\n所以说 $$ \\max_{i \\in [T]} E_{S \\sim D_i^m} \\left[ L_{D_i}(A(S)) \\right] \\geq \\frac{1}{4} $$\n令$\\mathcal{D} = D_i$:\n如果\n$$ \\Pr \\left[ L_{\\mathcal{D}}(A(S)) \\geq \\frac{1}{8} \\right] \u0026lt; \\frac{1}{7} $$\n那么\n$$ E_{S \\sim \\mathcal{D}^m} \\left[ L_{\\mathcal{D}}(A(S)) \\right] \u0026lt; \\frac{1}{7} \\cdot 1 + \\frac{6}{7} \\cdot \\frac{1}{8} $$\n$$ = \\frac{1}{7} + \\frac{3}{28} = \\frac{1}{4}.\\quad\\blacksquare $$\n2.2 PAC-Learning 一些概念: Hypothesis Class (H) ：能够选择的假设$h$的集合 $ERM_H$ ：选择具有最小empirical loss的假设 $$ ERM_H(S) \\in \\arg\\min_{h \\in H} L_S(h) $$\nRealizability Assumption: 存在 $h^* \\in H$ 使得 $L_{D,f}(h^*) = 0$。这意味着对于每个训练集 $S$，我们有 $L_S(h^*) = 0$。 PAC-Learnable: 如果存在一个函数 $m_H: (0,1)^2 \\to \\mathbb{N}$ 和一个learning algorithm，使得对于任意的 $\\epsilon, \\delta \\in (0,1)$，对于定义在 $X$ 上的任意分布 $D$，以及任意labeling function $f: X \\to {0,1}$，若Realizability Assumption在 $H, D, f$ 下成立，则当在由 $D$ 生成并由 $f$ 标记的 $m \\geq m_H(\\epsilon, \\delta)$ 个独立同分布样本上运行该learning algorithm时，该算法返回一个假设 $h$，使得以至少 $1 - \\delta$ 的概率（在样本选择的随机性上），$L_{D,f}(h) \\leq \\epsilon$。 Finite Classes are PAC-learnable Thm. 给定 $\\delta \\in (0,1)$, $\\epsilon \u0026gt; 0$, 如果 $m \\geq \\frac{\\log(|H|/\\delta)}{\\epsilon}$，那么如果Realizability Assumption成立， 那么对于任意ERM hypothesis $h_S$: $$ \\Pr [ L_D(h_S) \\leq \\epsilon ] \\geq 1 - \\delta $$ Pf. 我们想要upper bound\n$$ \\Pr_{S\\sim \\mathcal{D}^m} [ S | L_D(h(S)) \u0026gt; \\epsilon ] $$\n定义所有不好的假设的集合为: $$ H_B := { h \\in H | L_D(f, h) \u0026gt; \\epsilon } $$ 定义misleading的假设的集合为： $$ M := { S \\mid \\exists h \\in H_B, L_S(h) = 0 } $$ 有 $$ { S \\mid L_D(h(S)) \u0026gt; \\epsilon } \\subseteq M $$ 所以 $$ \\Pr \\left[ L_D(h(S)) \u0026gt; \\epsilon \\right] \\leq \\Pr \\left[ S \\in M \\right] \\leq \\sum_{h \\in H_B} \\Pr \\left[ L_S(h) = 0 \\right] $$ 又因为 $$ \\Pr \\left[ L_S(h) = 0 \\right] = \\prod_{i=1}^m Pr_{x_i\\sim\\mathcal{D}} \\left[ h(x_i) = f(x_i) \\right] $$\n因为 $$ Pr_{x_i\\sim\\mathcal{D}} \\left[ h(x_i) = f(x_i) \\right] = 1 - L_D(f, h) \\leq 1 - \\epsilon $$ 所以\n$$ \\Pr \\left[ L_S(h) = 0 \\right] \\leq (1 - \\epsilon)^m \\leq e^{-m \\epsilon} $$\n$$ |H| \\cdot e^{-m \\epsilon} \\leq \\delta \\implies m = \\frac{\\log(|H|/\\delta)}{\\epsilon}. \\blacksquare $$\nThreshold Functions are PAC-learnable Threshold Functions: $$ \\mathcal{H}={h(x) = \\mathbf{1}[x \u0026lt; a]} $$ 注意到这是一个infinite class。 Thm. 设 $H$ 为Threshold Functions。则 $H$ 是 PAC-learnable的，使用 ERM 算法，其样本复杂度为$$ m_H(\\epsilon, \\delta) \\leq \\frac{\\lceil \\log(2/\\delta) \\rceil}{\\epsilon}$$\nPf. 记$ h^*(x) = \\mathbf{1}[x \u0026lt; a^*] $s.t.$L_D(h^*)=0$\n定义 $$ b_0 := \\sup {x \\mid (x, 1) \\in S}, \\quad b_1 := \\inf {x \\mid (x, 0) \\in S} $$ 注意到 $$ \\Pr \\left[ L_D(h) \u0026gt; \\epsilon \\right] \\leq \\Pr \\left[ b_0 \u0026lt; a_0 \\right] + \\Pr \\left[ b_1 \u0026gt; a_1 \\right] $$ 在$ m = \\frac{\\ln \\left(\\frac{2}{\\delta}\\right)}{\\epsilon} $的情况下: $$ \\Pr \\left[ b_0 \u0026lt; a_0 \\right] = (1 - \\epsilon)^m \\leq e^{-\\epsilon m} = \\frac{\\delta}{2} $$\n$$ \\Pr \\left[ b_1 \u0026gt; a_1 \\right] = (1 - \\epsilon)^m \\leq e^{-\\epsilon m} = \\frac{\\delta}{2}. \\blacksquare $$\n2.3 Agnostic PAC-Learnable 有时候Realizability Assumption太强了，我们希望能够得到一个在$\\mathcal{H}$中没有Loss=0的hypothesis的情况下衡量estimation error的手段:\nAgnostic PAC-Learnable: 一个假设类 $H$ 是 Agnostic PAC 可学习的，如果存在一个函数 $m_H: (0,1)^2 \\rightarrow \\mathbb{N}$ 和一个具有以下性质的学习算法：对于每一个 $\\epsilon, \\delta \\in (0,1)$，以及定义在 $X \\times Y$ 上的每个分布 $D$，当在由 $D$ 生成的 $m \\geq m_H(\\epsilon, \\delta)$ 个独立同分布（iid）样本上运行该学习算法时，算法会返回一个假设 $h$，使得以至少 $1 - \\delta$ 的概率（对于 $m$ 个训练样本的选择而言），满足\n$$ L_D(h) \\leq \\min_{h\u0026rsquo; \\in H} L_D(h\u0026rsquo;) + \\epsilon $$\nError Decomposition: $L_D(h_S) = \\epsilon_{app} + \\epsilon_{est}$ $\\epsilon_{app} = \\min_{h \\in H} L_D(h)$ $\\epsilon_{est} = L_D(h_S) - \\epsilon_{app}$ $\\epsilon_{app} = L_D(BO) + \\min_{h \\in H} L_D(h) - L_D(BO)$ $\\epsilon_{app}$描述的是这个hypothesis class的inductive bias的多少，而$\\epsilon_{est}$是与sample size和sample complexity相关的(sample complexity与hypothesis class的representation power成正比)，所以说当我们想要减少$L_D(h_S)$,我们面临一个bias-complexity tradeoff。\n其中BO指代的是Bayes Optimal Predictor。\nBayes Optimal Predictor 给定任何在 $X \\times {0,1}$ 上的概率分布 $D$，从 $X$ 到 ${0,1}$ 的最佳标签预测函数为\n$$ f_D(x) = 1 \\quad \\text{if } P[y = 1 \\mid x] \\geq \\frac{1}{2} $$ $$ f_D(x) = 0 \\quad \\text{otherwise} $$\n很容易验证，对于每个概率分布 $D$，贝叶斯最优预测器 $f_D$ 是最优的，因为没有其他分类器 $g: X \\rightarrow {0,1}$ 的错误率更低。即，对于每个分类器 $g$，有\n$$ L_D(f_D) \\leq L_D(g) $$\n2.4 VC-Dim Restriction of $H$ to $C$ 设 $H$ 是从 $X$ 到 ${0,1}$ 的函数类，$C = {c_1, \\cdots, c_m} \\subseteq X$。$H$ 在 $C$ 上的限制是从 $C$ 到 ${0,1}$ 的函数集合，这些函数可以从 $H$ 中导出。即\n$$ H_C = {(h(c_1), \\cdots, h(c_m)) : h \\in H} $$\n我们将从 $C$ 到 ${0,1}$ 的每个函数表示为 ${0,1}^{|C|}$ 中的一个向量。\nShattering 一个假设类 $H$ Shatter有限集 $C \\subseteq X$，如果 Restriction of $H$ to $C$是从 $C$ 到 ${0,1}$ 的所有函数集合。即\n$$ |H_C| = 2^{|C|} $$\nNFL Reexpressed 设 $H$ 是从 $X$ 到 ${0,1}$ 的hypothesis class。令 $m$ 为训练集大小。假设存在一个大小为 $2m$ 的集合 $C \\subseteq X$，它被 $H$ shatter。则对于任意学习算法 $A$，存在一个定义在 $X \\times {0,1}$ 上的分布 $D$ 和一个预测器 $h \\in H$，使得 $L_D(h) = 0$，但以至少 $\\frac{1}{7}$ 的概率，对于 $S \\sim D^m$ 的选择，有\n$$ L_D(A(S)) \\geq \\frac{1}{8} $$\nVC-Dimension Hypothesis class $H$ 的 VC-dimension（记作 $\\text{VCdim}(H)$）是 $H$ 可以shatter的集合 $C \\subseteq X$ 的最大大小。如果 $H$ 可以shatter任意大的集合，我们称 $\\text{VCdim}(H)=+ \\infty$.\nInifite VC-dim hypothesis classes are not PAC-learnable NFL的直接后果就是$\\text{VCdim}(H)=+ \\infty$的$H$不是PAC-learnable的。\n2.5 Fundamental theorem of statistical learning 设 $H$ 是从一个域 $X$ 到 ${0,1}$ 的hypothesis class，并且损失函数是 0-1 损失。假设 $\\text{VCdim}(H) = d \u0026lt; \\infty$。则存在常数 $C_1, C_2$，使得\n$H$ 是具有以下样本复杂度的Agnostic PAC-learnable：\n$$ C_1 \\frac{d + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon^2} \\leq m_H(\\epsilon, \\delta) \\leq C_2 \\frac{d + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon^2} $$\n$H$ 是具有以下样本复杂度的 PAC-learnable：\n$$ C_1 \\frac{d + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon} \\leq m_H(\\epsilon, \\delta) \\leq C_2 \\frac{d \\log \\left(\\frac{1}{\\epsilon}\\right) + \\log \\left(\\frac{1}{\\delta}\\right)}{\\epsilon} $$\n3.Supervised Learning 对于回归问题，我们构造一个函数$f: X\\rightarrow \\mathbb{R}$ 在分类问题中，我们构造一个函数$f: X\\rightarrow {0,1}$或者${-1,1}$。 前者我们的loss function很好design，比如说Mean Square Loss，但是后者的loss就不是特别好design。一种自然的想法是$f(x)=sign(w^Tx)$,但是问题就是这个loss不可导，下面是一种利用这种函数但是不需要导数的远古算法。\n3.1 Perceptron Algorithm Convergence Thm.\n合适缩放使得 $||x_i|| \\leq 1$ 。假设存在 $w_*$ 满足 $||w_*|| = 1$ 且 $y_i w_*^T x_i \u0026gt; \\gamma$（存在过原点的划分平面，安全距离为 $\\gamma$）。该算法收敛前最多触发 $\\frac{1}{\\gamma^2}$ 次预测错误。\nPf. 假设算法第 $t$ 次犯错是 $(x_t, y_t)$，这会使得\n$$w_{t+1} = w_t + y_t x_t$$\n且此时 $\\langle {w}^T, y_t x_t \\rangle \u0026lt; 0$（锐角）。这说明 $$ ||w_{t+1}||^2 \\leq ||w_t||^2 + ||y_t x_t||^2 = ||w_t||^2 + 1 \\ ||w_t||^2 \\leq t $$\n另一方面 $$ ||w_{t+1}|| \\geq \\langle w_{t+1}, w_* \\rangle \\geq \\langle w_t, w_* \\rangle + \\gamma \\ ||w_t|| \\geq \\gamma t $$\n综上 $$ \\gamma^2 t^2 \\leq |w_t|^2 \\leq t $$ 解得 $t \\leq \\frac{1}{\\gamma^2}$。$\\blacksquare$\n3.2 Logistic Regression 为了解决不可导的问题，更为现代的想法是通过sigmoid函数把$w^Tx$压缩到$(0,1)$之间的概率，即$$f(x)=\\frac{1}{1+e^{-w^Tx}}.$$ 衡量两个概率之间的差异，可以用l1-norm或者cross-entropy loss。\n熵 (Entropy) 对于离散概率分布 $(p_1, p_2, \\cdots, p_n)$，定义它的熵为$$ H(p) = \\sum_{i=1}^{n} p_i \\log \\frac{1}{p_i}$$\n交叉熵 (Cross entropy) 定义两个离散概率分布 $(p_1, p_2, \\cdots, p_n)$ 和 $(q_1, q_2, \\cdots, q_n)$ 的交叉熵为$$ XE(p, q) = \\sum_{i=1}^{n} p_i \\log \\frac{1}{q_i}$$\nKL 散度 定义两个离散概率分布 $(p_1, p_2, \\cdots, p_n)$ 和 $(q_1, q_2, \\cdots, q_n)$ 的 KL 散度为$$ KL(p, q) = XE(p, q) - H(p)$$\n交叉熵比l1-norm 好在：\nl1-norm：提供恒定的梯度。 交叉熵：差距越大，梯度越大 3.3 Regularization 当我们想要限制$f$的表达能力时，经典的看法就是通过在$||·||_2$或$||·||_1$意义下限制$w$的可能取值区间。\nRidge Regression 把loss function改为$$l(w)+\\lambda||w||^2$$ 这里是2-norm, 这相当于每一步先GD，之后再进行了一次 $$w_{t+1}=(1-\\eta \\lambda)\\tilde{w}_{t}$$ 这被称为weight decay。\nLasso Regression 有时候我们想要获得sparse的解，因此我们把loss function改为$$l(w)+\\lambda||w||_1^2$$ 这个直觉在于用diamond和凸集的交集更有可能是sparse的 3.4 Compressed Sensing Nyquist theorem: for a signal with frequency 𝑓, we need 2𝑓 sampling rate to fully reconstruct the signal\n这个是一个通用的定理，但是大部分情况下，我们的信号其实是存在一组基下的稀疏表示，所以我们会去想能不能通过更少的采样，来重构出信号，这就是compressed sensing的背景。\n在Compressed Sensing中，和supervised learning不同的是我们可以自己选择自己的measurement matrix，即训练集，在下图中也就是说我们可以自由选定$A$的每一行，然后获得对应的$y$，最终我们希望通过$y$还原出$x$。\n最后的得到的主要结论，用自然语言去描述，是如下三条:\n如果一个稀疏信号通过 $x \\mapsto Wx$ 进行了压缩，其中 $W$ 是满足$(\\epsilon, s)$-RIP 的矩阵，那么可以完全重构任何稀疏信号。满足此性质的矩阵保证了任何稀疏可表示向量的范数distortion较小。\n通过求解线性规划，重构可以在多项式时间内计算。\n给定 $n \\times d$ 的随机矩阵，在 $n$ 大于 $s \\log(d)$ 的数量级时，它很可能满足 RIP 条件。\n接下来让我formally用数学的语言build up都以上的结论。\nRIP-Condition 一个矩阵 $W \\in \\mathbb{R}^{n,d}$ 是 $(\\epsilon, s)$-RIP 的当且仅当对于所有 $x \\neq 0$ 且满足 $||x||_{0}\\leq s$ 的 $x$，我们有 $$ \\left| \\frac{||Wx||_2^2}{||x||_2^2} - 1 \\right| \\leq \\epsilon. $$\nThm.1 Thm.1 设 $\\epsilon \u0026lt; 1$，并且设 $W$ 为 $(\\epsilon, 2s)$-RIP 矩阵。设 $x$ 为一个满足 $||x||_0\\leq s$ 的向量，\n令 $y = Wx$ 为 $x$ 的压缩结果，并且令 $$\\tilde{x} \\in \\arg \\min_{{v}: W{v}=y} ||{v}||_0$$ 为重构向量。那么，$\\tilde{x} = x$。\n这个定理告诉我们对于RIP的矩阵，如果我们能够通过找到符合$Wv=y$的$v$的l0-norm最小的向量，我们就能够成功的(无损)重建出$x$。\nPf. 令 $h = \\tilde{x} - x$\n$$ |\\tilde{x}|_0 \\leq |x|_0 \\leq s $$\n因此 $h$ 是 $2s$-sparse的。\n$$ (1 - \\epsilon) |h|^2 \\leq |Wh|^2 \\leq (1 + \\epsilon) |h|^2 $$\n由于 $Wh = W(\\tilde{x} - x) = 0$\n$$ \\Rightarrow |h|^2 = 0 $$\n因此 $\\tilde{x} = x$。$\\blacksquare$\n但问题是，我们没有一个polytime求解l0-norm最小值的算法，所以这个定理在实际应用中没有意义，我们在实际应用中尝试吧l0-norm relax到 l1-norm，下面的thm2和3便是l1-norm下重建结果相似性的保证。\nThm.2 Thm.2 假设 $W$ 为 $(\\epsilon, 2s)$-RIP 矩阵。$x$ 为一个满足 $|x|_0 \\leq s$ 的向量，\n令 $y = Wx$ 为 $x$ 的压缩结果，并且 $\\epsilon \u0026lt; \\frac{1}{1 + \\sqrt{2}}$，那么,\n$$x=\\arg \\min_{v: Wv = y} ||v||_ {0}=\\arg \\min_{v:Wv = y}||v||_1$$\n这个定理说明在s-sparse的情况下，Relax 到l1-norm也可以重构出一样的向量。\n事实上，我们将证明一个更强的结果，该结果即使在 $x$ 不是一个稀疏向量的情况下也成立，即Thm.3。\nThm.3 Thm.3 设 $\\epsilon \u0026lt; \\frac{1}{1 + \\sqrt{2}}$ 并且 $W$ 是一个 $(\\epsilon, 2s)$-RIP 矩阵。设 $x$ 是任意向量，并定义 $$x_s \\in \\arg \\min_{v: ||v|| _ 0 \\leq s} ||x - v||_ 1 $$ 也就是说，$x_s$ 是一个在 $x$ 的 $s$ 个最大元素处等于 $x$ 并在其他地方等于 $0$ 的向量。设 $y = Wx$ ，并令 $$x^* \\in \\arg \\min_{v: Wv = y} |v|_1$$ 为重构的向量。那么， $$|x^* - x|_2 \\leq 2 \\frac{1 + \\rho}{1 - \\rho} s^{-1/2} |x - x_s|_1,$$ 其中 $\\rho = \\sqrt{2\\epsilon}/(1 - \\epsilon)$。\nPf.\n这个定理的证明相对比较复杂，主要是证明以下两个Claim:\nClaim 1： $$ |h_{T_{0,1}}|_ 2 \\leq |h _{T_0}|_2 + 2s^{-1/2}|x - x_s|_1。 $$\nClaim 2： $$ |h_{T_{0,1}}|_ 2 \\leq \\frac{2\\rho}{1 - \\rho}s^{-1/2}|x - x_s|_1。 $$ 符号说明： 给定一个向量 $v$ 和一组索引 $I$，我们用 $v_I$ 表示向量，其第 $i$ 个元素为 $v_i$ 如果 $i \\in I$，否则其第 $i$ 个元素为 0。令 $h = x^* - x$。\n我们使用的第一个技巧是将索引集合 $[d] = {1, \\ldots, d}$ 划分为大小为 $s$ 的不相交集合。也就是说，我们写作 $[d] = T_0 \\cup T_1 \\cup T_2 \\ldots T_{d/s-1}$，对于所有 $i$，我们有 $|T_i| = s$，并且我们为简便起见假设 $d/s$ 是一个整数。我们如下定义划分。在 $T_0$ 中，我们放置 $s$ 个对应于 $x$ 的绝对值中最大的元素的索引（如果有并列的情况，则任意打破平局）。设 $T_0^c = [d] \\setminus T_0$。接下来，$T_1$ 将是对应于 $h_{T_0^c}$ 绝对值中最大的 $s$ 个元素的索引。设 $T_{0,1} = T_0 \\cup T_1$，并令 $T_{0,1}^c = [d] \\setminus T_{0,1}$。接下来，$T_2$ 将是对应于 $h_{T_{0,1}^c}$ 绝对值中最大的 $s$ 个元素的索引。我们将继续构造 $T_3, T_4, \\ldots$ 以相同的方式。\nPf of Claim 1，我们不使用RIP条件，仅仅使用$x^*$最小化$\\ell_1$范数这一事实。设$j \u0026gt; 1$。对于每个$i \\in T_j$和$i\u0026rsquo; \\in T_{j-1}$，我们有$|h_i| \\leq |h_{i\u0026rsquo;}|$。因此，$|h_ {T_j}|_ \\infty \\leq |h_ {T_ {j-1}}|_ 1/s$。由此可以得到：\n$$ ||h_{T_j}||_ 2 \\leq s^{-1/2} ||h_{T_{j-1}}||_1 $$\n对$j = 2, 3, \\ldots$求和，并使用三角不等式，可以得到：\n$$ ||h_{T_{0,1}^c}||_ 2 \\leq \\sum_{j \\geq 2} ||h_{T_j}||_ 2 \\leq s^{-1/2} ||h_{T_{0,1}^c}||_1 $$\n接下来，我们证明$|h_{T_0}|_1$不能太大。实际上，由于$x^* = x + h$具有最小的$\\ell_1$范数，并且$x$满足$x^*$的定义中的约束条件，我们有$|x|_1 \\geq |x + h|_1$。因此，利用三角不等式我们可以得到：\n$$ ||x||_ 1 \\geq \\sum_{i \\in T_0} |x_i + h_i| + \\sum_{i \\in T_{0,1}^c} |x_i + h_i| \\geq ||x_{T_0}||_ 1 - ||h_{T_0}||_ 1 + ||x_{T_{0,1}^c}||_ 1 - ||h_{T_{0,1}^c}||_1 $$\n由于$|x_ {T_ {0,1}^c}|_ 1 = |x - x_s|_ 1 = |x|_ 1 - |x_ {T_ 0}|_ 1$，我们得到：\n$$ |h_{T_0}|_ 1 \\leq |h_{T_0}|_ 1 + 2|x_{T_{0,1}^c}|_1。 $$\n结合上述等式可以得到：\n$$ |h_{T_{0,1}^c}|_ 2 \\leq s^{-1/2} (|h_{T_0}|_ 1 + 2|x_{T_{0,1}^c}|_1)。\\blacksquare $$\nPf of Claim 2\n对于2s-稀疏的向量$h_{T_{0,1}}$，我们有：\n$$(1 - \\epsilon) ||h_{T_{0,1}}||_ 2^2 \\leq ||Wh_{T_{0,1}}||_2^2$$\n而\n$$Wh_{T_{0,1}} = Wh - \\sum_{j \\geq 2} Wh_{T_j} = -\\sum_{j \\geq 2} Wh_{T_j}$$\n因此\n$$||Wh_{T_{0,1}}||_ 2^2 = -\\sum_{j \\geq 2} \\langle Wh_{T_{0,1}}, Wh_{T_j} \\rangle$$\nLemma：如果$W$是$(\\epsilon, 2s)$-RIP矩阵，对于任意不相交的$I, J$集合，若$|I| \\leq s, |J| \\leq s$，则 $$ \\langle W u_{I}, W u_{J} \\rangle \\leq \\epsilon |u_{I}| |u_{J}| $$\nPf.\n$$\\langle W u_{I}, W u_{J} \\rangle = \\frac{|W(u_I + u_J)|^2 - |W(u_I - u_J)|^2}{4}$$\n$$ \\leq \\frac{(1 + \\epsilon) |u_I + u_J|^2 - (1 - \\epsilon) |u_I - u_J|^2}{4} $$\n由于$I, J$是不相交的集合：\n$$ = \\frac{(1 + \\epsilon) (|u_I|^2 + |u_J|^2) - (1 - \\epsilon) (|u_I|^2 + |u_J|^2)}{4} $$\n$$ = \\frac{\\epsilon}{2} ((|u_I|^2 + |u_J|^2) \\leq \\epsilon |u_I||u_J|.\\blacksquare $$\n原式代入Lemma，我们有：\n$$||Wh_{T_{0,1}}||_ 2^2 \\leq \\epsilon (||h_{T_0}||_ 2 + ||h_{T_{1}}||_ 2) \\cdot \\sum_{j \\geq 2} ||h_{T_j}||_ 2 $$ 利用$2(a^2 + b^2) \\geq (a + b)^2$: $$||h_{T_0}||_ 2 + ||h_{T_1}||_ 2 \\leq \\sqrt{2} ||h_ {T_{0,1}}|| _2$$\n所以\n$$ |Wh_{T_{0,1}}|_ 2^2 \\leq \\sqrt{2} \\epsilon |h_{T_{0,1}}| _ 2 \\cdot \\sum_{j \\geq 2} |h_{T_j}|_ 2 $$\n$$ \\leq \\sqrt{2} \\epsilon \\cdot s^{-1/2} |h_{T_ {0,1}}| _ 2 \\cdot |h _{T _{0,1}^C}| _1 $$\n因此\n$$ |h_{T_0,1}|_ 2 \\leq \\frac{\\sqrt{2} \\epsilon}{1 - \\epsilon} s^{-1/2} |h_{T_0^C}|_1 $$\n$$ |h_{T_0,1}|_ 2 \\leq \\frac{\\sqrt{2} \\epsilon}{1 - \\epsilon} s^{-1/2} (|h_{T_0}|_ 1 + 2|x_{T_0^C}|_1) $$\n$$ \\leq \\rho ||h_ {T_{0}}|| _{2} + 2 \\rho s^{-1/2} ||x _{T _{0}^{C}}|| _{1} $$\n由于\n$$||h_{T_ {1}}|| _ 2 \\leq ||h_{T _{0,1}}|| _2$$\n因此\n$$ ||h_{T_{0,1}}||_2 \\leq \\frac{2 \\rho}{1 - \\rho} s^{-1/2} ||x - x_s||_1\\blacksquare $$\n回到Thm.3的证明:\n$$ |h|_ 2 \\leq |h_{T _{0,1}}| _2 + |h _{T _{0,1}^C}| _2 $$\n$$ \\leq 2 |h_{T_0,1}|_2 + 2s^{-1/2} |x - x_s|_1 $$\n$$ \\leq \\left( \\frac{4 \\rho}{1 - \\rho} s^{-1/2} + 2s^{-1/2} \\right) |x - x_s|_1 $$\n$$ = 2 \\frac{1 + \\rho}{1 - \\rho} s^{-1/2} |x - x_s|_1. \\blacksquare $$\nThm.4 最后我们就剩下Thm.4了，\nThm.4\n设 $U$ 为任意固定的 $d \\times d$ 正交矩阵，设 $\\epsilon, \\delta$ 为在 $(0, 1)$ 之间的标量，设 $s$ 是 $[d]$ 中的一个整数，且设 $n$ 为满足以下条件的整数 $$ n \\geq 100 \\frac{s \\ln(40d/(\\delta \\epsilon))}{\\epsilon^2}.$$ 设 $W \\in \\mathbb{R}^{n, d}$ 为一个矩阵，其每个元素均以零均值和方差 $1/n$ 正态分布。则，对于至少 $1 - \\delta$ 的概率而言，矩阵 $WU$ 是 $(\\epsilon, s)$-RIP。\n这里的常数项可能有一些问题，证明也比较复杂，这里就不展开了。大体的Proof Sketch是:\n将连续空间映射到有限个点上\n考虑一个特定的大小为 $s$的索引集 $I$\n使用这个索引集进入稀疏空间\n对所有可能的 $I$ 应用union bound 具体可以参考Shai Shalev-Shwartz的paper: Compressed Sensing: Basic results and self contained proofs*.\n4. 后记 “The people who are crazy enough to think they can change the world, are the ones who do.”\n期中之前的内容大概是这些。在写作的过程中，我发现我往往会忽略一些我不那么感兴趣的部分而只是去写自认为有趣的部分，这一点亦如我的复习，其中植入了太多的个人理解而忽视掉了老师或者学界主流想让人关注的框架，形成的Map of Machine Learning World自然也会是不同的。这大抵也能解释考试为什么会寄的一部分原因吧。后半学期争取让自己学会的东西的分布和课上的分布接近一些，或者搞一个generative model,从自己的分布里采样，经过一些变换能够接近他的分布吧。 ","permalink":"http://localhost:1313/blog/posts/ml1/","summary":"This is the first article in the Machine Learning Series. It covers the basics of optimization(GD,SGD,SVRG,Mirror Descent,Linear Coupling), generalization(No Free Lunch, PAC Learning, VC Dimension), and supervised learning(Linear Regression, Logistic Regression, Compressed Sensing).","title":"Machine Learning Series: 1.Optimization, Generalization and Supervised Learning"},{"content":"之前忘记传复变的笔记了。现在补一下，链接如下：https://cloud.tsinghua.edu.cn/f/0e002cc2dca948b7824d/\nCredits: Lectures by 姚国武，24春季学期。\n至于为什么突然想到了，主要是因为General Physics讲到了复势，然后讲的可以说是不敢恭维，遂补充之。\n动机 首先，引入复变函数来描述电场和电势的动机是单纯的。 在我们已知电荷分布的情况下，通过电荷的积分就可以获得空间每一良定义的点的电势和电场 ： $$ \\phi(1)=\\int_{all space}\\frac{\\rho(2) dV_2}{4\\pi \\epsilon_0 r_{12}} $$ $$E=-\\nabla \\phi$$ 或者写成多级展开的形式： $$\\phi(\\mathbf{r}) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{n=0}^{\\infty} \\frac{1}{r^{n+1}} \\int (r\u0026rsquo;)^n P_n(\\cos\\alpha) \\rho(r\u0026rsquo;) , d\\tau' $$ 但问题是很多时候我们不知道全空间明确的电荷分布 ，比如有导体或者insulator的情况，我们不直接知道电荷在其中的分布，但是会有一些边界条件(Boundary Condition)，比如说导体构成一个等势体之类的。\nUniqueness Theorem\n在体积 $V$ 内，对于已知的电荷分布 $\\rho(\\mathbf{x})$，如果在 $V$ 的封闭边界面 $S$ 上：\n给定电势 $\\phi|_S$ （Dirichlet 边界条件） 或给定电势的法向导数 $\\left.\\frac{\\partial \\phi}{\\partial n}\\right|_S$ （Neumann 边界条件） 那么体积 $V$ 内的电场是唯一确定的。\n这里，一种很巧妙的方式是根据 Uniqueness theorem 做 镜像电荷(Image Charge)，即把做镜像电荷的区域之外的区域看成$V$，然后这部分的电荷分布没变，只要保证边界条件不变就有电场的等价性了。这从数学上和物理上都很有美感，但是并不是所有的情况都有明确的镜像电荷分布与之对应，所以我们也需要一些别的手段。\n注意到，其实我们需要解的问题的通用方法其实是解这样的一组方程:\nPoisson方程 $$\\nabla^2 \\phi=-\\frac{\\rho}{\\epsilon_0}$$ 边界条件 e.g. $$\\phi|_S=0$$ 在一维,这是个ODE,很容易。 从二维开始,这变成了一个PDE，不好找解析解，很多时候只能求助于数值方法。 所以，有性质好的函数满足这样的方程，无疑是一件好事。复变函数（准确来说全纯函数）就是这样一种特殊情况： 二维情况下Poisson方程取$\\nabla^2 \\phi=0$。\n但是这里有一个tricky的点，也就是说之前我们是尝试通过Poisson方程和边界条件解出对应的电场和电势，但这里干的一件事情是相反的： 即我们先给出一个复变函数，然后尝试构造以它的实部和虚部分别对应电场和电势的电荷分布 。\n复变小tips ok，插播一点复变知识：\n$$f: \\mathbb{C}\\rightarrow\\mathbb{C}$$ $$z=x+yi, f(z)=u+vi$$ 复变函数是一个从复数域到复数域的映射(或者理解成$\\mathbb{R}^2$到$\\mathbb{R}^2$，运算有Hermite性的映射)。\n1. 导数 定义：设 $w = f(z)$ 在 $D$ 上有定义，$z_0 = x + iy \\in D$，若\n$$ \\lim_{\\Delta z \\to 0} \\frac{f(z_0 + \\Delta z) - f(z_0)}{\\Delta z} = A \\in \\mathbb{C} $$\n（$\\Delta z = \\Delta x + i \\Delta y$）\n则 $f(z)$ 在 $z_0$ 可导，$A$ 称为 $f(z)$ 在 $z_0$ 处的导数，即 $A = f\u0026rsquo;(z_0) = \\frac{df}{dz}\\bigg|_{z=z_0}$。\n2. 可微与微分 定义：若 $f(z)$ 在 $z_0 = x_0 + iy_0$ 可微，且 $f(z)$ 在 $z_0$ 的某个邻域 $B_\\rho(z_0)$ 内有表达式\n$$ \\Delta f = f(z_0 + \\Delta z) - f(z_0) = A \\cdot \\Delta z + \\rho(\\Delta z) \\cdot \\Delta z $$\n其中 $A \\in \\mathbb{C}, \\lim_{\\Delta z \\to 0} \\rho(\\Delta z) = 0$。\n若 $A \\cdot \\Delta z$ 称作 $f(z)$ 在 $z_0$ 处的微分，记作 $$ df = A \\cdot \\Delta z \\quad (= A \\cdot dz) $$ Remark: 可导$\\Leftrightarrow$可微 3. 解析函数 定义：$z_0 \\in \\mathbb{C}$，若 $w = f(z)$ 在 $z_0$ 的某个邻域 $B_\\rho(z_0)$ 内处处可导，则称 $f(z)$ 在 $z_0$ 解析，$z_0$ 称为 $f(z)$ 的一个解析点。否则称 $f(z)$ 在 $z_0$ 不解析，$z_0$ 称为 $f(z)$ 的奇点。\n若 $f(z)$ 在 $z_0$ 解析 $\\Rightarrow f(z)$ 在整个 $B_\\rho(z_0)$ 上解析。\n注：解析是开集性定义。\n4. 函数可导(解析)的充要条件 令$f\u0026rsquo;(z)=\\alpha +i \\beta$, 那么 $$ \\Delta f(z)= f\u0026rsquo;(z) dz + \\rho(\\Delta z) dz $$\n$$ = (\\alpha + i\\beta) (\\Delta x + i\\Delta y) + (\\rho_1 + i\\rho_2)(\\Delta x + i\\Delta y) $$\n$$ = \\alpha \\Delta x - \\beta \\Delta y + \\rho_1 \\Delta x - \\rho_2 \\Delta y $$\n$$+ i (\\beta \\Delta x + \\alpha \\Delta y + \\rho_2 \\Delta x + \\rho_1 \\Delta y)$$\n因此：\n$$ \\Delta u = \\alpha \\Delta x - \\beta \\Delta y + \\rho_1 \\Delta x - \\rho_2 \\Delta y $$\n$$ \\Delta v = \\beta \\Delta x + \\alpha \\Delta y + \\rho_2 \\Delta x + \\rho_1 \\Delta y $$ 这能给我们3个结论：\nI. $$ u, v \\text{在} (x, y) \\text{上可微} $$ II. Cauchy-Riemann 方程 $$\\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y} $$ $$\\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x}$$\nIII.导数公式 $$ \\quad f\u0026rsquo;(z) = \\alpha + i\\beta = \\frac{\\partial u}{\\partial x} - i\\frac{\\partial u}{\\partial y} =\\frac{\\partial u}{\\partial x} + i\\frac{\\partial v}{\\partial x} = \\frac{\\partial v}{\\partial y} + i\\frac{\\partial v}{\\partial x} = \\frac{\\partial v}{\\partial y} - i\\frac{\\partial u}{\\partial y} $$ Remark: 注意到C-R方程蕴含$\\nabla^2u=\\nabla^2v=0$,即$u,v$满足Laplace方程\n定理 1：\n若 $w = f(z)$ 在 $z = x + iy$ 可导的充要条件是 $u, v$ 在 $(x, y)$ 点可微且满足 Cauchy-Riemann 方程。\n定理 2：\n若 $w = f(z) = u + iv$ 在 $D$ 上解析的充要条件是 $u, v$ 在 $D$ 上可微且处处满足 Cauchy-Riemann 方程。\n5. 形式导数 $z=x+iy, \\bar{z}=x-iy$ $x,y\\rightarrow z,\\bar{z}$是一组换基的线性变换，注意到：\n$f(z)$在$D$上解析$\\Leftrightarrow \\frac{\\partial{f(z)}}{\\partial(\\bar{z})}=0$ 这是另一种理解解析性的方式。\nPlug it in 所以逻辑上就是这样，在很多真实场景我们关心的是$\\nabla^2 \\phi=0$的情况，假如说存在$z$轴上的空间对称性或者其他方式保证$\\frac{\\partial^2\\phi}{\\partial z^2}=0$, 那么剩下的$$ \\frac{\\partial^2\\phi}{\\partial x^2}+\\frac{\\partial^2\\phi}{\\partial y^2}=0$$ 就是Laplace方程，且$u,v$满足这种性质。\n同时，注意到C-R方程保证$$ \\frac{\\partial U}{\\partial x} \\frac{\\partial V}{\\partial x} + \\frac{\\partial U}{\\partial y} \\frac{\\partial V}{\\partial y} = 0$$ 所以 $$ \\nabla U \\cdot \\nabla V = 0$$ 也就是说这两组线是正交的。所以，在实际问题中，可以一组线是电场线，一组线是等势线。\n比如说 $V$ 代表电势\n$$ \\mathbf{E} = -\\nabla V = -\\frac{\\partial V}{\\partial x} - i\\frac{\\partial V}{\\partial y} $$\n注意到在$\\mathbb{C}$上内积$\\mathbf{a}^T\\mathbf{b} = x_1x_2 + y_1y_2 = \\mathbf{R}e\\left[\\mathbf{z_1}\\overline{\\mathbf{z_2}}\\right]$\n所以\n$$ W = - \\int_A^B \\mathbf{E} \\cdot d\\mathbf{l} = \\int_A^B \\left(\\frac{\\partial V}{\\partial x} + i\\frac{\\partial V}{\\partial y} \\right){(dx-idy)} $$\n$$ = \\int_A^B \\frac{\\partial V}{\\partial x} dx + \\frac{\\partial V}{\\partial y} dy = \\int_A^B dV = V(B) - V(A) $$ V is the Potential function\nU is the Flux function $$ N = \\int_A^B \\mathbf{E} \\cdot d\\mathbf{S} = - \\int_A^B \\left( \\frac{\\partial V}{\\partial x} + i \\frac{\\partial V}{\\partial y} \\right) (-dy - idx) $$\n$$ = \\int_A^B \\left( \\frac{\\partial V}{\\partial x} dy - \\frac{\\partial V}{\\partial y} dx \\right) = \\int_A^B \\left( \\frac{\\partial U}{\\partial y} dy + \\frac{\\partial U}{\\partial x} dx \\right) $$\n$$ = U(B) - U(A) $$\n同时，注意到 $$ E = - \\frac{\\partial V}{\\partial x} - i \\frac{\\partial V}{\\partial y} = - \\frac{\\partial V}{\\partial x} - i \\frac{\\partial U}{\\partial x} = (-i) \\overline{f\u0026rsquo;(z)} $$ 所以有 $$ f\u0026rsquo;(z) = (-i) \\overline{E} $$\n举个栗子 最后看几个例子吧\n1. POC: 无穷长直导线 对于具有线电荷密度$\\lambda$的无限长直线，\n$$ E = \\frac{\\lambda}{2\\pi\\varepsilon_0 r^2}r $$\n这是一个二维问题，\n$$ E = \\frac{\\lambda z}{2\\pi \\varepsilon_0 z \\overline{z}} = \\frac{\\lambda}{2\\pi \\varepsilon_0 \\overline{z}} $$\n$$ f\u0026rsquo;(z) = (-i) \\frac{\\lambda}{2\\pi \\varepsilon_0 z} $$ 所以 $$ f(z) = (-i) \\frac{\\lambda}{2\\pi \\varepsilon_0} \\ln z $$\n$$ = (-i) \\frac{\\lambda}{2 \\pi \\varepsilon_0} \\ln (r e^{i\\theta}) $$\n$$ = \\frac{\\lambda}{2 \\pi \\varepsilon_0} \\theta - i \\frac{\\lambda}{2 \\pi \\varepsilon_0} \\ln r $$\n其中：\n$$ U = \\frac{\\lambda}{2 \\pi \\varepsilon_0} \\theta \\quad \\text{(Flux-line)} $$\n$$ V = - \\frac{\\lambda}{2 \\pi \\varepsilon_0} \\ln r \\quad \\text{(Potential line)} $$ 2. 更多美丽的图 References: [1] The Feynman Lectures on Physics Vol.2 [2] Introduction to ElectroDynamics J. Griffiths\n[3] Slides from General Physics Prof. Luyan Sun\n*注：笔者在之前的文章中曾说大抵是拿不到复变的4.0的，最后托老师的仁慈（应是调分幅度不小）还是拿到了。天下还是好人多呐🐶。\n有时会去想，姚班的课程中没有设置概率论、复变函数的课程，在讲到相关的内容是又往往会出现 a. 默认大家都会 b.默认大家都不会重新讲的情况(比如back-propogation已经在4门课中听过了)。或许，课业学习中的速度与系统性，类似于RL中的exploration-exploitation, Generalization Theory中的inductive bias-function complexity，铸定是一种tradeoff吧。\n","permalink":"http://localhost:1313/blog/posts/blogpost/","summary":"This blog post is a summary of my notes on complex analysis and its application in electrostatics.","title":"Complex Analysis and its application in Electrostatics"},{"content":"A Brief Introduction This semester, I was fortunate enough to be enrolled in the NLP course offered by Prof. Tianxing He at IIIS, Tsinghua University. I find this course and it\u0026rsquo;s related topics quite intriguing. This course combines both theory and technology, with intuition and tricks being equally prominent. As a result, I decided to organize my notes to outline the key points in hopes that it will help deepen my understanding.\nPart A: Classical Methods PART A lists some classic natural language processing methods that predate the era of LLMs. It mainly includes CFG, LSA, HMM, N-Gram, and Word2Vec. Although these techniques may seem somewhat outdated today, they still possess a certain elegance and may even come back to haunt you. The use of CFG Parsing in Zeyuan Zhu\u0026rsquo;s Physics of Language Models series of papers is a great example.\nThe lecture notes are typed in Typst, and I really recommend it for those who are interested in typing notes.\n","permalink":"http://localhost:1313/blog/posts/nlp1/","summary":"This is the first part of the Natural Language Processing Series. It covers classical methods in natural language processing, including CFG, LSA, HMM, N-gram, Word2Vec, etc.","title":"Natural Language Processing: Part A. Classical Methods"}]