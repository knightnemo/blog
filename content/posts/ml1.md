---
title: "Machine Learning Series: 1.Optimization, Generalization and Supervised Learning"
date: 2024-11-09
draft: false
ShowToc: true
tags: ["machine-learning", "computer-science", "optimization", "math", "artificial-intelligence"]
summary: "This is the first article in the Machine Learning Series. It covers the basics of optimization(GD,SGD,SVRG,Mirror Descent,Linear Coupling), generalization(No Free Lunch, PAC Learning, VC Dimension), and supervised learning(Linear Regression, Logistic Regression, Compressed Sensing)."
---

# 0.é¥­åç”œå“ï¼Œä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·
> _Everything should be made as simple as possible, but not simpler._ 
<br><div style="text-align: right"> Albert Einstein.</div>

è®°å¾—é«˜ä¸‰çš„æ—¶å€™å†™è¿‡ä¸€ç¯‡ä½œæ–‡ï¼Œæ–‡ç« çš„ç«‹æ„å¤§æ¦‚æ˜¯ __â€œæ•´é¡¿æ——é¼“å†å‡ºå‘â€__ ã€‚æ˜¯å•Šï¼Œå¤šå°‘æ¬¡ï¼Œæˆ‘ä»¬å¥‹åŠ›ç‹‚å¥”ï¼Œè¿æ¥ç€ç‹‚é£éª¤é›¨çš„æ•²æ‰“ï¼Œå´ä¸æ„¿æ„æ”¾æ…¢è„šæ­¥ï¼Œä»å¯¹æœªæ¥ä¸ç¡®å®šæ€§çš„ç„¦è™‘ä¹‹ä¸­è·³è„±å‡ºæ¥ï¼Œçœ‹çœ‹è‡ªå·±çš„æ¥æ—¶è·¯ï¼Œçœ‹çœ‹æ˜¨æ—¥ä¹‹æˆ‘ã€ä»Šæ—¥ä¹‹æˆ‘ã€‚åœ¨å¿™å¿™å¨å¨ä¹‹ä¸­æ—¶å…‰ä¾¿æµé€æ‰äº†ï¼Œæœ‰æ—¶ä¸å¦¨åšç‚¹ __reflection__,æ•´ç†ä¸€ä¸‹æ‚ä¹±çš„æ€ç»ªå’Œæ²¡æƒ³æ˜ç™½çš„é—®é¢˜ã€‚

__å¦ä¸€ä¸ªè½åœ¨å®å¤„çš„åŠ¨æœºæ˜¯æˆ‘å‘ç°æˆ‘å­¦ä¸œè¥¿æœ‰ä¸ªç‰¹ç‚¹ï¼Œå°±æ˜¯å¿˜ä¸œè¥¿å¾ˆå¿«ã€‚å¦‚æœä¸ç•™ä¸‹ç‚¹ä¸œè¥¿å‘¢ï¼Œä¼šå¿˜ï¼Œç„¶åå¿˜äº†æ²¡æœ‰ç¬”è®°åˆå¾ˆéš¾æ¡èµ·æ¥ã€‚__ æ‰€ä»¥æˆ‘æƒ³ï¼Œä¸ºä»€ä¹ˆä¸åœ¨è‡ªå·±å¯¹è¿™ä¸ªé¢†åŸŸçš„å†…å®¹è®¤è¯†æœ€æ·±åˆ»çš„æ—¶å€™ç•™ä¸‹ç‚¹è®°å¿†ï¼Œå¯„å¸Œæœ›äºæœªæ¥çš„è‡ªå·±æˆ–è€…æˆ–è®¸å¯¹æœºå™¨å­¦ä¹ æœ‰å…´è¶£çš„è¯»è€…èƒ½å¤Ÿé€šè¿‡ä»Šæ—¥çš„ä¸€ç¯‡æ–‡ç« äº†è§£ä¸€äº›ä»Šæ—¥ä¹‹æˆ‘æ‰€æ€æ‰€æƒ³çš„ä¸€äº›å†…å®¹å‘¢ï¼Œäºæ˜¯å°±è¯ç”Ÿäº†è¿™ç¯‡æ–‡ç« ã€‚

ä½†è¿™ä»¶äº‹æ€ä¹ˆçœ‹éƒ½è¿˜æ˜¯å¾ˆå‘†ï¼Œéƒ½è€ƒå®Œäº†ï¼Œç„¶ååœ¨å†™çš„è¿‡ç¨‹ä¸­è‚¯å®šåˆèƒ½å­¦åˆ°ç‚¹ä¸œè¥¿ã€‚ä¸€ä½æœ‹å‹è·Ÿæˆ‘è¯´ __â€œé¥­åç”œå“ï¼Œä½ ä¸èƒ½æŒ‡æœ›è·Ÿæ­£é¤ä¸€èµ·â€__ ï¼Œäºæ˜¯æœ¬ç€ä¸€ä¸ªå“å‘³ç”œå“çš„é£Ÿå®¢çš„å¿ƒæ€ï¼Œæˆ‘å†³å®šå°†è¿™ç¯‡æ–‡ç« å°½é‡å†™çš„è½»é‡åŒ–ä¸€ç‚¹ã€æ•…äº‹æ€§å¼ºä¸€ç‚¹ï¼Œç©¿èµ·ä¸€ä¸ªæ€è€ƒçš„ä¸»çº¿ã€‚

# 1. Optimization
ä¼˜åŒ–é—®é¢˜è‡ªç„¶è€Œç„¶åœ°å‡ºç°åœ¨è®¸å¤šåº”ç”¨é¢†åŸŸä¸­ã€‚æ— è®ºäººä»¬åšä»€ä¹ˆï¼Œåœ¨æŸäº›æ—¶å€™ï¼Œä»–ä»¬éƒ½ä¼šäº§ç”Ÿä¸€ç§æƒ³è¦ä»¥æœ€ä½³æ–¹å¼ç»„ç»‡äº‹ç‰©çš„æ¸´æœ›ã€‚è¿™ç§æ„å›¾ï¼Œå½“è¢«è½¬æ¢æˆæ•°å­¦å½¢å¼æ—¶ï¼Œå°±ä¼šå˜æˆæŸç§ç±»å‹çš„ä¼˜åŒ–é—®é¢˜ã€‚ä¸‹é¢ä»‹ç»å‡ ç§ä¼˜åŒ–ç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š*Gradient Descent*, *Stochastic Gradient Descent*,  *SVRG*, *Mirror Desent*, *Linear Coupling*.

## 1.1 L-Smooth & Convex

åœ¨ä¼˜åŒ–å‡½æ•°çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¾€å¾€éœ€è¦ä¸€äº›æœ‰å…³å‡½æ•°æ€§è´¨çš„ä¿éšœï¼Œæ‰èƒ½å¤Ÿç¡®ä¿ä»–æœ‰å¥½çš„æ”¶æ•›ç‡ã€‚

### L-smooth
ä»¥ä¸‹ä¸‰æ¡ç­‰ä»·ï¼š
- $f(x) \leq f(x_0) + \langle \nabla f(x_0), x-x_0 \rangle + \frac{L}{2}||x-x_0||^2$

- $|\lambda_{\nabla^2 f(x)}| \leq L$

- $||\nabla f(x) - \nabla f(y)|| \leq L||x-y||$

æ³¨æ„åˆ°L-smoothå…¶å®å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯æ¢¯åº¦å˜åŒ–ä¸ä¼šå¤ªå¿«ï¼Œå¦å¤–ä¸€ä¸ªæœ‰è¶£çš„çœ‹æ³•æ˜¯ï¼š
- Upper Bound:
  $f(x) \leq f(x_0) + \langle \nabla f(x_0), x-x_0 \rangle + \frac{L}{2}||x-x_0||^2$
- Lower Bound:
  $f(x) \geq f(x_0) + \langle \nabla f(x_0), x-x_0 \rangle - \frac{L}{2}||x-x_0||^2$
  
ä¹Ÿå°±æ˜¯è¯´ç»™å®šä¸€ä¸ªç‚¹$f(x_0)$çš„é›¶é˜¶å’Œä¸€é˜¶ä¿¡æ¯ï¼Œæˆ‘ä»¬å°±å¯ä»¥è·å¾—åˆ«çš„ç‚¹çš„å‡½æ•°å€¼çš„ä¸€ä¸ªäºŒæ¬¡å‹çš„ä¸Šä¸‹ç•Œã€‚
![figure1](../img/ml1/image.png)

### Convex
ä»¥ä¸‹å››æ¡ç­‰ä»·ï¼š

- $ f(x) \geq f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle $
- $ f(x) \leq f(x_0) + \langle \nabla f(x), x - x_0 \rangle $  
- $ \lambda_{\min}(\nabla^2 f(x)) \geq 0 $  
-  $
\frac{1}{T} \sum_{i=1}^{T} f(x_i) \geq f(\bar{x}), \quad \bar{x} = \frac{1}{T} \sum_{i=1}^{T} x_i
$

### $\mu$-strongly Convex
ä»¥ä¸‹ä¸‰æ¡ç­‰ä»·ï¼š
-  $
f(x) \geq f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle + \frac{\mu}{2} \|x - x_0\|^2
$  
- $ \lambda_{\min}(\nabla^2 f(x)) \geq \mu $
- $
\|\nabla f(x) - \nabla f(y)\| \geq \mu \|x - y\|
$

### Convex & L-Smooth:
åœ¨ä¸€ä¸ªå‡½æ•°åˆconvexåˆL-Smoothçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šæœ‰ä¸€äº›æ›´å¥½çš„æ€§è´¨ï¼š
> **Thm.1** $$
 f(y) - f(x) - \langle \nabla f(x), y - x \rangle \geq \frac{1}{2L} \|\nabla f(x) - \nabla f(y)\|^2 $$

è¯æ˜å¦‚ä¸‹: 

ä»¤
$h(y) = f(y) - f(x) - \langle \nabla f(x), y - x \rangle$

æ³¨æ„åˆ°
$$
\nabla h(y) = \nabla f(y) - \nabla f(x) 
$$
$$
\nabla^2 h(y) = \nabla^2 f(y) 
$$
æ‰€ä»¥è¯´$h(y)$ä¹Ÿæ˜¯convexä¸”L-smoothçš„ï¼Œè€Œä¸”æœ€å°å€¼ç‚¹åœ¨$y=x$å¤„å–çš„ã€‚
æ‰€ä»¥,  
$$
h(x) \leq h(y - \frac{1}{L} \nabla h(y))\\
$$
$$\leq h(y) - \frac{1}{L} \|\nabla h(y)\|^2 + \frac{1}{2L} \|\nabla h(y)\|^2
$$
$$
=h(y) - \frac{1}{2L} \|\nabla h(y)\|^2 
$$

å› æ­¤,  
$$
f(y) - f(x) - \langle \nabla f(x), y - x \rangle \geq \frac{1}{2L} \|\nabla f(y)-\nabla f(x)\|^2 
$$
> **Thm.2** 
$$
\langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \frac{1}{L} \|\nabla f(x) - \nabla f(y)\|^2$$

è¿™ä¸ªçš„è¯æ˜å¯ä»¥ç”±Thm.1äº¤æ¢$x,y$æ¬¡åºä¹‹åç›¸åŠ å¾—åˆ°ã€‚
## 1.2 Gradient Descent
GDçš„update ruleå¦‚ä¸‹:
$$x_{t+1}=x_{t}-\eta \nabla f(x_t)$$
åœ¨ä»¥ä¸‹ä¸‰ç§æƒ…å†µä¸‹ï¼Œåˆ†åˆ«æœ‰ä¸åŒçš„æ”¶æ•›ç‡ï¼š
### Convex, L-Smooth
$$
x_{t+1} = x_t - \eta \nabla f(x_t)
$$

$$
f(x_{t+1}) \leq f(x_t) + \langle \nabla f(x_t), x_{t+1} - x_t \rangle + \frac{L}{2} \|x_{t+1} - x_t\|^2
$$

$$
= f(x_t) - \eta \|\nabla f(x_t)\|^2 - \frac{L \eta^2}{2} \|\nabla f(x_t)\|^2
$$

å–$\eta \leq \frac{1}{L}$:

$$ f(x_{t+1}) \leq f(x_t) - \frac{\eta}{2} \|\nabla f(x_t)\|^2 $$

ç”±convexity:

$$ 
f(x_{t+1}) \leq f(x^\*) + \langle \nabla f(x_t), x_t - x^\* \rangle - \frac{\eta}{2} \|\nabla f(x_t)\|^2
$$

$$
= f(x^\*) - \frac{1}{\eta} \langle x_{t+1} - x_t, x_t - x^\* \rangle - \frac{1}{2\eta} \|x_{t+1} - x_t\|^2
$$

$$
= f(x^\*) - \frac{1}{2\eta} \|x_{t+1} - x^\*\|^2 + \frac{1}{2\eta} \|x_t - x^\*\|^2
$$

æ¥ä¸‹æ¥æˆ‘ä»¬åštelescope:

$$
\sum_{t=0}^{T-1} (f(x_{t+1}) - f(x^\*)) \leq \frac{1}{2\eta} (\|x_0 - x^\*\|^2 - \|x_T - x^\*\|^2)
$$

å› ä¸º$f(x_t)$æ˜¯å•è°ƒé€’å‡çš„(convexä¿è¯)

$$ f(x_T) - f(x^\*) \leq \frac{1}{2\eta T} \|x_0 - x^\*\|^2 = \epsilon $$
æ‰€ä»¥è¯´
$$ T = \frac{\|x_0 - x^\*\|^2}{2\eta \epsilon} = O\left(\frac{L}{\epsilon}\right) $$
åœ¨è¿™ç§æƒ…å†µä¸‹éœ€è¦è¿­ä»£$O(\frac{1}{\epsilon})$æ¬¡ï¼Œæ”¶æ•›ç‡ä¸º$O(\frac{1}{T})$.
### $\mu$-strongly Convex & L-smooth
è¿™é‡Œèµ·æ‰‹å¼æˆ‘ä»¬å¡$||x-x^\*||$:
$$
\|x_{t+1} - x^\*\|^2 = \|x_t - \eta \nabla f(x_t) - x^\*\|^2
$$

$$
= \|x_t - x^\*\|^2 - 2\eta \langle \nabla f(x_t), x_t - x^\* \rangle + \eta^2 \|\nabla f(x_t)\|^2
$$
å› ä¸ºå¼ºå‡¸æ€§ï¼š
$$
f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle + \frac{\mu}{2} \|y - x\|^2
$$

ä»£å…¥ $x = x_t$, $y = x^\*$:

$$
f(x^\*) \geq f(x_t) + \langle \nabla f(x_t), x^\* - x_t \rangle + \frac{\mu}{2} \|x_t - x^\*\|^2
$$

$$
\langle \nabla f(x_t), x_t - x^\* \rangle \geq f(x_t) - f(x^\*) + \frac{\mu}{2} \|x_t - x^\*\|^2
$$
æ‰€ä»¥
$$
\|x_{t+1} - x^\*\|^2 \leq \|x_t - x^\*\|^2 - 2\eta (f(x_t) - f(x^\*) + \frac{\mu}{2} \|x_t - x^\*\|^2ï¼‰ + \eta^2 \|\nabla f(x_t)\|^2
$$
æ ¹æ®ä¹‹å‰çš„Thm.1:
$$
\frac{1}{2L} \|\nabla f(x_t)\|^2 \leq f(x_t) - f(x^\*)
$$
æ‰€ä»¥
$$
\|x_{t+1} - x^\*\|^2 \leq (1 - \eta \mu) \|x_t - x^\*\|^2 + (2\eta^2 L - 2\eta )(f(x_t) - f(x^\*))
$$

å– $\eta = \frac{1}{L}$:

$$
\|x_{t+1} - x^\*\|^2 \leq (1 - \frac{\mu}{L}) \|x_t - x^\*\|^2
$$
æ‰€ä»¥è¯´Linear Convergence, åæ˜ åœ¨$f(x)$ä¸Š:
$$f(x_T)\leq f(x^\*)+\frac{L}{2}||x_T-x^\*||^2$$
$$\leq f(x^\*)+\frac{L}{2}(1 - \frac{\mu}{L})^T||x_0-x^\*||^2$$
ä¹Ÿå°±æ˜¯è¯´éœ€è¦è¿­ä»£æ¬¡æ•°$O(log(\frac{1}{\epsilon}))$, æ”¶æ•›ç‡ä¸ºLinear Convergence.
> *Remark*: <br>
  å¯¹äº$\mu$-strongly Convex & L-smoothçš„å‡½æ•°æœ‰å¦‚ä¸‹æ€§è´¨ï¼š$
  \frac{\mu}{2} \| \mathbf{x}^\* - \mathbf{x} \|^2 \leq f(\mathbf{x}) - f^\* \leq \frac{L}{2} \| \mathbf{x}^\* - \mathbf{x} \|^2
  $
  $
\frac{1}{2L} \| \nabla f(\mathbf{x}) \|^2 \leq f(\mathbf{x}) - f^\* \leq \frac{1}{2\mu} \| \nabla f(\mathbf{x}) \|^2
$æ ¹æ®è¿™äº›æ€§è´¨æœ‰ä¸€ä¸ªæ›´ä¸ºç®€æ´çš„è¯æ˜ã€‚

### L-Smooth
æ ¹æ®ç¬¬ä¸€ç§æƒ…å†µä¸‹çš„åˆ†æï¼š
$$
f(x_{t+1}) - f(x_t) \leq -\frac{\eta}{2} \|\nabla f(x_t)\|^2
$$

ç„¶ååšTelescope:

$$
\min_{k \in [T]} \|\nabla f(x_t)\|^2 \leq \frac{2L(f(x_0) - f(x^\*))}{T} = \epsilon^2
$$
æ‰€ä»¥è¯´å½“æˆ‘ä»¬æƒ³è·å¾—$\|\nabla f(x_t)\|^2<\epsilon$ï¼Œæˆ‘ä»¬éœ€è¦
$
T = O\left(\frac{1}{\epsilon^2}\right)
$çš„è¿­ä»£æ¬¡æ•°ï¼Œæ”¶æ•›ç‡ä¸º
$
O\left(\frac{1}{\sqrt{T}}\right)
$ã€‚
### Recap:
æ€»ç»“èµ·æ¥å¤§æ¦‚æ˜¯:
![table](../img/ml1/image2.png)
## 1.3 Stochastic Gradient Descent
### Why SGD
GDçœ‹èµ·æ¥ä¸é”™ï¼Œä½†æ˜¯æœ‰ä¸¤ä¸ªé—®é¢˜:
* è®¡ç®—ä¸€æ¬¡full gradientå¾ˆè´µ
* GDä¼šåœ¨local maximumå’Œsaddle pointï¼ˆéç‚¹ï¼‰å¡ä½

äºæ˜¯æˆ‘ä»¬å°±ä¼šå»æƒ³ï¼Œèƒ½ä¸èƒ½å°‘ç®—å‡ ä¸ªæ•°æ®ç‚¹å¯¹åº”çš„loss functionï¼ŒåŒæ—¶åˆèƒ½æœ‰ä¸€äº›convergence guaranteeå‘¢ï¼ŒSGDä¾¿æ˜¯è¿™æ ·çš„ä¸€ç§ç®—æ³•ã€‚

### Algorithm
SGDçš„update ruleå¦‚ä¸‹æ‰€ç¤º:
$$
x_{t+1} = x_t - \eta G_t, $$
å…¶ä¸­$G_t$æ»¡è¶³:
$$ \mathbb{E}[G_t] = \nabla f(x_t), \quad \text{Var}(G_t) \leq \sigma^2
$$
### Convergence
ä¸‹é¢æˆ‘ä»¬è¯æ˜SGDåœ¨L-Smooth, Convex, $\text{Var}(G_t) \leq \sigma^2$çš„æ¡ä»¶ä¸‹çš„æ”¶æ•›ç‡:

å› ä¸ºL-smooth:
$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) + \mathbb{E}[\langle \nabla f(x_t), x_{t+1} - x_t \rangle] + \frac{L}{2} \mathbb{E}[\|x_{t+1} - x_t\|^2]
$$

$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) - \eta \|\nabla f(x_t)\|^2 + \frac{L \eta^2}{2} \mathbb{E}[\|G_t\|^2]
$$

æ ¹æ®æ–¹å·®çš„å®šä¹‰ï¼š 
$$\mathbb{E}[ ||G_t||^2 ] = \text{Var}(G_t) + ||\mathbb{E}[G_t]||^2 \leq \sigma^2 + \|\nabla f(x_t)\|^2$$
æ‰€ä»¥æœ‰
$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) + \left(\frac{L \eta^2}{2} - \eta\right) \|\nabla f(x_t)\|^2 + \frac{L \eta^2}{2} \sigma^2
$$

å– $\eta = \frac{1}{L}$:

$$
\mathbb{E}[f(x_{t+1})] \leq f(x_t) - \frac{\eta}{2} \|\nabla f(x_t)\|^2 + \frac{\eta}{2} \sigma^2
$$

æ ¹æ®convexity:

$$
f(x_t) \leq f(x^\*) + \langle \nabla f(x_t), x_t - x^\* \rangle
$$

$$
\mathbb{E}[f(x_{t+1})] \leq f(x^\*) + \mathbb{E}[\langle G_t, x_t - x^\* \rangle] - \frac{\eta}{2} \|\nabla f(x_t)\|^2 + \frac{\eta}{2} \sigma^2
$$
åˆå› ä¸º
$$
\|\nabla f(x_t)\|^2 = \mathbb{E}[\|G_t\|^2] - \text{Var}(G_t) \geq \mathbb{E}[\|G_t\|^2] - \sigma^2
$$

æ‰€ä»¥
$$
\mathbb{E}[f(x_{t+1})] \leq f(x^\*) + \mathbb{E}[\langle G_t, x_t - x^\* \rangle - \frac{\eta}{2} \|G_t\|^2] + \eta \sigma^2
$$
æ³¨æ„åˆ°
$$
\langle G_t, x_t - x^\* \rangle - \frac{\eta}{2} \|G_t\|^2
$$

$$
= -\frac{1}{2\eta} \|(x_{t+1} - x_t) - (x^\* - x_t)\|^2 + \frac{1}{2\eta} \|x_t - x^\*\|^2
$$

$$
= \frac{1}{2\eta} (\|x_t - x^\*\|^2 - \|x_{t+1} - x^\*\|^2)
$$
ä¹Ÿå°±æ˜¯è¯´
$$
\mathbb{E}[f(x_{t+1})] \leq f(x^\*) + \frac{\eta}{2} \mathbb{E}[\|x_t - x^\*\|^2 - \|x_{t+1} - x^\*\|^2] + \eta \sigma^2
$$

ä» $t = 0$ åˆ° $T-1$æ±‚å’Œ(telescope):

$$
\frac{1}{T}\sum_{t=0}^{T-1} (\mathbb{E}[f(x_t)] - f(x^\*)) \leq \frac{1}{2\eta T} \|x_0 - x^\*\|^2 + \eta \sigma^2
$$

å– $\eta = \frac{\epsilon}{2\sigma^2} \leq \frac{1}{L}$, åˆ™æœ‰:

$$
T = \frac{2 \sigma^2 \|x_0 - x^\*\|^2}{\epsilon^2}
$$

Stochastic Gradient Descent (SGD) çš„æ”¶æ•›ç‡æ˜¯
$
O\left(\frac{1}{\sqrt{T}}\right)
$ã€‚
## 1.4 SVRG
æˆ‘ä»¬çœ‹åˆ°äº†é€šè¿‡Stochastic Gradientå¯ä»¥å‡å°‘computation cost,ä½†æ˜¯éšä¹‹è€Œæ¥çš„é—®é¢˜æ˜¯å› ä¸º $G_t$æ‹¥æœ‰çš„variance,å¯¼è‡´åŸæ¥$O(\frac{1}{T})$çš„convergence rateå˜æˆäº†$O(\frac{1}{\sqrt{T}})$,äºæ˜¯æˆ‘ä»¬å»æƒ³ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆåŠæ³•èƒ½å¤Ÿåœ¨ä¿æŒcomputation costæ¯”è¾ƒå°çš„æƒ…å†µä¸‹åŒæ—¶æŠŠvarianceé™ä¸‹æ¥ï¼ŒSVRGæ˜¯å…¶ä¸­çš„ä¸€ç§ç®—æ³•ï¼Œåœ¨strongly-convexå’Œl-smoothçš„æƒ…å†µä¸‹æœ€åèƒ½å¤Ÿè·å¾—å’ŒGDä¸€æ ·çš„convergence rateã€‚

### Algorithm
> ### Procedure SVRG
>  **Parameters**: update frequency $m$ and learning rate $\eta$  
  **Initialize** $\tilde{w}_0$  
  **Iterate**: for $s = 1, 2, \ldots$  
>  1. $\tilde{w} = \tilde{w}_{s-1}$  
>  2. $\tilde{\mu} = \frac{1}{n} \sum_{i=1}^{n} \nabla l_i(\tilde{w})$  
>  3. $w_0 = \tilde{w}$  
     **Iterate**: for $t = 1, 2, \ldots, m$  
     i. Randomly pick $i_t \in \{1, \ldots, n\}$ and update weight  
       $
       w_t = w_{t-1} - \eta \left( \nabla l_{i_t}(w_{t-1}) - \nabla l_{i_t}(\tilde{w}) + \tilde{\mu} \right)
       $
     **end**  
     **Option I**: set $\tilde{w}_s = w_m$  
     **Option II**: set $\tilde{w}_s = w_t$ for randomly chosen $t \in \{0, \ldots, m - 1\}$  
  **end**

### Convergence Rate
* å‰æå‡è®¾ï¼š

  L-smooth, $l_i$: convex, $f$: strong-convex  

* Bound $\mathbb{E}[||v_t||^2]$:

ä»¤  $v_t = \nabla l_i(w_{t-1}) - \nabla l_i(\tilde{w}) + \tilde{u}$

$\mathbb{E}[||v_t||^{2}] = \mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(\tilde{w}) + \tilde{u})^2]$

å› ä¸º$  (a+b)^2 \leq 2a^2 + 2b^2 $ï¼š

$\leq 2\mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(w^\*))^2] + 2\mathbb{E}[(\nabla l_i(w^\*) - \nabla l_i(\tilde{w}) + \tilde{u})^2]$



$= 2\mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(w^\*))^2] $

$+ 2\mathbb{E}[\left((\nabla l_i(w^\*) - \nabla l_i(\tilde{w}))-\mathbb{E}[(\nabla l_i(w^\*) - \nabla l_i(\tilde{w}))]\right)^2]$

åˆå› ä¸º$$\mathbb{E}[(x - \mathbb{E}[x])^2] = \mathbb{E}[x^2] - (\mathbb{E}[x])^2 \leq \mathbb{E}[x^2]:$$

æ‰€ä»¥$\mathbb{E}[||v_t||^{2}]$

$\leq 2\mathbb{E}[(\nabla l_i(w_{t-1}) - \nabla l_i(w^\*))^2] + 2\mathbb{E}[(\nabla l_i(w^\*) - \nabla l_i(\tilde{w}))^2]$

æ ¹æ®Thm.1:

$\leq 4L(f(w_{t-1}) - f(w^\*) + f(\tilde{w}) - f(w^\*))$
* Bound $||w_t-w^\*||$
$$
\mathbb{E}[\|w_{t} - w^\*\|^2] = \mathbb{E}[\|w_t - w_{t-1} + w_{t-1} - w^\*\|^2]
$$

$$
= \mathbb{E}[\|w_{t} - w^\*\|^2] + 2 \mathbb{E}[\langle w_t - w_{t-1}, w_{t-1} - w^\* \rangle] + \mathbb{E}[\|w_t - w_{t-1}\|^2]
$$

$$
= \|w_{t-1} - w^\*\|^2 - 2\eta \mathbb{E}[\langle v_t, w_{t-1} - w^\* \rangle] + \eta^2 \mathbb{E}[v_t^2]
$$

$$
\leq \|w_{t-1} - w^\*\|^2 - 2\eta \mathbb{E}[\langle v_t, w_{t-1} - w^\* \rangle] + 4\eta^2 L(f(w_{t-1}) - f(w^\*) + f(\tilde{w}) - f(w^\*))
$$

$$
= \|w_{t} - w^\*\|^2 - 2\eta \langle \nabla f(w_{t-1}), w_{t+1} - w^\* \rangle + 4L\eta^2 (f(w_{t-1}) - f(w^\*) + f(\tilde{w}) - f(w^\*))
$$
åˆå› ä¸ºconvexityï¼š
$$
f(w_{t-1}) - f(w^\*) \geq \langle \nabla f(w_{t-1}), w_{t-1} - w^\* \rangle
$$

$$
\Rightarrow \mathbb{E}[\|w_{t} - w^\*\|^2] \leq \|w_{t-1} - w^\*\|^2 - 2\eta (f(w_{t-1}) - f(w^\*)) + 4L\eta^2 (f(w_{t-1}) - f(w^\*) + f(\tilde{w}) - f(w^\*))
$$

$$
= \|w_{t+1} - w^\*\|^2 + 4L\eta^2 (f(\tilde{w}) - f(w^\*)) + 2\eta (2L\eta - 1)(f(w_{t-1}) - f(w^\*))
$$
* Telescope

ä»$\sum_{t=1}^{m}$ï¼Œç”¨option 2:
$$
\mathbb{E}[\|w_m - w^\*\|^2] \leq \mathbb{E}[\|\tilde{w} - w^\*\|^2] + 4mL\eta^2 (f(\tilde{w}) - f(w^\*)) + 2m\eta (2L\eta - 1) \mathbb{E}[f(\tilde{w}_s) - f(w^\*)]
$$

é‡æ–°æ•´ç†æˆ:

$$
\mathbb{E}[\|w_m - w^\*\|^2] + 2m\eta  (1 - 2L\eta) \mathbb{E}[f(\tilde{w}_s) - f(w^\*)]
$$

$$
\leq \mathbb{E}[\|\tilde{w} - w^\*\|^2] + 4mL\eta^2 (f(\tilde{w}) - f(w^\*))
$$

$$
\leq \left(\frac{2}{u} + 4mL\eta^2\right)(f(\tilde{w}) - f(w^\*))
$$

æ‰€ä»¥

$$
\mathbb{E}[f(\tilde{w}_s) - f(w^\*)] \leq (\frac{1}{u\eta (1 - 2L\eta)m} + \frac{2L\eta}{1-2L\eta}) $$

$$\cdot \mathbb{E} [f(\tilde{w}_{s - 1})-f(w^\*)]$$

æ‰€ä»¥æ”¶æ•›ç‡æ˜¯Linear Convergence, $\frac{L}{u}$å¤§æ—¶æ¯”GDå¿«ã€‚ 

## 1.5 Mirror Descent


![](../img/ml1/image3.png)
### Algorithm
å¯¹äºä¸€ä¸ª1-strongly convexçš„Distance Generating Function$w(x)$,æˆ‘ä»¬å®šä¹‰Bergman Divergence:$$V_x(y)=w(y)-w(x)-\langle \nabla w(x),y-x \rangle$$
ç„¶åæˆ‘ä»¬å®šä¹‰:
$$\text{Mirror}_ {x}(\zeta) = \arg \min_ {y} \{ V_ {x}(y) + \langle \zeta, y - x \rangle \} $$

ä¸€ä¸ªMirror Descentçš„å®šä¹‰æ˜¯
$$
x_{t+1} = \text{Mirror}_ {x_t} (\alpha \nabla f(x_t))
$$

$$
= \arg \min_{y} \left( w(y) - w(x_t) - \langle \nabla w(x_t), y - x_t \rangle + \alpha \langle \nabla f(x_t), y - x_t \rangle \right)
$$

### Intuition
ç¬¬äºŒç§è§†è§’ç§°ä¸ºé•œåƒç©ºé—´ (Mirror space) è§†è§’ï¼Œä¸€ä¸ª Mirror step å¯ä»¥è¢«è§†ä½œå°†å¶ç©ºé—´ä¸Šçš„æ¢¯åº¦ä¸‹é™ï¼Œå³æœå¦ä¸€ä¸ªæ–°çš„æå€¼ç‚¹è¿›è¡Œæœç´¢ã€‚è¿‡ç¨‹å½¢å¦‚ï¼š

* å°† $x$ é€šè¿‡ Mirror map æ˜ å°„åˆ°å¯¹å¶ç©ºé—´ä¸Šçš„ $\theta_k$ã€‚
* $\theta_ {k+1} = \theta_ k - \alpha \nabla f(x_k)$ã€‚
* å°† $\theta_ {k+1}$ æ˜ å°„å›åŸç©ºé—´ä¸Šçš„ $\overline{x} _{k+1}$ã€‚
* å°† $\overline{x}_ {k+1}$ æŠ•å½±åˆ°çº¦æŸé›†ï¼ŒæŠ•å½±ä½¿ç”¨ Bregman divergence ä½œä¸ºå…¶è·ç¦»ï¼Œå³ $x_ {k+1} = \arg \min_ {y} V_ {x_{k+1}}(y)$ã€‚

æŒ‰ç…§ Mirror step çš„å¼å­ï¼Œå¯ä»¥çœ‹å‡º Mirror map å°±æ˜¯ $\nabla w(\cdot)$ã€‚å› æ­¤å®é™…è¿‡ç¨‹ä¸ºï¼š

* $\theta_k = \nabla w(x)$ã€‚
* $\theta_{k+1} = \theta_k - \alpha \nabla f(x_k)$ã€‚
* $\overline{x}_{k+1} = (\nabla w)^{-1}(\theta{k+1})$ã€‚
* $x_{k+1} = \arg \min_{y} V_{\overline{x}_{k+1}}(y)$ã€‚

è¿™ä¸ªè§†è§’æå‡ºäº†ä¸€ç‚¹å‡è®¾ï¼Œ$(\nabla w)^{-1}(\overline{x}_{k+1})$ å§‹ç»ˆå­˜åœ¨ï¼Œå³ ${\nabla w(x)} = \mathbb{R}^n$ã€‚
### Relationship between GD & MD

è¿™ä¸ªé—®é¢˜æ›¾å¾ˆé•¿ä¸€æ®µæ—¶é—´è®©ç¬”è€…æ„Ÿåˆ°å›°æƒ‘ã€‚ç¬”è€…å¯¹äºè¿™ä¸€å—å¹¶éå¾ˆæ‡‚ï¼Œç¬”è€…ç°åœ¨çš„ç†è§£æ˜¯:

æˆ‘ä»¬çŸ¥é“ä¸€ä¸ªPrimal Spaceå’ŒDual Spaceçš„èŒƒæ•°ä¹‹é—´æ»¡è¶³$\frac{1}{p}+\frac{1}{q}=1$

GDæ˜¯MDåœ¨ $\alpha=\frac{1}{L}$ï¼Œprimal spaceå–$||Â·||_2$èŒƒæ•°ï¼ŒDistance Generating Functionå– $w(x)=\frac{1}{2} x^2$ä¸‹çš„ç‰¹æ®Šæƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå› ä¸ºL2-normçš„Dualå°±æ˜¯L2-normï¼Œæ‰€ä»¥è¿™ä¸ªå¯¹å¶ç©ºé—´å°±æ˜¯åŸç©ºé—´ã€‚

ä½†æ˜¯å¦ä¸€ç§ç†è§£æ–¹å¼æ˜¯ï¼ŒMDæ˜¯å…ˆé€šè¿‡æ¢¯åº¦æ˜ å°„åˆ°Dual Spaceä¹‹ååœ¨è¿™ä¸ªç©ºé—´ä¸‹åšGDå†é€†æ˜ å°„åprojectå›åŸæ¥çš„ç©ºé—´ä¸­ã€‚

### Convergence:
* å‰ææ¡ä»¶:

$f(x)$ convex, $w(x)$ 1-strongly convex, $\nabla f(x)\leq \rho$
* Bound $f(x_t)-f(x^\*)$:

å› ä¸ºconvexityï¼š
$$
\alpha (f(x_{t+1}) - f(u)) \leq \langle \alpha \nabla f(x_t), x_t - u \rangle $$
åˆå› ä¸ºMDçš„æ›´æ–°è§„åˆ™ï¼š
$$
x_{t+1} = \arg \min_{y} \left( V_{x_t}(y) + \langle \alpha \nabla f(x_t), y - x_t \rangle \right)
$$
æ‰€ä»¥è¯´ç”±æœ€å°å€¼ç‚¹æ¢¯åº¦ç­‰äº0:
$$
\alpha \nabla f(x_t) = - \nabla V_{x_t}(x_{t+1})
$$
å› æ­¤
$$
\alpha (f(x_t) - f(u)) \leq \langle \alpha \nabla f(x_t), x_t - x_{k+1} \rangle + \langle - \nabla V_{x_t}(x_{k+1}), x_{k+1} - u \rangle
$$
æ¥ä¸‹æ¥æˆ‘ä»¬è¯æ˜ä¸€ä¸ªé‡è¦çš„triangle inequality:
$$
\langle - \nabla V_{x_t}(y), y - u \rangle = \langle \nabla w(x) - \nabla w(y), y - u \rangle
$$

---

$$
= (w(u) - w(x)) - \langle \nabla w(x), u - x \rangle - (w(y) - w(x) - \langle \nabla w(x), y - x \rangle)
$$

$$
= V_x(u) - V_x(y) - V_y(u)
$$
å¸¦å›åŸå¼:
$$
\alpha (f(x_t) - f(u))
\leq \langle \alpha \nabla f(x_t), x_t - x_{k+1} \rangle + V_{x_k}(u) - V_{x_k}(x_{k+1}) - V_{x_{k+1}}(u)
$$

ç”±äºDGFçš„1-strongly convex:

$$
\leq \langle \alpha \nabla f(x_t), x_t - x_{k+1} \rangle- \frac{1}{2} \|x_{k+1} - x_t\|^2 + V_{x_k}(u) - V_{x_{k+1}}(u)
$$
è¿™æ­¥æ˜¯å‰ä¸¤é¡¹åšä¸ªé…æ–¹æ³•:
$$
\leq \frac{\alpha^2}{2} \|\nabla f(x_t)\|^2 + V_{x_k}(u) - V_{x_k}(x_{k+1})
$$

* Telescoping:

$$
\alpha T (f(\overline{x}) - f(x_t)) \leq \sum \text{LHS} \leq \sum \text{RHS}
$$
$$
\leq \frac{\alpha^2 T}{2} \cdot \rho^2 + V_{x_0}(x^\*) - V_{x_T}(x^\*)
$$
æ‰€ä»¥è¯´
$$
f(\overline{x}) - f(x^\*) \leq \frac{\alpha}{2} \rho^2 + \frac{\Theta}{\alpha T} 
$$
ä»¤$\alpha = \sqrt{\frac{2\Theta}{T \rho^2}}$. 

æœ‰$f(x_T) - f(x^\*) \leq \sqrt{\frac{2\Theta}{T }}\rho= \epsilon$
äºæ˜¯æˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„æ”¶æ•›ç‡
$$
T = \Omega \left( \frac{\rho^2}{\epsilon^2} \right)
$$
## 1.6 Linear Coupling
### Wishful Thinking
æˆ‘ä»¬é€šè¿‡1.5çš„åˆ†æå·²ç»çŸ¥é“Mirror Descentæœ‰
$
T = O\left(\frac{\rho^2}{\epsilon^2}\right)
$çš„æ”¶æ•›ç‡

ç„¶åæˆ‘ä»¬çŸ¥é“åœ¨GDä¸­
$$
f(x_{t+1}) - f(x_t) \leq -\frac{1}{2L} \|\nabla f(x_t)\|^2
$$
æ‰€ä»¥è¯´åœ¨gradientæ¯”è¾ƒå¤§çš„æ—¶å€™:
$$
\|\nabla f(x_t)\| > \rho : \Omega\left(\frac{L \epsilon}{\rho^2}\right) \text{ steps}
$$

åœ¨gradientæ¯”è¾ƒå°çš„æ—¶å€™MD:

$$
\|\nabla f(x_t)\| < \rho : \Omega\left(\frac{\rho^2}{\epsilon^2}\right) \text{ steps}
$$
æ‰€ä»¥æˆ‘ä»¬æƒ³èƒ½ä¸èƒ½åœ¨æ¢¯åº¦å¤§çš„æ—¶å€™è·‘GDï¼Œåœ¨æ¢¯åº¦å°çš„æ—¶å€™è·‘MDï¼Œè¿™æ ·ä¼šè·å¾—ä¸€ä¸ªæ›´å¥½çš„æ”¶æ•›ç‡

Coupling:

$$\Omega ( \max \{ \frac{L \epsilon}{\rho^2}, \frac{\rho^2}{\epsilon^2} \})$$
å–$\rho = (L \epsilon^{3})^\frac{1}{4}$:
$$ \Omega\left(\sqrt{\frac{L}{\epsilon}}\right) \text{ steps}
$$
### Algorithm
* åˆå§‹åŒ–
$$x_0 = y_0 = z_0$$
* æ¯ä¸€æ­¥æ›´æ–°,æ›´æ–°$x$:
$$
x_{k+1} = \tau z_k + (1 - \tau) y_k
$$
* æ›´æ–°$y$:

$$
y_{k+1} = \arg \min_{y \in \mathcal{Q}} \{ \frac{L}{2} \|y - x_{k+1}\|^2 + \langle \nabla f(x_{k+1}), y - x_{k+1} \rangle \}
$$

$$
= x_{k+1} - \frac{1}{L} \nabla f(x_{k+1}) \quad \text{(GD step)}
$$
* æ›´æ–°$z$:
$$
z_{k+1} = Mirror_{z_k} (\alpha \nabla f(x_{k+1}))
$$
### Convergence
æ ¹æ®MDçš„åˆ†æ:
$$
\alpha \langle \nabla f(x_{k+1}), z_k - u \rangle \leq \frac{\alpha^2}{2} \|\nabla f(x_{k+1})\|^2 + V_{z_k}(u) - V_{z_{k+1}}(u)
$$
ç”±äº
$$
f(x_{k+1}) - f(y_{k+1}) \geq \frac{1}{2L} \|\nabla f(x_{k+1})\|^2$$
æ‰€ä»¥åŸå¼
$$ \leq \alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u)
$$
åˆå› ä¸ºconvexity:
$$
\alpha (f(x_{k+1}) - f(u)) \leq \alpha \langle \nabla f(x_{k+1}), x_{k+1} - u \rangle
$$

$$
= \alpha \langle \nabla f(x_{k+1}), z_k - u \rangle + \alpha \langle \nabla f(x_{k+1}), x_{k+1} - z_k \rangle
$$
å‰é¢ä¸€é¡¹æˆ‘ä»¬å·²ç»MDåšæ‰äº†ï¼Œåé¢ä¸€é¡¹
$$
\alpha \langle \nabla f(x_{k+1}), x_{k+1} - z_k \rangle
$$

$$
= \frac{(1 - \tau) \alpha}{\tau} \langle \nabla f(x_{k+1}), y_k - x_{k+1} \rangle
$$

$$
\leq \frac{(1 - \tau) \alpha}{\tau} (f(y_k) - f(x_{k+1}))
$$
æ‰€ä»¥è¯´
$$
\alpha (f(x_{k+1}) - f(u)) \leq \alpha^2 L (f(x_{k+1}) - f(y_{k+1})) + \frac{(1 - \tau) \alpha}{\tau} (f(y_k) - f(x_{k+1}))
$$

$$+ V_{z_k}(u) - V_{z_{k+1}}(u)
$$
ä»¤
$
\frac{(1 - \tau) \alpha}{\tau} = \alpha^2 L
$ï¼Œ
æœ‰
$$
f(x_{k+1}) - f(u) \leq \alpha^2 L (f(y_k) - f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u)
$$

Telescope:

$$
\alpha T (f(\overline{x}) - f(x^\*)) \leq \alpha^2 L (f(y_0) - f(y_T)) + V_{x_0}(x^\*) - V_{z_T}(x^\*)
$$

å‡è®¾ $f(y_0) - f(x^\*) = d$, $V_{x_0}(x^\*) = \Theta$
æœ‰
$$
f(x_i) - f(x^\*) \leq \frac{\alpha dL}{T} + \frac{\Theta}{\alpha T}
$$
ä»¤$
\alpha = \sqrt{\frac{\Theta}{dL}}$,
æœ‰
$$ f(\overline{x}) - f(x^\*) \leq \frac{2 \sqrt{\Theta Ld}}{T}$$

å– $ T = 4 \sqrt{\frac{L\Theta}{d}}$,
æœ‰$$f(\overline{x})-f(x^\*)\leq \frac{d}{2}$$
æ‰€ä»¥è¯´æˆ‘ä»¬æ¯ $2\epsilon\rightarrow \epsilon$è¿‡ç¨‹é‡æ–°è°ƒæ•´ä¸€æ¬¡$\tau,\alpha$ï¼Œæœ€åå¾—åˆ°çš„è¿­ä»£æ¬¡æ•°æ˜¯:
$$O(\sqrt{\frac{L \Theta}{\epsilon}})+O(\sqrt{\frac{L \Theta}{2\epsilon}})+O(\sqrt{\frac{L \Theta}{4\epsilon}})+...=O(\sqrt{\frac{L \Theta}{\epsilon}})$$
Nesterovå‘Šè¯‰æˆ‘ä»¬$O(\frac{1}{T^2})$(aka.$O(\sqrt{\frac{L}{\epsilon}})$)å°±æ˜¯æˆ‘ä»¬å¯¹äºconvexä¸”L-smoothå‡½æ•°èƒ½å¾—åˆ°çš„æœ€å¥½ç»“æœäº†ï¼Œæ‰€ä»¥Linear Couplingç¡®å®å¾ˆç‰›ã€‚

## 1.7 Non-Convex Optimization
### Matrix Completion
$A \in \mathbb{R}^{m \times n}$æ»¡è¶³ä»¥ä¸‹å‡è®¾:
> 1Â° $A$ is low rank
> 
> 2Â° Known entries are uniformly distributed
> 
> 3Â° **Incoherence**: $$
A = U \Sigma V^T \quad \text{for } i \in [n], j \in [m]$$ $$\exists \mu: 1 \leq \mu \leq \frac{min(m,n)}{r}$$$$
\|e_i^T U\| \leq \sqrt{\frac{\mu r}{n}}, \quad \|e_j^T V\| \leq \sqrt{\frac{\mu r}{m}}$$

é‚£ä¹ˆæˆ‘ä»¬çš„ç›®æ ‡($P_\Omega$ä»£è¡¨ä¸çŸ¥é“çš„å…ƒç´ éƒ½maskæ‰):
$$
\min \|P_\Omega(UV^T) - P_\Omega(A)\|_F^2
$$
å¯ä»¥æœ‰ä»¥ä¸‹ç®—æ³•:
> Algorithm:
> 
> For $t = 0, 1, 2, \ldots, T$
> - $V^{t+1} \leftarrow \arg \min_V ||P_{\Omega}(U^t V) - P_{\Omega}(A)||_F^2$
> - $U^{t+1} \leftarrow \arg \min_U ||P_{\Omega}(U V^{t}) - P_{\Omega}(A)||_F^2$
### Escaping Saddle Points
SGDåœ¨éå‡¸ä¼˜åŒ–ä¸­æœ‰ä¸€äº›GDä¹‹ç±»ç®—æ³•æ²¡æœ‰çš„å¥½å¤„ï¼Œè¿™å°±æ˜¯å™ªå£°æ‰€å¸¦æ¥çš„éšæœºæ€§æ‰€å±•ç°çš„ä¼˜åŠ¿:
> **Thm.**<br>If ğ¿ is **smooth, bounded and strict saddle** (actually more general version, applies to points with small gradients, rather than zero gradients), and **Hessian is smooth**. If **SGD noise has non-negligible variance in every direction with constant probability**, SGD will **escape all saddle points and local maxima, converge to a local minimum after polynomial number of steps.**

å…¶ä¸­Strict Saddle Pointæ˜¯æŒ‡ä¸€ä¸ªç‚¹$\nabla f(x)=0$ï¼Œ $\nabla^2 f(x)$åˆæœ‰æ­£ç‰¹å¾å€¼åˆæœ‰è´Ÿç‰¹å¾å€¼ã€‚Flat Saddle Pointæ˜¯æŒ‡ä¸€ä¸ªç‚¹$\nabla f(x)=0$ï¼Œ $\nabla^2 f(x)$çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½å¤§äºç­‰äº0ï¼Œä¸”æœ‰ä¸€ä¸ªç­‰äº0çš„ç‰¹å¾å€¼ã€‚
# 2.Generalization
## 2.1 No Free Lunch Thm.
> *Thm.*
> è®¾ $A$ ä¸ºåœ¨å®šä¹‰åŸŸ $\mathcal{X}$ ä¸Šç›¸å¯¹äº 0-1 æŸå¤±çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡çš„ä»»æ„å­¦ä¹ ç®—æ³•ã€‚è®¾ $m$ ä¸ºå°äº $|\mathcal{X}|/2$ çš„ä»»æ„æ•°ï¼Œè¡¨ç¤ºè®­ç»ƒé›†å¤§å°ã€‚åˆ™å­˜åœ¨ä¸€ä¸ªåœ¨ $\mathcal{X} \times \{0, 1\}$ ä¸Šçš„åˆ†å¸ƒ $\mathcal{D}$ ä½¿å¾—ï¼š
> 1. å­˜åœ¨ä¸€ä¸ªå‡½æ•° $f : \mathcal{X} \to \{0, 1\}$ï¼Œä½¿å¾— $L_\mathcal{D}(f) = 0$ã€‚
> 2. ä»¥è‡³å°‘ $1/7$ çš„æ¦‚ç‡ï¼Œå¯¹äºä» $\mathcal{D}^m$ ä¸­é€‰å–çš„ $S$ï¼Œæœ‰ $L_\mathcal{D}(A(S)) \geq 1/8$ã€‚

è¿™ä¸ªçš„ç›´è§‰åœ¨äºç”±Markovä¸ç­‰å¼ï¼Œ$\mathbb{E}_{S \sim D^m }[L_D(A(S))]\geq \frac{1}{4}$,ä¹Ÿå°±æ˜¯è¯´å¯¹äºä¸€ä¸ªå®Œå…¨é èƒŒè¯µçš„ç®—æ³•: å‡å¦‚è§è¿‡$(X,y)$,è¾“å‡º$y$ï¼Œå‡å¦‚æ²¡è§è¿‡å°±éšæœºè¾“å‡º0æˆ–1ã€‚è¿™æ ·å¯¹äºä¸€ä¸ª$|C|=2m$çš„$X$çš„å­é›†ï¼Œè¿™æ ·â€œèƒŒè¯µ+çè’™â€çš„loss functionæ˜¯$\frac{1}{4}$ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ²¡æœ‰ä»€ä¹ˆåŠæ³•èƒ½å¤Ÿä»æœŸæœ›ä¸Šæ¯”â€œèƒŒè¯µ+çè’™â€æ•ˆæœæ›´å¥½ï¼Œä¹Ÿå°±æ˜¯è¯´å­¦ä¹ ç®—æ³•å¤±è´¥äº†ã€‚

**è¯æ˜**:

ä¸ºäº†ç®€æ´æ€§ï¼Œä¸å¦¨è®¾$|C| = 2m$.

è®°$T = 2^{2m}$ã€‚ä»$C$åˆ°$\{0, 1\}$çš„å‡½æ•°ä¸€å…±æœ‰$f_1, \ldots, f_T$ï¼Œå…±$T$ä¸ª

è®°
$$
D_i(\{x, y\}) =
\frac{1}{|C|} \quad \text{if } y = f_i(x) 
$$
$$
D_i(\{x, y\}) = 0 \quad \text{otherwise.}
$$

æ˜¾ç„¶ï¼Œ$L_{D_i}(f_i) = 0$.

æˆ‘ä»¬æ¥ä¸‹æ¥è¯æ˜:

$$\max_{i \in [T]} E_{S \sim D_{i}^{m}} [ L_{D_i}(A(S)) ] \geq \frac{1}{4}$$

---

è®°ä¸€å…±æœ‰$k$ä¸ªå¯èƒ½çš„ä»$C$ä¸­å–æ ·å‡ºçš„$m$ä¸ªæ•°æ®ç‚¹$x_i$åºåˆ—:
æœ‰$k = (2m)^m$ï¼Œè®°
$S_j = (x_1, \ldots, x_m)$
ï¼Œè®°
$S_j^i = \left( (x_1, f_i(x_1)), \ldots, (x_m, f_i(x_m)) \right)$ã€‚

æˆ‘ä»¬åªéœ€è¦å–å‡ºä¸€ä¸ª$i \in [T]$èƒ½å¤Ÿè®©$E_{S \sim D_i^m} \left[ L_{D_i}(A(S)) \right]\geq \frac{1}{4}$,é‚£ä¹ˆå¯¹åº”çš„$D_i$ä¾¿æ˜¯æˆ‘ä»¬åœ¨NFLä¸­æ‰€å¸Œæœ›æ‰¾åˆ°çš„$D$ã€‚
$$
\max_{i \in [T]} E_{S \sim D_i^m} \left[ L_{D_i}(A(S)) \right]
$$

$$
= \max_{i \in [T]} \frac{1}{k} \sum_{j=1}^k L_{D_i}(A(S_j^i))
$$

$$
\geq \frac{1}{T} \sum_{i=1}^T \frac{1}{k} \sum_{j=1}^k L_{D_i}(A(S_j^i))
$$

$$
= \frac{1}{k} \sum_{j=1}^k \frac{1}{T} \sum_{i=1}^T L_{D_i}(A(S_j^i))
$$

$$
\geq \min_{j \in [k]} \frac{1}{T} \sum_{i=1}^T L_{D_i}(A(S_j^i))
$$
å¯¹äºç»™å®šçš„ $j$:

ä»¤$v_1, \ldots, v_p$ ä¸º$S_j$ä¸­æ²¡æœ‰å‡ºç°çš„$x\in C$, æ³¨æ„åˆ°$p \geq m$ã€‚

$$
L_{D_i}(A(S_j^i)) = \frac{1}{2m} \sum_{x \in C} \mathbf{1}[h(x) \neq f_i(x)]
$$

$$
\geq \frac{1}{2m} \sum_{r=1}^p \mathbf{1}[h(v_r) \neq f_i(v_r)]
$$

$$
\geq \frac{1}{2p} \sum_{r=1}^p \mathbf{1}[h(v_r) \neq f_i(v_r)]
$$

æ‰€ä»¥è¯´

$$
\frac{1}{T} \sum_{i=1}^T L_{D_i}(A(S_j^i))
$$

$$
\geq \frac{1}{T} \sum_{i=1}^T \frac{1}{2p} \sum_{r=1}^p \mathbf{1}[h(v_r) \neq f_i(v_r)]
$$

æˆ‘ä»¬å¯ä»¥å°† $f_1, \ldots, f_T$ ä¸­çš„æ‰€æœ‰å‡½æ•°åˆ’åˆ†æˆ $T/2$ å¯¹ä¸ç›¸äº¤çš„å‡½æ•°å¯¹ï¼Œå…¶ä¸­å¯¹äºæ¯ä¸€å¯¹ $(f_i, f_{i'})$ï¼Œå¯¹äºä»»æ„ $c \in C$ï¼Œéƒ½æœ‰ $f_i(c) \neq f_{i'}(c)$ã€‚

äºæ˜¯æœ‰
$$
\frac{1}{2p} \sum_{r=1}^p \frac{1}{T} \sum_{i=1}^T \mathbf{1}[h(v_r) \neq f_i(v_r)] = \frac{1}{4}
$$

æ‰€ä»¥è¯´
$$
\max_{i \in [T]} E_{S \sim D_i^m} \left[ L_{D_i}(A(S)) \right] \geq \frac{1}{4}
$$

---

ä»¤$\mathcal{D} = D_i$:

å¦‚æœ

$$
\Pr \left[ L_{\mathcal{D}}(A(S)) \geq \frac{1}{8} \right] < \frac{1}{7}
$$

é‚£ä¹ˆ

$$
E_{S \sim \mathcal{D}^m} \left[ L_{\mathcal{D}}(A(S)) \right] < \frac{1}{7} \cdot 1 + \frac{6}{7} \cdot \frac{1}{8}
$$

$$
= \frac{1}{7} + \frac{3}{28} = \frac{1}{4}.\quad\blacksquare
$$
## 2.2 PAC-Learning
### ä¸€äº›æ¦‚å¿µ:

- **Hypothesis Class (H)** ï¼šèƒ½å¤Ÿé€‰æ‹©çš„å‡è®¾$h$çš„é›†åˆ
- **$ERM_H$** ï¼šé€‰æ‹©å…·æœ‰æœ€å°empirical lossçš„å‡è®¾

$$
ERM_H(S) \in \arg\min_{h \in H} L_S(h)
$$
- **Realizability Assumption**: å­˜åœ¨ $h^\* \in H$ ä½¿å¾— $L_{D,f}(h^\*) = 0$ã€‚è¿™æ„å‘³ç€å¯¹äºæ¯ä¸ªè®­ç»ƒé›† $S$ï¼Œæˆ‘ä»¬æœ‰ $L_S(h^\*) = 0$ã€‚
- **PAC-Learnable**: å¦‚æœå­˜åœ¨ä¸€ä¸ªå‡½æ•° $m_H: (0,1)^2 \to \mathbb{N}$ å’Œä¸€ä¸ªlearning algorithmï¼Œä½¿å¾—å¯¹äºä»»æ„çš„ $\epsilon, \delta \in (0,1)$ï¼Œå¯¹äºå®šä¹‰åœ¨ $X$ ä¸Šçš„ä»»æ„åˆ†å¸ƒ $D$ï¼Œä»¥åŠä»»æ„labeling function $f: X \to \{0,1\}$ï¼Œè‹¥Realizability Assumptionåœ¨ $H, D, f$ ä¸‹æˆç«‹ï¼Œåˆ™å½“åœ¨ç”± $D$ ç”Ÿæˆå¹¶ç”± $f$ æ ‡è®°çš„ $m \geq m_H(\epsilon, \delta)$ ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒæ ·æœ¬ä¸Šè¿è¡Œè¯¥learning algorithmæ—¶ï¼Œè¯¥ç®—æ³•è¿”å›ä¸€ä¸ªå‡è®¾ $h$ï¼Œä½¿å¾—ä»¥è‡³å°‘ $1 - \delta$ çš„æ¦‚ç‡ï¼ˆåœ¨æ ·æœ¬é€‰æ‹©çš„éšæœºæ€§ä¸Šï¼‰ï¼Œ$L_{D,f}(h) \leq \epsilon$ã€‚


### Finite Classes are PAC-learnable
> **Thm.** ç»™å®š $\delta \in (0,1)$, $\epsilon > 0$, å¦‚æœ $m \geq \frac{\log(|H|/\delta)}{\epsilon}$ï¼Œé‚£ä¹ˆå¦‚æœRealizability Assumptionæˆç«‹ï¼Œ é‚£ä¹ˆå¯¹äºä»»æ„ERM hypothesis $h_S$:
> $$
\Pr [ L_D(h_S) \leq \epsilon ] \geq 1 - \delta
$$
**Pf.** æˆ‘ä»¬æƒ³è¦upper bound

$$
\Pr_{S\sim \mathcal{D}^m} [ S | L_D(h(S)) > \epsilon ]
$$

å®šä¹‰æ‰€æœ‰ä¸å¥½çš„å‡è®¾çš„é›†åˆä¸º:
$$
H_B := \{ h \in H | L_D(f, h) > \epsilon \}
$$
å®šä¹‰misleadingçš„å‡è®¾çš„é›†åˆä¸ºï¼š
$$
M := \{ S \mid \exists h \in H_B, L_S(h) = 0 \}
$$
æœ‰
$$
\{ S \mid L_D(h(S)) > \epsilon \} \subseteq M
$$
æ‰€ä»¥
$$
\Pr \left[ L_D(h(S)) > \epsilon \right] \leq \Pr \left[ S \in M \right] \leq \sum_{h \in H_B} \Pr \left[ L_S(h) = 0 \right]
$$
åˆå› ä¸º
$$
\Pr \left[ L_S(h) = 0 \right] = \prod_{i=1}^m Pr_{x_i\sim\mathcal{D}} \left[ h(x_i) = f(x_i) \right]
$$

å› ä¸º
$$
Pr_{x_i\sim\mathcal{D}} \left[ h(x_i) = f(x_i) \right] = 1 - L_D(f, h) \leq 1 - \epsilon
$$
æ‰€ä»¥

$$
\Pr \left[ L_S(h) = 0 \right] \leq (1 - \epsilon)^m \leq e^{-m \epsilon}
$$


$$
|H| \cdot e^{-m \epsilon} \leq \delta \implies m = \frac{\log(|H|/\delta)}{\epsilon}. \blacksquare
$$
### Threshold Functions are PAC-learnable
* Threshold Functions:
  $$
  \mathcal{H}=\{h(x) = \mathbf{1}[x < a]\}
  $$
  ![](../img/ml1/image4.png)
  æ³¨æ„åˆ°è¿™æ˜¯ä¸€ä¸ªinfinite classã€‚
> **Thm.** è®¾ $H$ ä¸ºThreshold Functionsã€‚åˆ™ $H$ æ˜¯ PAC-learnableçš„ï¼Œä½¿ç”¨ ERM ç®—æ³•ï¼Œå…¶æ ·æœ¬å¤æ‚åº¦ä¸º$$
m_H(\epsilon, \delta) \leq \frac{\lceil \log(2/\delta) \rceil}{\epsilon}$$

- **Pf.**

è®°$
h^\*(x) = \mathbf{1}[x < a^\*]
$s.t.$L_D(h^\*)=0$

å®šä¹‰
$$
b_0 := \sup \{x \mid (x, 1) \in S\}, \quad b_1 := \inf \{x \mid (x, 0) \in S\}
$$
![](../img/ml1/image11.png)
æ³¨æ„åˆ°
$$
\Pr \left[ L_D(h) > \epsilon \right] \leq \Pr \left[ b_0 < a_0 \right] + \Pr \left[ b_1 > a_1 \right]
$$
åœ¨$
m = \frac{\ln \left(\frac{2}{\delta}\right)}{\epsilon}
$çš„æƒ…å†µä¸‹:
$$
\Pr \left[ b_0 < a_0 \right] = (1 - \epsilon)^m \leq e^{-\epsilon m} = \frac{\delta}{2}
$$

$$
\Pr \left[ b_1 > a_1 \right] = (1 - \epsilon)^m \leq e^{-\epsilon m} = \frac{\delta}{2}. \blacksquare
$$
## 2.3 Agnostic PAC-Learnable
æœ‰æ—¶å€™Realizability Assumptionå¤ªå¼ºäº†ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªåœ¨$\mathcal{H}$ä¸­æ²¡æœ‰Loss=0çš„hypothesisçš„æƒ…å†µä¸‹è¡¡é‡estimation errorçš„æ‰‹æ®µ:
### **Agnostic PAC-Learnable**:
  ä¸€ä¸ªå‡è®¾ç±» $H$ æ˜¯ Agnostic PAC å¯å­¦ä¹ çš„ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªå‡½æ•° $m_H: (0,1)^2 \rightarrow \mathbb{N}$ å’Œä¸€ä¸ªå…·æœ‰ä»¥ä¸‹æ€§è´¨çš„å­¦ä¹ ç®—æ³•ï¼šå¯¹äºæ¯ä¸€ä¸ª $\epsilon, \delta \in (0,1)$ï¼Œä»¥åŠå®šä¹‰åœ¨ $X \times Y$ ä¸Šçš„æ¯ä¸ªåˆ†å¸ƒ $D$ï¼Œå½“åœ¨ç”± $D$ ç”Ÿæˆçš„ $m \geq m_H(\epsilon, \delta)$ ä¸ªç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰æ ·æœ¬ä¸Šè¿è¡Œè¯¥å­¦ä¹ ç®—æ³•æ—¶ï¼Œç®—æ³•ä¼šè¿”å›ä¸€ä¸ªå‡è®¾ $h$ï¼Œä½¿å¾—ä»¥è‡³å°‘ $1 - \delta$ çš„æ¦‚ç‡ï¼ˆå¯¹äº $m$ ä¸ªè®­ç»ƒæ ·æœ¬çš„é€‰æ‹©è€Œè¨€ï¼‰ï¼Œæ»¡è¶³

$$
L_D(h) \leq \min_{h' \in H} L_D(h') + \epsilon
$$

### **Error Decomposition**:

  - $L_D(h_S) = \epsilon_{app} + \epsilon_{est}$
  - $\epsilon_{app} = \min_{h \in H} L_D(h)$
  - $\epsilon_{est} = L_D(h_S) - \epsilon_{app}$
  - $\epsilon_{app} = L_D(BO) + \min_{h \in H} L_D(h) - L_D(BO)$
  
$\epsilon_{app}$æè¿°çš„æ˜¯è¿™ä¸ªhypothesis classçš„inductive biasçš„å¤šå°‘ï¼Œè€Œ$\epsilon_{est}$æ˜¯ä¸sample sizeå’Œsample complexityç›¸å…³çš„(sample complexityä¸hypothesis classçš„representation poweræˆæ­£æ¯”)ï¼Œæ‰€ä»¥è¯´å½“æˆ‘ä»¬æƒ³è¦å‡å°‘$L_D(h_S)$,æˆ‘ä»¬é¢ä¸´ä¸€ä¸ªbias-complexity tradeoffã€‚

å…¶ä¸­BOæŒ‡ä»£çš„æ˜¯Bayes Optimal Predictorã€‚

### Bayes Optimal Predictor

ç»™å®šä»»ä½•åœ¨ $X \times \{0,1\}$ ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ $D$ï¼Œä» $X$ åˆ° $\{0,1\}$ çš„æœ€ä½³æ ‡ç­¾é¢„æµ‹å‡½æ•°ä¸º

$$
f_D(x) = 1 \quad \text{if } P[y = 1 \mid x] \geq \frac{1}{2}
$$
$$
f_D(x) = 0 \quad \text{otherwise}
$$

å¾ˆå®¹æ˜“éªŒè¯ï¼Œå¯¹äºæ¯ä¸ªæ¦‚ç‡åˆ†å¸ƒ $D$ï¼Œè´å¶æ–¯æœ€ä¼˜é¢„æµ‹å™¨ $f_D$ æ˜¯æœ€ä¼˜çš„ï¼Œå› ä¸ºæ²¡æœ‰å…¶ä»–åˆ†ç±»å™¨ $g: X \rightarrow \{0,1\}$ çš„é”™è¯¯ç‡æ›´ä½ã€‚å³ï¼Œå¯¹äºæ¯ä¸ªåˆ†ç±»å™¨ $g$ï¼Œæœ‰

$$
L_D(f_D) \leq L_D(g)
$$

## 2.4 VC-Dim
### Restriction of $H$ to $C$
è®¾ $H$ æ˜¯ä» $X$ åˆ° $\{0,1\}$ çš„å‡½æ•°ç±»ï¼Œ$C = \{c_1, \cdots, c_m\} \subseteq X$ã€‚$H$ åœ¨ $C$ ä¸Šçš„é™åˆ¶æ˜¯ä» $C$ åˆ° $\{0,1\}$ çš„å‡½æ•°é›†åˆï¼Œè¿™äº›å‡½æ•°å¯ä»¥ä» $H$ ä¸­å¯¼å‡ºã€‚å³

$$
H_C = \{(h(c_1), \cdots, h(c_m)) : h \in H\}
$$

æˆ‘ä»¬å°†ä» $C$ åˆ° $\{0,1\}$ çš„æ¯ä¸ªå‡½æ•°è¡¨ç¤ºä¸º $\{0,1\}^{|C|}$ ä¸­çš„ä¸€ä¸ªå‘é‡ã€‚

### Shattering
ä¸€ä¸ªå‡è®¾ç±» $H$ Shatteræœ‰é™é›† $C \subseteq X$ï¼Œå¦‚æœ Restriction of $H$ to $C$æ˜¯ä» $C$ åˆ° $\{0,1\}$ çš„æ‰€æœ‰å‡½æ•°é›†åˆã€‚å³

$$
|H_C| = 2^{|C|}
$$
### NFL Reexpressed
è®¾ $H$ æ˜¯ä» $X$ åˆ° $\{0,1\}$ çš„hypothesis classã€‚ä»¤ $m$ ä¸ºè®­ç»ƒé›†å¤§å°ã€‚å‡è®¾å­˜åœ¨ä¸€ä¸ªå¤§å°ä¸º $2m$ çš„é›†åˆ $C \subseteq X$ï¼Œå®ƒè¢« $H$ shatterã€‚åˆ™å¯¹äºä»»æ„å­¦ä¹ ç®—æ³• $A$ï¼Œå­˜åœ¨ä¸€ä¸ªå®šä¹‰åœ¨ $X \times \{0,1\}$ ä¸Šçš„åˆ†å¸ƒ $D$ å’Œä¸€ä¸ªé¢„æµ‹å™¨ $h \in H$ï¼Œä½¿å¾— $L_D(h) = 0$ï¼Œä½†ä»¥è‡³å°‘ $\frac{1}{7}$ çš„æ¦‚ç‡ï¼Œå¯¹äº $S \sim D^m$ çš„é€‰æ‹©ï¼Œæœ‰

$$
L_D(A(S)) \geq \frac{1}{8}
$$

### VC-Dimension
Hypothesis class $H$ çš„ VC-dimensionï¼ˆè®°ä½œ $\text{VCdim}(H)$ï¼‰æ˜¯ $H$ å¯ä»¥shatterçš„é›†åˆ $C \subseteq X$ çš„æœ€å¤§å¤§å°ã€‚å¦‚æœ $H$ å¯ä»¥shatterä»»æ„å¤§çš„é›†åˆï¼Œæˆ‘ä»¬ç§° $\text{VCdim}(H)=+ \infty$.
### Inifite VC-dim hypothesis classes are not PAC-learnable
NFLçš„ç›´æ¥åæœå°±æ˜¯$\text{VCdim}(H)=+ \infty$çš„$H$ä¸æ˜¯PAC-learnableçš„ã€‚
## 2.5 Fundamental theorem of statistical learning 


è®¾ $H$ æ˜¯ä»ä¸€ä¸ªåŸŸ $X$ åˆ° $\{0,1\}$ çš„hypothesis classï¼Œå¹¶ä¸”æŸå¤±å‡½æ•°æ˜¯ 0-1 æŸå¤±ã€‚å‡è®¾ $\text{VCdim}(H) = d < \infty$ã€‚åˆ™å­˜åœ¨å¸¸æ•° $C_1, C_2$ï¼Œä½¿å¾—

$H$ æ˜¯å…·æœ‰ä»¥ä¸‹æ ·æœ¬å¤æ‚åº¦çš„Agnostic PAC-learnableï¼š

$$
C_1 \frac{d + \log \left(\frac{1}{\delta}\right)}{\epsilon^2} \leq m_H(\epsilon, \delta) \leq C_2 \frac{d + \log \left(\frac{1}{\delta}\right)}{\epsilon^2}
$$

$H$ æ˜¯å…·æœ‰ä»¥ä¸‹æ ·æœ¬å¤æ‚åº¦çš„ PAC-learnableï¼š

$$
C_1 \frac{d + \log \left(\frac{1}{\delta}\right)}{\epsilon} \leq m_H(\epsilon, \delta) \leq C_2 \frac{d \log \left(\frac{1}{\epsilon}\right) + \log \left(\frac{1}{\delta}\right)}{\epsilon}
$$
# 3.Supervised Learning
- å¯¹äºå›å½’é—®é¢˜ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªå‡½æ•°$f: X\rightarrow \mathbb{R}$
- åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªå‡½æ•°$f: X\rightarrow \{0,1\}$æˆ–è€…$\{-1,1\}$ã€‚

å‰è€…æˆ‘ä»¬çš„loss functionå¾ˆå¥½designï¼Œæ¯”å¦‚è¯´Mean Square Lossï¼Œä½†æ˜¯åè€…çš„losså°±ä¸æ˜¯ç‰¹åˆ«å¥½designã€‚ä¸€ç§è‡ªç„¶çš„æƒ³æ³•æ˜¯$f(x)=sign(w^Tx)$,ä½†æ˜¯é—®é¢˜å°±æ˜¯è¿™ä¸ªlossä¸å¯å¯¼ï¼Œä¸‹é¢æ˜¯ä¸€ç§åˆ©ç”¨è¿™ç§å‡½æ•°ä½†æ˜¯ä¸éœ€è¦å¯¼æ•°çš„è¿œå¤ç®—æ³•ã€‚
## 3.1 Perceptron
### Algorithm
![](../img/ml1/image10.png)
### Convergence 
> Thm. 
> 
> åˆé€‚ç¼©æ”¾ä½¿å¾— $||x_i|| \leq 1$ ã€‚å‡è®¾å­˜åœ¨ $w_\*$ æ»¡è¶³ $||w_\*|| = 1$ ä¸” $y_i w_\*^T x_i > \gamma$ï¼ˆå­˜åœ¨è¿‡åŸç‚¹çš„åˆ’åˆ†å¹³é¢ï¼Œå®‰å…¨è·ç¦»ä¸º $\gamma$ï¼‰ã€‚è¯¥ç®—æ³•æ”¶æ•›å‰æœ€å¤šè§¦å‘ $\frac{1}{\gamma^2}$ æ¬¡é¢„æµ‹é”™è¯¯ã€‚

**Pf.**
å‡è®¾ç®—æ³•ç¬¬ $t$ æ¬¡çŠ¯é”™æ˜¯ $(x_t, y_t)$ï¼Œè¿™ä¼šä½¿å¾—

$$w_{t+1} = w_t + y_t x_t$$

ä¸”æ­¤æ—¶ $\langle {w}^T, y_t x_t \rangle < 0$ï¼ˆé”è§’ï¼‰ã€‚è¿™è¯´æ˜
$$
||w_{t+1}||^2 \leq ||w_t||^2 + ||y_t x_t||^2 = ||w_t||^2 + 1 \\
||w_t||^2 \leq t
$$

å¦ä¸€æ–¹é¢
$$
||w_{t+1}|| \geq \langle w_{t+1}, w_\* \rangle \geq \langle w_t, w_\* \rangle + \gamma \\
||w_t|| \geq \gamma t
$$

ç»¼ä¸Š
$$
\gamma^2 t^2 \leq \|w_t\|^2 \leq t
$$
è§£å¾— $t \leq \frac{1}{\gamma^2}$ã€‚$\blacksquare$
## 3.2 Logistic Regression
ä¸ºäº†è§£å†³ä¸å¯å¯¼çš„é—®é¢˜ï¼Œæ›´ä¸ºç°ä»£çš„æƒ³æ³•æ˜¯é€šè¿‡sigmoidå‡½æ•°æŠŠ$w^Tx$å‹ç¼©åˆ°$(0,1)$ä¹‹é—´çš„æ¦‚ç‡ï¼Œå³$$f(x)=\frac{1}{1+e^{-w^Tx}}.$$
è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡ä¹‹é—´çš„å·®å¼‚ï¼Œå¯ä»¥ç”¨l1-normæˆ–è€…cross-entropy lossã€‚

![](../img/ml1/image9.png)
> **ç†µ (Entropy)**<br>
å¯¹äºç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \cdots, p_n)$ï¼Œå®šä¹‰å®ƒçš„ç†µä¸º$$
H(p) = \sum_{i=1}^{n} p_i \log \frac{1}{p_i}$$

> **äº¤å‰ç†µ (Cross entropy)**<br>
å®šä¹‰ä¸¤ä¸ªç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \cdots, p_n)$ å’Œ $(q_1, q_2, \cdots, q_n)$ çš„äº¤å‰ç†µä¸º$$
XE(p, q) = \sum_{i=1}^{n} p_i \log \frac{1}{q_i}$$

> **KL æ•£åº¦**<br>
å®šä¹‰ä¸¤ä¸ªç¦»æ•£æ¦‚ç‡åˆ†å¸ƒ $(p_1, p_2, \cdots, p_n)$ å’Œ $(q_1, q_2, \cdots, q_n)$ çš„ KL æ•£åº¦ä¸º$$
KL(p, q) = XE(p, q) - H(p)$$

äº¤å‰ç†µæ¯”l1-norm å¥½åœ¨ï¼š
* l1-normï¼šæä¾›æ’å®šçš„æ¢¯åº¦ã€‚
* äº¤å‰ç†µï¼šå·®è·è¶Šå¤§ï¼Œæ¢¯åº¦è¶Šå¤§
## 3.3 Regularization
å½“æˆ‘ä»¬æƒ³è¦é™åˆ¶$f$çš„è¡¨è¾¾èƒ½åŠ›æ—¶ï¼Œç»å…¸çš„çœ‹æ³•å°±æ˜¯é€šè¿‡åœ¨$||Â·||_2$æˆ–$||Â·||_1$æ„ä¹‰ä¸‹é™åˆ¶$w$çš„å¯èƒ½å–å€¼åŒºé—´ã€‚
### Ridge Regression
æŠŠloss functionæ”¹ä¸º$$l(w)+\lambda||w||^2$$
è¿™é‡Œæ˜¯2-norm, è¿™ç›¸å½“äºæ¯ä¸€æ­¥å…ˆGDï¼Œä¹‹åå†è¿›è¡Œäº†ä¸€æ¬¡ 
$$w_{t+1}=(1-\eta \lambda)\tilde{w}_{t}$$
è¿™è¢«ç§°ä¸ºweight decayã€‚
### Lasso Regression
æœ‰æ—¶å€™æˆ‘ä»¬æƒ³è¦è·å¾—sparseçš„è§£ï¼Œå› æ­¤æˆ‘ä»¬æŠŠloss functionæ”¹ä¸º$$l(w)+\lambda||w||_1^2$$
è¿™ä¸ªç›´è§‰åœ¨äºç”¨diamondå’Œå‡¸é›†çš„äº¤é›†æ›´æœ‰å¯èƒ½æ˜¯sparseçš„
![](../img/ml1/image8.png)
## 3.4 Compressed Sensing
> **Nyquist theorem**: <br>for a signal with frequency ğ‘“, we need 2ğ‘“ sampling rate to fully reconstruct the signal

è¿™ä¸ªæ˜¯ä¸€ä¸ªé€šç”¨çš„å®šç†ï¼Œä½†æ˜¯å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä¿¡å·å…¶å®æ˜¯å­˜åœ¨ä¸€ç»„åŸºä¸‹çš„ç¨€ç–è¡¨ç¤ºï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šå»æƒ³èƒ½ä¸èƒ½é€šè¿‡æ›´å°‘çš„é‡‡æ ·ï¼Œæ¥é‡æ„å‡ºä¿¡å·ï¼Œè¿™å°±æ˜¯compressed sensingçš„èƒŒæ™¯ã€‚

![](../img/ml1/image7.png)

åœ¨Compressed Sensingä¸­ï¼Œå’Œsupervised learningä¸åŒçš„æ˜¯æˆ‘ä»¬å¯ä»¥è‡ªå·±é€‰æ‹©è‡ªå·±çš„measurement matrixï¼Œå³è®­ç»ƒé›†ï¼Œåœ¨ä¸‹å›¾ä¸­ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰å®š$A$çš„æ¯ä¸€è¡Œï¼Œç„¶åè·å¾—å¯¹åº”çš„$y$ï¼Œæœ€ç»ˆæˆ‘ä»¬å¸Œæœ›é€šè¿‡$y$è¿˜åŸå‡º$x$ã€‚

![](../img/ml1/image6.png)
æœ€åçš„å¾—åˆ°çš„ä¸»è¦ç»“è®ºï¼Œç”¨è‡ªç„¶è¯­è¨€å»æè¿°ï¼Œæ˜¯å¦‚ä¸‹ä¸‰æ¡:
1. å¦‚æœä¸€ä¸ªç¨€ç–ä¿¡å·é€šè¿‡ $x \mapsto Wx$ è¿›è¡Œäº†å‹ç¼©ï¼Œå…¶ä¸­ $W$ æ˜¯æ»¡è¶³$(\epsilon, s)$-RIP çš„çŸ©é˜µï¼Œé‚£ä¹ˆå¯ä»¥å®Œå…¨é‡æ„ä»»ä½•ç¨€ç–ä¿¡å·ã€‚æ»¡è¶³æ­¤æ€§è´¨çš„çŸ©é˜µä¿è¯äº†ä»»ä½•ç¨€ç–å¯è¡¨ç¤ºå‘é‡çš„èŒƒæ•°distortionè¾ƒå°ã€‚

2. é€šè¿‡æ±‚è§£çº¿æ€§è§„åˆ’ï¼Œé‡æ„å¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å†…è®¡ç®—ã€‚

3. ç»™å®š $n \times d$ çš„éšæœºçŸ©é˜µï¼Œåœ¨ $n$ å¤§äº $s \log(d)$ çš„æ•°é‡çº§æ—¶ï¼Œå®ƒå¾ˆå¯èƒ½æ»¡è¶³ RIP æ¡ä»¶ã€‚

æ¥ä¸‹æ¥è®©æˆ‘formallyç”¨æ•°å­¦çš„è¯­è¨€build upéƒ½ä»¥ä¸Šçš„ç»“è®ºã€‚
### RIP-Condition
ä¸€ä¸ªçŸ©é˜µ $W \in \mathbb{R}^{n,d}$ æ˜¯ $(\epsilon, s)$-RIP çš„å½“ä¸”ä»…å½“å¯¹äºæ‰€æœ‰ $x \neq 0$ ä¸”æ»¡è¶³ $||x||_{0}\leq s$ çš„ $x$ï¼Œæˆ‘ä»¬æœ‰
$$
\left| \frac{||Wx||_2^2}{||x||_2^2} - 1 \right| \leq \epsilon.
$$
### Thm.1
> **Thm.1** è®¾ $\epsilon < 1$ï¼Œå¹¶ä¸”è®¾ $W$ ä¸º $(\epsilon, 2s)$-RIP çŸ©é˜µã€‚è®¾ $x$ ä¸ºä¸€ä¸ªæ»¡è¶³ $||x||_0\leq s$ çš„å‘é‡ï¼Œ
> 
> ä»¤ $y = Wx$ ä¸º $x$ çš„å‹ç¼©ç»“æœï¼Œå¹¶ä¸”ä»¤ 
> $$\tilde{x} \in \arg \min_{{v}: W{v}=y} ||{v}||_0$$ ä¸ºé‡æ„å‘é‡ã€‚é‚£ä¹ˆï¼Œ$\tilde{x} = x$ã€‚

è¿™ä¸ªå®šç†å‘Šè¯‰æˆ‘ä»¬å¯¹äºRIPçš„çŸ©é˜µï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡æ‰¾åˆ°ç¬¦åˆ$Wv=y$çš„$v$çš„l0-normæœ€å°çš„å‘é‡ï¼Œæˆ‘ä»¬å°±èƒ½å¤ŸæˆåŠŸçš„(æ— æŸ)é‡å»ºå‡º$x$ã€‚

**Pf.**
ä»¤ $h = \tilde{x} - x$

$$
\|\tilde{x}\|_0 \leq \|x\|_0 \leq s
$$

å› æ­¤ $h$ æ˜¯ $2s$-sparseçš„ã€‚

$$
(1 - \epsilon) \|h\|^2 \leq \|Wh\|^2 \leq (1 + \epsilon) \|h\|^2
$$

ç”±äº $Wh = W(\tilde{x} - x) = 0$

$$
\Rightarrow \|h\|^2 = 0
$$

å› æ­¤ $\tilde{x} = x$ã€‚$\blacksquare$

ä½†é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªpolytimeæ±‚è§£l0-normæœ€å°å€¼çš„ç®—æ³•ï¼Œæ‰€ä»¥è¿™ä¸ªå®šç†åœ¨å®é™…åº”ç”¨ä¸­æ²¡æœ‰æ„ä¹‰ï¼Œæˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­å°è¯•å§l0-norm relaxåˆ° l1-normï¼Œä¸‹é¢çš„thm2å’Œ3ä¾¿æ˜¯l1-normä¸‹é‡å»ºç»“æœç›¸ä¼¼æ€§çš„ä¿è¯ã€‚
### Thm.2
> **Thm.2** å‡è®¾ $W$ ä¸º $(\epsilon, 2s)$-RIP çŸ©é˜µã€‚$x$ ä¸ºä¸€ä¸ªæ»¡è¶³ $\|x\|_0 \leq s$ çš„å‘é‡ï¼Œ
>
> ä»¤ $y = Wx$ ä¸º $x$ çš„å‹ç¼©ç»“æœï¼Œå¹¶ä¸” $\epsilon < \frac{1}{1 + \sqrt{2}}$ï¼Œé‚£ä¹ˆ,
> 
> $$x=\arg \min_{v: Wv = y} ||v||_ {0}=\arg \min_{v:Wv = y}||v||_1$$

è¿™ä¸ªå®šç†è¯´æ˜åœ¨s-sparseçš„æƒ…å†µä¸‹ï¼ŒRelax åˆ°l1-normä¹Ÿå¯ä»¥é‡æ„å‡ºä¸€æ ·çš„å‘é‡ã€‚

äº‹å®ä¸Šï¼Œæˆ‘ä»¬å°†è¯æ˜ä¸€ä¸ªæ›´å¼ºçš„ç»“æœï¼Œè¯¥ç»“æœå³ä½¿åœ¨ $x$ ä¸æ˜¯ä¸€ä¸ªç¨€ç–å‘é‡çš„æƒ…å†µä¸‹ä¹Ÿæˆç«‹ï¼Œå³Thm.3ã€‚
### Thm.3
> **Thm.3** è®¾ $\epsilon < \frac{1}{1 + \sqrt{2}}$ å¹¶ä¸” $W$ æ˜¯ä¸€ä¸ª $(\epsilon, 2s)$-RIP çŸ©é˜µã€‚è®¾ $x$ æ˜¯ä»»æ„å‘é‡ï¼Œå¹¶å®šä¹‰
> $$x_s \in \arg \min_{v\: ||v|| _ 0 \leq s} ||x - v||_ 1 $$
> ä¹Ÿå°±æ˜¯è¯´ï¼Œ$x_s$ æ˜¯ä¸€ä¸ªåœ¨ $x$ çš„ $s$ ä¸ªæœ€å¤§å…ƒç´ å¤„ç­‰äº $x$ å¹¶åœ¨å…¶ä»–åœ°æ–¹ç­‰äº $0$ çš„å‘é‡ã€‚è®¾ $y = Wx$ ï¼Œå¹¶ä»¤
> $$x^* \in \arg \min_{v: Wv = y} \|v\|_1$$
> ä¸ºé‡æ„çš„å‘é‡ã€‚é‚£ä¹ˆï¼Œ
> $$\|x^* - x\|_2 \leq 2 \frac{1 + \rho}{1 - \rho} s^{-1/2} \|x - x_s\|_1,$$
> å…¶ä¸­ $\rho = \sqrt{2\epsilon}/(1 - \epsilon)$ã€‚

**Pf.**

è¿™ä¸ªå®šç†çš„è¯æ˜ç›¸å¯¹æ¯”è¾ƒå¤æ‚ï¼Œä¸»è¦æ˜¯è¯æ˜ä»¥ä¸‹ä¸¤ä¸ªClaim:
> **Claim 1ï¼š** 
> $$
\|h_{T_{0,1}}\|_ 2 \leq \|h _{T_0}\|_2 + 2s^{-1/2}\|x - x_s\|_1ã€‚
$$

> **Claim 2ï¼š** 
> $$
\|h_{T_{0,1}}\|_ 2 \leq \frac{2\rho}{1 - \rho}s^{-1/2}\|x - x_s\|_1ã€‚
$$
**ç¬¦å·è¯´æ˜ï¼š** ç»™å®šä¸€ä¸ªå‘é‡ $v$ å’Œä¸€ç»„ç´¢å¼• $I$ï¼Œæˆ‘ä»¬ç”¨ $v_I$ è¡¨ç¤ºå‘é‡ï¼Œå…¶ç¬¬ $i$ ä¸ªå…ƒç´ ä¸º $v_i$ å¦‚æœ $i \in I$ï¼Œå¦åˆ™å…¶ç¬¬ $i$ ä¸ªå…ƒç´ ä¸º 0ã€‚ä»¤ $h = x^* - x$ã€‚

æˆ‘ä»¬ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªæŠ€å·§æ˜¯å°†ç´¢å¼•é›†åˆ $[d] = \{1, \ldots, d\}$ åˆ’åˆ†ä¸ºå¤§å°ä¸º $s$ çš„ä¸ç›¸äº¤é›†åˆã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å†™ä½œ $[d] = T_0 \cup T_1 \cup T_2 \ldots T_{d/s-1}$ï¼Œå¯¹äºæ‰€æœ‰ $i$ï¼Œæˆ‘ä»¬æœ‰ $|T_i| = s$ï¼Œå¹¶ä¸”æˆ‘ä»¬ä¸ºç®€ä¾¿èµ·è§å‡è®¾ $d/s$ æ˜¯ä¸€ä¸ªæ•´æ•°ã€‚æˆ‘ä»¬å¦‚ä¸‹å®šä¹‰åˆ’åˆ†ã€‚åœ¨ $T_0$ ä¸­ï¼Œæˆ‘ä»¬æ”¾ç½® $s$ ä¸ªå¯¹åº”äº $x$ çš„ç»å¯¹å€¼ä¸­æœ€å¤§çš„å…ƒç´ çš„ç´¢å¼•ï¼ˆå¦‚æœæœ‰å¹¶åˆ—çš„æƒ…å†µï¼Œåˆ™ä»»æ„æ‰“ç ´å¹³å±€ï¼‰ã€‚è®¾ $T_0^c = [d] \setminus T_0$ã€‚æ¥ä¸‹æ¥ï¼Œ$T_1$ å°†æ˜¯å¯¹åº”äº $h_{T_0^c}$ ç»å¯¹å€¼ä¸­æœ€å¤§çš„ $s$ ä¸ªå…ƒç´ çš„ç´¢å¼•ã€‚è®¾ $T_{0,1} = T_0 \cup T_1$ï¼Œå¹¶ä»¤ $T_{0,1}^c = [d] \setminus T_{0,1}$ã€‚æ¥ä¸‹æ¥ï¼Œ$T_2$ å°†æ˜¯å¯¹åº”äº $h_{T_{0,1}^c}$ ç»å¯¹å€¼ä¸­æœ€å¤§çš„ $s$ ä¸ªå…ƒç´ çš„ç´¢å¼•ã€‚æˆ‘ä»¬å°†ç»§ç»­æ„é€  $T_3, T_4, \ldots$ ä»¥ç›¸åŒçš„æ–¹å¼ã€‚

**Pf of Claim 1**ï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨RIPæ¡ä»¶ï¼Œä»…ä»…ä½¿ç”¨$x^*$æœ€å°åŒ–$\ell_1$èŒƒæ•°è¿™ä¸€äº‹å®ã€‚è®¾$j > 1$ã€‚å¯¹äºæ¯ä¸ª$i \in T_j$å’Œ$i' \in T_{j-1}$ï¼Œæˆ‘ä»¬æœ‰$|h_i| \leq |h_{i'}|$ã€‚å› æ­¤ï¼Œ$\|h_ {T_j}\|_ \infty \leq \|h_ {T_ {j-1}}\|_ 1/s$ã€‚ç”±æ­¤å¯ä»¥å¾—åˆ°ï¼š

$$
||h_{T_j}||_ 2 \leq s^{-1/2} ||h_{T_{j-1}}||_1
$$

å¯¹$j = 2, 3, \ldots$æ±‚å’Œï¼Œå¹¶ä½¿ç”¨ä¸‰è§’ä¸ç­‰å¼ï¼Œå¯ä»¥å¾—åˆ°ï¼š

$$
||h_{T_{0,1}^c}||_ 2 \leq \sum_{j \geq 2} ||h_{T_j}||_ 2 \leq s^{-1/2} ||h_{T_{0,1}^c}||_1
$$

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯æ˜$\|h_{T_0}\|_1$ä¸èƒ½å¤ªå¤§ã€‚å®é™…ä¸Šï¼Œç”±äº$x^* = x + h$å…·æœ‰æœ€å°çš„$\ell_1$èŒƒæ•°ï¼Œå¹¶ä¸”$x$æ»¡è¶³$x^*$çš„å®šä¹‰ä¸­çš„çº¦æŸæ¡ä»¶ï¼Œæˆ‘ä»¬æœ‰$\|x\|_1 \geq \|x + h\|_1$ã€‚å› æ­¤ï¼Œåˆ©ç”¨ä¸‰è§’ä¸ç­‰å¼æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

$$
||x||_ 1 \geq \sum_{i \in T_0} |x_i + h_i| + \sum_{i \in T_{0,1}^c} |x_i + h_i| \geq ||x_{T_0}||_ 1 - ||h_{T_0}||_ 1 + ||x_{T_{0,1}^c}||_ 1 - ||h_{T_{0,1}^c}||_1
$$

ç”±äº$\|x_ {T_ {0,1}^c}\|_ 1 = \|x - x_s\|_ 1 = \|x\|_ 1 - \|x_ {T_ 0}\|_ 1$ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

$$
\|h_{T_0}\|_ 1 \leq \|h_{T_0}\|_ 1 + 2\|x_{T_{0,1}^c}\|_1ã€‚
$$

ç»“åˆä¸Šè¿°ç­‰å¼å¯ä»¥å¾—åˆ°ï¼š

$$
\|h_{T_{0,1}^c}\|_ 2 \leq s^{-1/2} (\|h_{T_0}\|_ 1 + 2\|x_{T_{0,1}^c}\|_1)ã€‚\blacksquare
$$

**Pf of Claim 2**

å¯¹äº2s-ç¨€ç–çš„å‘é‡$h_{T_{0,1}}$ï¼Œæˆ‘ä»¬æœ‰ï¼š

$$(1 - \epsilon) ||h_{T_{0,1}}||_ 2^2 \leq ||Wh_{T_{0,1}}||_2^2$$

è€Œ

$$Wh_{T_{0,1}} = Wh - \sum_{j \geq 2} Wh_{T_j} = -\sum_{j \geq 2} Wh_{T_j}$$

å› æ­¤

$$||Wh_{T_{0,1}}||_ 2^2 = -\sum_{j \geq 2} \langle Wh_{T_{0,1}}, Wh_{T_j} \rangle$$

> **Lemma**ï¼šå¦‚æœ$W$æ˜¯$(\epsilon, 2s)$-RIPçŸ©é˜µï¼Œå¯¹äºä»»æ„ä¸ç›¸äº¤çš„$I, J$é›†åˆï¼Œè‹¥$|I| \leq s, |J| \leq s$ï¼Œåˆ™
> $$
\langle W u_{I}, W u_{J} \rangle \leq \epsilon \|u_{I}\| \|u_{J}\|
$$

**Pf.**

$$\langle W u_{I}, W u_{J} \rangle = \frac{\|W(u_I + u_J)\|^2 - \|W(u_I - u_J)\|^2}{4}$$


$$
\leq \frac{(1 + \epsilon) \|u_I + u_J\|^2 - (1 - \epsilon) \|u_I - u_J\|^2}{4}
$$

ç”±äº$I, J$æ˜¯ä¸ç›¸äº¤çš„é›†åˆï¼š

$$
= \frac{(1 + \epsilon) (\|u_I\|^2 + \|u_J\|^2) - (1 - \epsilon) (\|u_I\|^2 + \|u_J\|^2)}{4}
$$

$$
= \frac{\epsilon}{2} ((\|u_I\|^2 + \|u_J\|^2) \leq \epsilon \|u_I\|\|u_J\|.\blacksquare
$$

åŸå¼ä»£å…¥Lemmaï¼Œæˆ‘ä»¬æœ‰ï¼š

$$||Wh_{T_{0,1}}||_ 2^2 \leq \epsilon (||h_{T_0}||_ 2 + ||h_{T_{1}}||_ 2) \cdot \sum_{j \geq 2} ||h_{T_j}||_ 2
$$
åˆ©ç”¨$2(a^2 + b^2) \geq (a + b)^2$:
$$||h_{T_0}||_ 2 + ||h_{T_1}||_ 2 \leq \sqrt{2} ||h_ {T_{0,1}}|| _2$$

æ‰€ä»¥

$$
\|Wh_{T_{0,1}}\|_ 2^2 \leq \sqrt{2} \epsilon \|h_{T_{0,1}}\| _ 2 \cdot \sum_{j \geq 2} \|h_{T_j}\|_ 2
$$

$$
\leq \sqrt{2} \epsilon \cdot s^{-1/2} \|h_{T_ {0,1}}\| _ 2 \cdot \|h _{T _{0,1}^C}\| _1
$$

å› æ­¤

$$
\|h_{T_0,1}\|_ 2 \leq \frac{\sqrt{2} \epsilon}{1 - \epsilon} s^{-1/2} \|h_{T_0^C}\|_1
$$

$$
\|h_{T_0,1}\|_ 2 \leq \frac{\sqrt{2} \epsilon}{1 - \epsilon} s^{-1/2} (\|h_{T_0}\|_ 1 + 2\|x_{T_0^C}\|_1)
$$

$$
\leq \rho ||h_ {T_{0}}|| _{2} + 2 \rho s^{-1/2} ||x _{T _{0}^{C}}|| _{1}
$$

ç”±äº

$$||h_{T_ {1}}|| _ 2 \leq ||h_{T _{0,1}}|| _2$$

å› æ­¤

$$
||h_{T_{0,1}}||_2 \leq \frac{2 \rho}{1 - \rho} s^{-1/2} ||x - x_s||_1\blacksquare
$$

å›åˆ°*Thm.3*çš„è¯æ˜:

$$
\|h\|_ 2 \leq \|h_{T _{0,1}}\| _2 + \|h _{T _{0,1}^C}\| _2
$$

$$
\leq 2 \|h_{T_0,1}\|_2 + 2s^{-1/2} \|x - x_s\|_1
$$

$$
\leq \left( \frac{4 \rho}{1 - \rho} s^{-1/2} + 2s^{-1/2} \right) \|x - x_s\|_1
$$

$$
= 2 \frac{1 + \rho}{1 - \rho} s^{-1/2} \|x - x_s\|_1. \blacksquare
$$
### Thm.4
æœ€åæˆ‘ä»¬å°±å‰©ä¸‹Thm.4äº†ï¼Œ
> **Thm.4**  
è®¾ $U$ ä¸ºä»»æ„å›ºå®šçš„ $d \times d$ æ­£äº¤çŸ©é˜µï¼Œè®¾ $\epsilon, \delta$ ä¸ºåœ¨ $(0, 1)$ ä¹‹é—´çš„æ ‡é‡ï¼Œè®¾ $s$ æ˜¯ $[d]$ ä¸­çš„ä¸€ä¸ªæ•´æ•°ï¼Œä¸”è®¾ $n$ ä¸ºæ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„æ•´æ•°
> $$
n \geq 100 \frac{s \ln(40d/(\delta \epsilon))}{\epsilon^2}.$$
> è®¾ $W \in \mathbb{R}^{n, d}$ ä¸ºä¸€ä¸ªçŸ©é˜µï¼Œå…¶æ¯ä¸ªå…ƒç´ å‡ä»¥é›¶å‡å€¼å’Œæ–¹å·® $1/n$ æ­£æ€åˆ†å¸ƒã€‚åˆ™ï¼Œå¯¹äºè‡³å°‘ $1 - \delta$ çš„æ¦‚ç‡è€Œè¨€ï¼ŒçŸ©é˜µ $WU$ æ˜¯ $(\epsilon, s)$-RIPã€‚

è¿™é‡Œçš„å¸¸æ•°é¡¹å¯èƒ½æœ‰ä¸€äº›é—®é¢˜ï¼Œè¯æ˜ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œå°±ä¸å±•å¼€äº†ã€‚å¤§ä½“çš„Proof Sketchæ˜¯:
* å°†è¿ç»­ç©ºé—´æ˜ å°„åˆ°æœ‰é™ä¸ªç‚¹ä¸Š
* è€ƒè™‘ä¸€ä¸ªç‰¹å®šçš„å¤§å°ä¸º $s$çš„ç´¢å¼•é›† $I$

* ä½¿ç”¨è¿™ä¸ªç´¢å¼•é›†è¿›å…¥ç¨€ç–ç©ºé—´
* å¯¹æ‰€æœ‰å¯èƒ½çš„ $I$ åº”ç”¨union bound
å…·ä½“å¯ä»¥å‚è€ƒ*Shai Shalev-Shwartz*çš„paper: [Compressed Sensing:
Basic results and self contained proofs*](https://www.cs.huji.ac.il/~shais/compressedSensing.pdf).
# 4. åè®°
> â€œThe people who are crazy enough to think they can change the world, are the ones who do.â€

æœŸä¸­ä¹‹å‰çš„å†…å®¹å¤§æ¦‚æ˜¯è¿™äº›ã€‚åœ¨å†™ä½œçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘å‘ç°æˆ‘å¾€å¾€ä¼šå¿½ç•¥ä¸€äº›æˆ‘ä¸é‚£ä¹ˆæ„Ÿå…´è¶£çš„éƒ¨åˆ†è€Œåªæ˜¯å»å†™è‡ªè®¤ä¸ºæœ‰è¶£çš„éƒ¨åˆ†ï¼Œè¿™ä¸€ç‚¹äº¦å¦‚æˆ‘çš„å¤ä¹ ï¼Œå…¶ä¸­æ¤å…¥äº†å¤ªå¤šçš„ä¸ªäººç†è§£è€Œå¿½è§†æ‰äº†è€å¸ˆæˆ–è€…å­¦ç•Œä¸»æµæƒ³è®©äººå…³æ³¨çš„æ¡†æ¶ï¼Œå½¢æˆçš„Map of Machine Learning Worldè‡ªç„¶ä¹Ÿä¼šæ˜¯ä¸åŒçš„ã€‚è¿™å¤§æŠµä¹Ÿèƒ½è§£é‡Šè€ƒè¯•ä¸ºä»€ä¹ˆä¼šå¯„çš„ä¸€éƒ¨åˆ†åŸå› å§ã€‚ååŠå­¦æœŸäº‰å–è®©è‡ªå·±å­¦ä¼šçš„ä¸œè¥¿çš„åˆ†å¸ƒå’Œè¯¾ä¸Šçš„åˆ†å¸ƒæ¥è¿‘ä¸€äº›ï¼Œæˆ–è€…æä¸€ä¸ªgenerative model,ä»è‡ªå·±çš„åˆ†å¸ƒé‡Œé‡‡æ ·ï¼Œç»è¿‡ä¸€äº›å˜æ¢èƒ½å¤Ÿæ¥è¿‘ä»–çš„åˆ†å¸ƒå§ã€‚
![](../img/ml1/image5.png#center)



