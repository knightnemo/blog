---
title: "The Transformer Stoppers"
date: 2025-01-29
draft: true
ShowToc: true
tags: ["machine-learning", "computer-science", "deep-learning", "transformer", "nlp", "aritificial-intelligence"]
summary: "Just like the famous 'Jordan Stoppers' in basketball, there are a number of works claiming to substitute and overperform the Transformer. This article goes through some of them, namely Mamba, Titans, Transformer^2, TTT, Infini-Transformer and RetNet."
---

# 1. Mamba
Mamba is a new architecture that is claimed to be faster than the Transformer. It is based on the idea of using a more efficient attention mechanism, which is claimed to be faster and more memory-efficient.

# 2. Titans

# 3. Transformer$^2$

# 4. TTT

# 5. Infini-Transformer

# 6. RetNet

# RWKV

# FNET
'''
https://zhuanlan.zhihu.com/p/673781418
https://github.com/fla-org/flash-linear-attention

'''
